{
    "docs": [
        {
            "location": "/", 
            "text": "UncertainData.jl\n\n\n\n\nMotivation\n\n\nUncertainData.jl was born to systematically deal with uncertain data, and to  \nsample\n from  \nuncertain datasets\n more rigorously.  It makes workflows involving  \nuncertain data of different types\n  and from different sources significantly easier. \n\n\n\n\nPackage philosophy\n\n\nWay too often in data analysis the uncertainties in observational data are ignored or not  dealt with in a systematic manner. The core concept of the package is that uncertain data  should live in the probability domain, not as single value representations of the data  (e.g. the mean).\n\n\nIn this package, uncertain data values are thus  \nstored as probability distributions\n.  Only when performing a computation or plotting, the uncertain values are realized by  resampling the probability distributions furnishing them. \n\n\n\n\nOrganising uncertain data\n\n\nIndividual uncertain observations may be collected in  \nUncertainDatasets\n, which can be  sampled according to user-provided sampling constraints. Likewise, indices (e.g. time,  depth or any other index) of observations can also be represented as probability  distributions and may also sampled using constraints. \n\n\nThe \nUncertainIndexValueDataset\n type  allows you to work with datasets where both the  \nindices\n and the  \ndata values\n are uncertain. This may be useful when you, for example, want to draw realizations of your dataset while  simultaneously enforcing  \nsequential resampling\n,  for example  \nstrictly increasing\n age models.\n\n\n\n\nMathematical operations\n\n\nSeveral \nelementary mathematical operations\n and  \ntrigonometric functions\n are supported  for uncertain values. Computations are done using a  \nresampling approach\n.\n\n\n\n\nStatistics on uncertain datasets\n\n\nStatistics\n on  uncertain observations and uncertain datasets are obtained using a resampling approach. \n\n\n\n\nBasic workflow\n\n\n\n\nDefine uncertain values\n by probability distributions.\n\n\nDefine uncertain datasets\n by gathering uncertain values.\n\n\nUse sampling constraints\n to \nconstraint the support of the distributions furnishing the uncertain values\n (i.e. apply subjective criteria to decide what is acceptable data and what is not).\n\n\nResample the uncertain values\n or \nuncertain datasets\n.\n\n\nExtend existing algorithm\n to accept uncertain values/datasets.\n\n\nQuantify the uncertainty\n in your dataset or on whatever measure your algorithm computes.\n\n\n\n\n\n\nRelated software\n\n\nA related package is \nMeasurements.jl\n, which propagates errors exactly and handles correlated uncertainties. However,  Measurements.jl accepts only normally distributed values. This package serves a slightly  different purpose: it was born to provide an easy way of handling uncertainties of  \nmany different types\n,  using a \nresampling\n approach to obtain  \nstatistics\n when needed, and providing a rich set of  \nsampling constraints\n that makes it easy  for the user to reason about and plot their uncertain data under different assumptions.\n\n\nDepending on your needs, \nMeasurements.jl\n  may be a better (and faster) choice if your data satisfies the requirements for the package  (normally distributed) and if your uncertainties are correlated.\n\n\n\n\nContributing\n\n\nIf you have questions, or a good idea for new functionality that could be useful to have in  the package, please submit an issue, or even better - a pull request.", 
            "title": "Overview"
        }, 
        {
            "location": "/#uncertaindatajl", 
            "text": "", 
            "title": "UncertainData.jl"
        }, 
        {
            "location": "/#motivation", 
            "text": "UncertainData.jl was born to systematically deal with uncertain data, and to   sample  from   uncertain datasets  more rigorously.  It makes workflows involving   uncertain data of different types   and from different sources significantly easier.", 
            "title": "Motivation"
        }, 
        {
            "location": "/#package_philosophy", 
            "text": "Way too often in data analysis the uncertainties in observational data are ignored or not  dealt with in a systematic manner. The core concept of the package is that uncertain data  should live in the probability domain, not as single value representations of the data  (e.g. the mean).  In this package, uncertain data values are thus   stored as probability distributions .  Only when performing a computation or plotting, the uncertain values are realized by  resampling the probability distributions furnishing them.", 
            "title": "Package philosophy"
        }, 
        {
            "location": "/#organising_uncertain_data", 
            "text": "Individual uncertain observations may be collected in   UncertainDatasets , which can be  sampled according to user-provided sampling constraints. Likewise, indices (e.g. time,  depth or any other index) of observations can also be represented as probability  distributions and may also sampled using constraints.   The  UncertainIndexValueDataset  type  allows you to work with datasets where both the   indices  and the   data values  are uncertain. This may be useful when you, for example, want to draw realizations of your dataset while  simultaneously enforcing   sequential resampling ,  for example   strictly increasing  age models.", 
            "title": "Organising uncertain data"
        }, 
        {
            "location": "/#mathematical_operations", 
            "text": "Several  elementary mathematical operations  and   trigonometric functions  are supported  for uncertain values. Computations are done using a   resampling approach .", 
            "title": "Mathematical operations"
        }, 
        {
            "location": "/#statistics_on_uncertain_datasets", 
            "text": "Statistics  on  uncertain observations and uncertain datasets are obtained using a resampling approach.", 
            "title": "Statistics on uncertain datasets"
        }, 
        {
            "location": "/#basic_workflow", 
            "text": "Define uncertain values  by probability distributions.  Define uncertain datasets  by gathering uncertain values.  Use sampling constraints  to  constraint the support of the distributions furnishing the uncertain values  (i.e. apply subjective criteria to decide what is acceptable data and what is not).  Resample the uncertain values  or  uncertain datasets .  Extend existing algorithm  to accept uncertain values/datasets.  Quantify the uncertainty  in your dataset or on whatever measure your algorithm computes.", 
            "title": "Basic workflow"
        }, 
        {
            "location": "/#related_software", 
            "text": "A related package is  Measurements.jl , which propagates errors exactly and handles correlated uncertainties. However,  Measurements.jl accepts only normally distributed values. This package serves a slightly  different purpose: it was born to provide an easy way of handling uncertainties of   many different types ,  using a  resampling  approach to obtain   statistics  when needed, and providing a rich set of   sampling constraints  that makes it easy  for the user to reason about and plot their uncertain data under different assumptions.  Depending on your needs,  Measurements.jl   may be a better (and faster) choice if your data satisfies the requirements for the package  (normally distributed) and if your uncertainties are correlated.", 
            "title": "Related software"
        }, 
        {
            "location": "/#contributing", 
            "text": "If you have questions, or a good idea for new functionality that could be useful to have in  the package, please submit an issue, or even better - a pull request.", 
            "title": "Contributing"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/", 
            "text": "The core concept of \nUncertainData\n is to replace an uncertain data value with a  probability distribution describing the point's uncertainty.\n\n\nThere are currently three ways of doing so:\n\n\n\n\nby \ntheoretical distributions with known parameters\n\n\nby \ntheoretical distributions with parameters fitted to empirical data\n\n\nby \nkernel density estimates to empirical data\n\n\nby \nweighted populations\n where the probability of drawing values are    already known, so you can skip kernel density estimation.\n\n\na type representing \nvalues without uncertainty\n, so you can mix    uncertain values with certain values\n\n\n\n\n\n\nSome quick examples\n\n\nSee also the \nextended examples\n!\n\n\n\n\nKernel density estimation (KDE)\n\n\nIf the data doesn't follow an obvious theoretical distribution, the recommended course of action is to represent the uncertain value with a kernel density estimate of the distribution.\n\n\n\n\n\n\nImplicit KDE estimate\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a kernel density estimate (it is inferred\n\n\n# that KDE is wanted when no distribution is provided to the constructor).\n\n\nuv\n \n=\n \nUncertainValue\n(\nsome_sample\n)\n\n\n\n\n\n\n\n\nExplicit KDE estimate\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n\n# Specify that we want a kernel density estimate representation\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\n\n\nPopulations\n\n\nIf you have a population of values where each value has a probability assigned to it,  you can construct an uncertain value by providing the values and uncertainties as  two equal-length vectors to the constructor. Weights are normalized by default.\n\n\n1\n2\n3\nvals\n \n=\n \nrand\n(\n100\n)\n\n\nweights\n \n=\n \nrand\n(\n100\n)\n\n\np\n \n=\n \nUncertainValue\n(\nvals\n,\n \nweights\n)\n\n\n\n\n\n\n\n\n\nFitting a theoretical distribution\n\n\nIf your data has a histogram closely resembling some theoretical distribution, the uncertain value may be represented by fitting such a distribution to the data.\n\n\n\n\n\n\nExample 1: fitting a normal distribution\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a theoretical normal distribution with\n\n\n# parameters fitted to the data.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nExample 2: fitting a gamma distribution\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a gamma distribution, so that we get a\n\n\n# histogram resembling a gamma distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a theoretical gamma distribution with\n\n\n# parameters fitted to the data.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\n\n\nTheoretical distribution with known parameters\n\n\nIt is common when working with uncertain data found in the scientific literature that data value are stated to follow a distribution with given parameters. For example, a data value may be given as normal distribution with a given mean \n\u03bc\n \n=\n \n2\n.\n2\n and standard deviation \n\u03c3\n \n=\n \n0\n.\n3\n.\n\n\n\n\n\n\nExample 1: theoretical normal distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical normal distribution with\n\n\n# known parameters \u03bc = 2.2 and \u03c3 = 0.3\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.2\n,\n \n0.3\n)\n\n\n\n\n\n\n\n\nExample 2: theoretical gamma distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical gamma distribution with\n\n\n# known parameters \u03b1 = 2.1 and \u03b8 = 3.1\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2.1\n,\n \n3.1\n)\n\n\n\n\n\n\n\n\nExample 3: theoretical binomial distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical binomial distribution with\n\n\n# known parameters p = 32 and p = 0.13\n\n\nuv\n \n=\n \nUncertainValue\n(\nBinomial\n,\n \n32\n,\n \n0.13\n)\n\n\n\n\n\n\n\n\n\n\nValues with no uncertainty\n\n\nScalars with no uncertainty can also be represented. \n\n\n1\nc1\n,\n \nc2\n \n=\n \nUncertainValue\n(\n2\n),\n \nUncertainValue\n(\n2.2\n)", 
            "title": "Overview"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#some_quick_examples", 
            "text": "See also the  extended examples !", 
            "title": "Some quick examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#kernel_density_estimation_kde", 
            "text": "If the data doesn't follow an obvious theoretical distribution, the recommended course of action is to represent the uncertain value with a kernel density estimate of the distribution.    Implicit KDE estimate  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData ,   KernelDensity  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Uncertain value represented by a kernel density estimate (it is inferred  # that KDE is wanted when no distribution is provided to the constructor).  uv   =   UncertainValue ( some_sample )     Explicit KDE estimate  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Specify that we want a kernel density estimate representation  uv   =   UncertainValue ( UnivariateKDE ,   some_sample )", 
            "title": "Kernel density estimation (KDE)"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#populations", 
            "text": "If you have a population of values where each value has a probability assigned to it,  you can construct an uncertain value by providing the values and uncertainties as  two equal-length vectors to the constructor. Weights are normalized by default.  1\n2\n3 vals   =   rand ( 100 )  weights   =   rand ( 100 )  p   =   UncertainValue ( vals ,   weights )", 
            "title": "Populations"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#fitting_a_theoretical_distribution", 
            "text": "If your data has a histogram closely resembling some theoretical distribution, the uncertain value may be represented by fitting such a distribution to the data.    Example 1: fitting a normal distribution  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Uncertain value represented by a theoretical normal distribution with  # parameters fitted to the data.  uv   =   UncertainValue ( Normal ,   some_sample )     Example 2: fitting a gamma distribution  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData  # Generate some random data from a gamma distribution, so that we get a  # histogram resembling a gamma distribution.  some_sample   =   rand ( Gamma (),   1000 )  # Uncertain value represented by a theoretical gamma distribution with  # parameters fitted to the data.  uv   =   UncertainValue ( Gamma ,   some_sample )", 
            "title": "Fitting a theoretical distribution"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#theoretical_distribution_with_known_parameters", 
            "text": "It is common when working with uncertain data found in the scientific literature that data value are stated to follow a distribution with given parameters. For example, a data value may be given as normal distribution with a given mean  \u03bc   =   2 . 2  and standard deviation  \u03c3   =   0 . 3 .    Example 1: theoretical normal distribution  1\n2\n3 # Uncertain value represented by a theoretical normal distribution with  # known parameters \u03bc = 2.2 and \u03c3 = 0.3  uv   =   UncertainValue ( Normal ,   2.2 ,   0.3 )     Example 2: theoretical gamma distribution  1\n2\n3 # Uncertain value represented by a theoretical gamma distribution with  # known parameters \u03b1 = 2.1 and \u03b8 = 3.1  uv   =   UncertainValue ( Gamma ,   2.1 ,   3.1 )     Example 3: theoretical binomial distribution  1\n2\n3 # Uncertain value represented by a theoretical binomial distribution with  # known parameters p = 32 and p = 0.13  uv   =   UncertainValue ( Binomial ,   32 ,   0.13 )", 
            "title": "Theoretical distribution with known parameters"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_overview/#values_with_no_uncertainty", 
            "text": "Scalars with no uncertainty can also be represented.   1 c1 ,   c2   =   UncertainValue ( 2 ),   UncertainValue ( 2.2 )", 
            "title": "Values with no uncertainty"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/", 
            "text": "First, load the necessary packages:\n\n\n1\nusing\n \nUncertainData\n,\n \nDistributions\n,\n \nKernelDensity\n,\n \nPlots\n\n\n\n\n\n\n\n\n\nExample 1: Uncertain values defined by theoretical distributions\n\n\n\n\nA uniformly distributed uncertain value\n\n\nConsider the following contrived example. We've measure a data value with a poor instrument  that tells us that the value lies between \n-\n2\n and \n3\n. However, we but that we know nothing  more about how the value is distributed on that interval. Then it may be reasonable to  represent that value as a uniform distribution on \n[\n-\n2\n,\n \n3\n]\n.\n\n\nTo construct an uncertain value following a uniform distribution, we use the constructor  for theoretical distributions with known parameters  (\nUncertainValue\n(\ndistribution\n,\n \nparams\n...)\n). \n\n\nThe uniform distribution is defined by its lower and upper bounds, so we'll provide  these bounds as the parameters. \n\n\n1\n2\n3\n4\nu\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n1\n,\n \n2\n)\n\n\n\n# Plot the estimated density\n\n\nbar\n(\nu\n,\n \nlabel\n \n=\n \n,\n \nxlabel\n \n=\n \nvalue\n,\n \nylabel\n \n=\n \nprobability density\n)\n\n\n\n\n\n\n\n\n\n\n\nA normally distributed uncertain value\n\n\nA situation commonly encountered is to want to use someone else's data from a publication.  Usually, these values are reported as the mean or median, with some associated uncertainty.  Say we want to use an uncertain value which is normally distributed with mean \n2\n.\n1\n and  standard deviation \n0\n.\n3\n.\n\n\nNormal distributions also have two parameters, so we'll use the two-parameter constructor  as we did above. \n\n\n1\n2\n3\n4\nu\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.1\n,\n \n0.3\n)\n\n\n\n# Plot the estimated density\n\n\nbar\n(\nu\n,\n \nlabel\n \n=\n \n,\n \nxlabel\n \n=\n \nvalue\n,\n \nylabel\n \n=\n \nprobability density\n)\n\n\n\n\n\n\n\n\n\n\n\nOther distributions\n\n\nYou may define uncertain values following any of the  \nsupported distributions\n. \n\n\n\n\nExample 2: Uncertain values defined by kernel density estimated distributions\n\n\nOne may also be given a a distribution of numbers that's not quite normally distributed.  How to represent this uncertainty? Easy: we use a kernel density estimate to the distribution.\n\n\nLet's define a complicated distribution which is a mixture of two different normal  distributions, then draw a sample of numbers from it.\n\n\n1\n2\nM\n \n=\n \nMixtureModel\n([\nNormal\n(\n-\n5\n,\n \n0.5\n),\n \nNormal\n(\n0.2\n)])\n\n\nsome_sample\n \n=\n \nrand\n(\nM\n,\n \n250\n)\n\n\n\n\n\n\n\nNow, pretend that \nsome_sample\n is a list of measurements we got from somewhere.  KDE estimates to the distribution can be defined implicitly or explicitly as follows:\n\n\n1\n2\n3\n4\n5\n# If the only argument to `UncertainValue()` is a vector of number, KDE will be triggered.\n\n\nu\n \n=\n \nUncertainValue\n(\nrand\n(\nM\n,\n \n250\n))\n \n\n\n# You may also tell the constructor explicitly that you want KDE. \n\n\nu\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\nM\n,\n \n250\n))\n\n\n\n\n\n\n\nNow, let's plot the resulting distribution. \nNote: this is not the original mixture of  Gaussians we started out with, it's the kernel density estimate to that mixture!\n\n\n1\n2\n# Plot the estimated distribution.\n\n\nplot\n(\nu\n,\n \nxlabel\n \n=\n \nValue\n,\n \nylabel\n \n=\n \nProbability density\n)\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Uncertain values defined by theoretical distributions fitted to empirical data\n\n\nOne may also be given a dataset whose histogram looks a lot like a theoretical distribution. We may then select a theoretical distribution and fit its parameters to the empirical data. \n\n\nSay our data was a sample that looks like it obeys Gamma distribution. \n\n\n1\n2\n# Draw a 2000-point sample from a Gamma distribution with parameters \u03b1 = 1.7 and \u03b8 = 5.5\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n1.7\n,\n \n5.5\n),\n \n2000\n)\n\n\n\n\n\n\n\nTo perform a parameter estimation, simply provide the distribution as the first  argument and the sample as the second argument to the \nUncertainValue\n constructor.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Take a sample from a Gamma distribution with parameters \u03b1 = 1.7 and \u03b8 = 5.5 and \n\n\n# create a histogram of the sample.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n1.7\n,\n \n5.5\n),\n \n2000\n)\n\n\n\np1\n \n=\n \nhistogram\n(\nsome_sample\n,\n \nnormalize\n \n=\n \ntrue\n,\n\n    \nfc\n \n=\n \n:\nblack\n,\n \nlc\n \n=\n \n:\nblack\n,\n\n    \nlabel\n \n=\n \n,\n \nxlabel\n \n=\n \nvalue\n,\n \nylabel\n \n=\n \ndensity\n)\n\n\n\n# For the uncertain value representation, fit a gamma distribution to the sample. \n\n\n# Then, compare the histogram obtained from the original distribution to that obtained \n\n\n# when resampling the fitted distribution \n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nsome_sample\n)\n\n\n\n# Resample the fitted theoretical distribution\n\n\np2\n \n=\n \nhistogram\n(\nresample\n(\nuv\n,\n \n10000\n),\n \nnormalize\n \n=\n \ntrue\n,\n\n    \nfc\n \n=\n \n:\nblue\n,\n \nlc\n \n=\n \n:\nblue\n,\n\n    \nlabel\n \n=\n \n,\n \nxlabel\n \n=\n \nvalue\n,\n \nylabel\n \n=\n \ndensity\n)\n\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nlink\n \n=\n \n:\nx\n)\n\n\n\n\n\n\n\nAs expected, the histograms closely match (but are not exact because we estimated the distribution using a limited sample).", 
            "title": "Extended examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#example_1_uncertain_values_defined_by_theoretical_distributions", 
            "text": "", 
            "title": "Example 1: Uncertain values defined by theoretical distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#a_uniformly_distributed_uncertain_value", 
            "text": "Consider the following contrived example. We've measure a data value with a poor instrument  that tells us that the value lies between  - 2  and  3 . However, we but that we know nothing  more about how the value is distributed on that interval. Then it may be reasonable to  represent that value as a uniform distribution on  [ - 2 ,   3 ] .  To construct an uncertain value following a uniform distribution, we use the constructor  for theoretical distributions with known parameters  ( UncertainValue ( distribution ,   params ...) ).   The uniform distribution is defined by its lower and upper bounds, so we'll provide  these bounds as the parameters.   1\n2\n3\n4 u   =   UncertainValue ( Uniform ,   1 ,   2 )  # Plot the estimated density  bar ( u ,   label   =   ,   xlabel   =   value ,   ylabel   =   probability density )", 
            "title": "A uniformly distributed uncertain value"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#a_normally_distributed_uncertain_value", 
            "text": "A situation commonly encountered is to want to use someone else's data from a publication.  Usually, these values are reported as the mean or median, with some associated uncertainty.  Say we want to use an uncertain value which is normally distributed with mean  2 . 1  and  standard deviation  0 . 3 .  Normal distributions also have two parameters, so we'll use the two-parameter constructor  as we did above.   1\n2\n3\n4 u   =   UncertainValue ( Normal ,   2.1 ,   0.3 )  # Plot the estimated density  bar ( u ,   label   =   ,   xlabel   =   value ,   ylabel   =   probability density )", 
            "title": "A normally distributed uncertain value"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#other_distributions", 
            "text": "You may define uncertain values following any of the   supported distributions .", 
            "title": "Other distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#example_2_uncertain_values_defined_by_kernel_density_estimated_distributions", 
            "text": "One may also be given a a distribution of numbers that's not quite normally distributed.  How to represent this uncertainty? Easy: we use a kernel density estimate to the distribution.  Let's define a complicated distribution which is a mixture of two different normal  distributions, then draw a sample of numbers from it.  1\n2 M   =   MixtureModel ([ Normal ( - 5 ,   0.5 ),   Normal ( 0.2 )])  some_sample   =   rand ( M ,   250 )    Now, pretend that  some_sample  is a list of measurements we got from somewhere.  KDE estimates to the distribution can be defined implicitly or explicitly as follows:  1\n2\n3\n4\n5 # If the only argument to `UncertainValue()` is a vector of number, KDE will be triggered.  u   =   UncertainValue ( rand ( M ,   250 ))   # You may also tell the constructor explicitly that you want KDE.   u   =   UncertainValue ( UnivariateKDE ,   rand ( M ,   250 ))    Now, let's plot the resulting distribution.  Note: this is not the original mixture of  Gaussians we started out with, it's the kernel density estimate to that mixture!  1\n2 # Plot the estimated distribution.  plot ( u ,   xlabel   =   Value ,   ylabel   =   Probability density )", 
            "title": "Example 2: Uncertain values defined by kernel density estimated distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_examples/#example_3_uncertain_values_defined_by_theoretical_distributions_fitted_to_empirical_data", 
            "text": "One may also be given a dataset whose histogram looks a lot like a theoretical distribution. We may then select a theoretical distribution and fit its parameters to the empirical data.   Say our data was a sample that looks like it obeys Gamma distribution.   1\n2 # Draw a 2000-point sample from a Gamma distribution with parameters \u03b1 = 1.7 and \u03b8 = 5.5  some_sample   =   rand ( Gamma ( 1.7 ,   5.5 ),   2000 )    To perform a parameter estimation, simply provide the distribution as the first  argument and the sample as the second argument to the  UncertainValue  constructor.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Take a sample from a Gamma distribution with parameters \u03b1 = 1.7 and \u03b8 = 5.5 and   # create a histogram of the sample.  some_sample   =   rand ( Gamma ( 1.7 ,   5.5 ),   2000 )  p1   =   histogram ( some_sample ,   normalize   =   true , \n     fc   =   : black ,   lc   =   : black , \n     label   =   ,   xlabel   =   value ,   ylabel   =   density )  # For the uncertain value representation, fit a gamma distribution to the sample.   # Then, compare the histogram obtained from the original distribution to that obtained   # when resampling the fitted distribution   uv   =   UncertainValue ( Gamma ,   some_sample )  # Resample the fitted theoretical distribution  p2   =   histogram ( resample ( uv ,   10000 ),   normalize   =   true , \n     fc   =   : blue ,   lc   =   : blue , \n     label   =   ,   xlabel   =   value ,   ylabel   =   density )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ),   link   =   : x )    As expected, the histograms closely match (but are not exact because we estimated the distribution using a limited sample).", 
            "title": "Example 3: Uncertain values defined by theoretical distributions fitted to empirical data"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_kde/", 
            "text": "When your data have an empirical distribution that doesn't follow any obvious theoretical distribution, the data may be represented by a kernel density estimate.\n\n\n\n\nExamples\n\n\n\n\n\n\nImplicit KDE constructor\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the implicit KDE constructor to create the uncertain value\n\n\nuv\n \n=\n \nUncertainValue\n(\nv\n::\nVector\n)\n\n\n\n\n\n\n\n\nExplicit KDE constructor\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value.\n\n\n# This constructor follows the same convention as when fitting distributions\n\n\n# to empirical data, so this is the recommended way to construct KDE estimates.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n)\n\n\n\n\n\n\n\n\nChanging the kernel\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value, specifying\n\n\n# that we want to use normal distributions as the kernel. The kernel can be\n\n\n# any valid kernel from Distributions.jl, and the default is to use normal\n\n\n# distributions.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n;\n \nkernel\n \n=\n \nNormal\n)\n\n\n\n\n\n\n\n\nAdjusting number of points\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value, specifying\n\n\n# the number of points we want to use for the kernel density estimate. Fast\n\n\n# Fourier transforms are used behind the scenes, so the number of points\n\n\n# should be a power of 2 (the default is 2048 points).\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n;\n \nnpoints\n \n=\n \n1024\n)\n\n\n\n\n\n\n\n\n\n\nExtended example\n\n\nLet's create a bimodal distribution, then sample 10000 values from it.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nusing\n \nDistributions\n\n\n\nn1\n \n=\n \nNormal\n(\n-\n3.0\n,\n \n1.2\n)\n\n\nn2\n \n=\n \nNormal\n(\n8.0\n,\n \n1.2\n)\n\n\nn3\n \n=\n \nNormal\n(\n0.0\n,\n \n2.5\n)\n\n\n\n# Use a mixture model to create a bimodal distribution\n\n\nM\n \n=\n \nMixtureModel\n([\nn1\n,\n \nn2\n,\n \nn3\n])\n\n\n\n# Sample the mixture model.\n\n\nsamples_empirical\n \n=\n \nrand\n(\nM\n,\n \nInt\n(\n1e4\n));\n\n\n\n\n\n\n\n\n\nIt is not obvious which distribution to fit to such data.\n\n\nA kernel density estimate, however, will always be a decent representation of the data, because it doesn't follow a specific distribution and adapts to the data values.\n\n\nTo create a kernel density estimate, simply call the \nUncertainValue\n(\nv\n::\nVector\n{\nNumber\n}\n)\n constructor with a vector containing the sample:\n\n\n1\nuv\n \n=\n \nUncertainValue\n(\nsamples_empirical\n)\n\n\n\n\n\n\n\nThe plot below compares the empirical histogram (here represented as a density plot) with our kernel density estimate.\n\n\n1\n2\n3\n4\n5\n6\n7\nusing\n \nPlots\n,\n \nStatPlots\n,\n \nUncertainData\n\n\nuv\n \n=\n \nUncertainValue\n(\nsamples_empirical\n)\n\n\ndensity\n(\nmvals\n,\n \nlabel\n \n=\n \n10000 mixture model (M) samples\n)\n\n\ndensity!\n(\nrand\n(\nuv\n,\n \nInt\n(\n1e4\n)),\n\n    \nlabel\n \n=\n \n10000 samples from KDE estimate to M\n)\n\n\nxlabel!\n(\ndata value\n)\n\n\nylabel!\n(\nprobability density\n)\n\n\n\n\n\n\n\n\n\n\n\nConstructor\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nUncertainValue\n \n \nMethod\n.\n\n\n1\n2\n3\nUncertainValue\n(\ndata\n::\nVector\n{\nT\n};\n\n    \nkernel\n::\nType\n{\nD\n}\n \n=\n \nNormal\n,\n\n    \nnpoints\n::\nInt\n=\n2048\n)\n \nwhere\n \n{\nD\n \n:\n \nDistributions\n.\nDistribution\n,\n \nT\n}\n\n\n\n\n\n\n\nConstruct an uncertain value by a kernel density estimate to \ndata\n.\n\n\nFast Fourier transforms are used in the kernel density estimation, so the number of points should be a power of 2 (default = 2048).\n\n\nsource\n\n\n\n\nAdditional keyword arguments and examples\n\n\nIf the only argument to the \nUncertainValue\n constructor is a vector of values, the default behaviour is to represent the distribution by a kernel density estimate (KDE), i.e. \nUncertainValue\n(\ndata\n)\n. Gaussian kernels are used by default. The syntax \nUncertainValue\n(\nUnivariateKDE\n,\n \ndata\n)\n will also work if \nKernelDensity\n.\njl\n is loaded.", 
            "title": "Kernel density estimates (KDE)"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_kde/#examples", 
            "text": "Implicit KDE constructor   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the implicit KDE constructor to create the uncertain value  uv   =   UncertainValue ( v :: Vector )     Explicit KDE constructor   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value.  # This constructor follows the same convention as when fitting distributions  # to empirical data, so this is the recommended way to construct KDE estimates.  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector )     Changing the kernel   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value, specifying  # that we want to use normal distributions as the kernel. The kernel can be  # any valid kernel from Distributions.jl, and the default is to use normal  # distributions.  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector ;   kernel   =   Normal )     Adjusting number of points   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value, specifying  # the number of points we want to use for the kernel density estimate. Fast  # Fourier transforms are used behind the scenes, so the number of points  # should be a power of 2 (the default is 2048 points).  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector ;   npoints   =   1024 )", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_kde/#extended_example", 
            "text": "Let's create a bimodal distribution, then sample 10000 values from it.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 using   Distributions  n1   =   Normal ( - 3.0 ,   1.2 )  n2   =   Normal ( 8.0 ,   1.2 )  n3   =   Normal ( 0.0 ,   2.5 )  # Use a mixture model to create a bimodal distribution  M   =   MixtureModel ([ n1 ,   n2 ,   n3 ])  # Sample the mixture model.  samples_empirical   =   rand ( M ,   Int ( 1e4 ));     It is not obvious which distribution to fit to such data.  A kernel density estimate, however, will always be a decent representation of the data, because it doesn't follow a specific distribution and adapts to the data values.  To create a kernel density estimate, simply call the  UncertainValue ( v :: Vector { Number } )  constructor with a vector containing the sample:  1 uv   =   UncertainValue ( samples_empirical )    The plot below compares the empirical histogram (here represented as a density plot) with our kernel density estimate.  1\n2\n3\n4\n5\n6\n7 using   Plots ,   StatPlots ,   UncertainData  uv   =   UncertainValue ( samples_empirical )  density ( mvals ,   label   =   10000 mixture model (M) samples )  density! ( rand ( uv ,   Int ( 1e4 )), \n     label   =   10000 samples from KDE estimate to M )  xlabel! ( data value )  ylabel! ( probability density )", 
            "title": "Extended example"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_kde/#constructor", 
            "text": "#  UncertainData . UncertainValues . UncertainValue     Method .  1\n2\n3 UncertainValue ( data :: Vector { T }; \n     kernel :: Type { D }   =   Normal , \n     npoints :: Int = 2048 )   where   { D   :   Distributions . Distribution ,   T }    Construct an uncertain value by a kernel density estimate to  data .  Fast Fourier transforms are used in the kernel density estimation, so the number of points should be a power of 2 (default = 2048).  source", 
            "title": "Constructor"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_kde/#additional_keyword_arguments_and_examples", 
            "text": "If the only argument to the  UncertainValue  constructor is a vector of values, the default behaviour is to represent the distribution by a kernel density estimate (KDE), i.e.  UncertainValue ( data ) . Gaussian kernels are used by default. The syntax  UncertainValue ( UnivariateKDE ,   data )  will also work if  KernelDensity . jl  is loaded.", 
            "title": "Additional keyword arguments and examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/", 
            "text": "It is common in the scientific literature to encounter uncertain data values which are reported as following a specific distribution. For example, an author report the mean and standard deviation of a value stated to follow a normal distribution. \nUncertainData\n makes it easy to represent such values!\n\n\n\n\nSupported distributions\n\n\nSupported distributions are \nUniform\n, \nNormal\n, \nGamma\n, \nBeta\n, \nBetaPrime\n, \nFrechet\n, \nBinomial\n, \nBetaBinomial\n (more distributions will be added in the future!).\n\n\n\n\nConstructors\n\n\nThere are two constructors that creates uncertain values represented by theoretical distributions. Parameters are provided to the constructor in the same order as for constructing the equivalent distributions in \nDistributions\n.\njl\n.\n\n\nUncertain values represented by theoretical distributions may be constructed using the two-parameter or three-parameter constructors \nUncertainValue\n(\nd\n::\nType\n{\nD\n}\n,\n \na\n:\nNumber\n,\n \nb\n:\nNumber\n)\n or \nUncertainValue\n(\nd\n::\nType\n{\nD\n}\n,\n \na\n:\nNumber\n,\n \nb\n:\nNumber\n,\n \nc\n:\nNumber\n)\n (see below).\n\n\n\n\nTwo-parameter distributions\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nUncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\ndistribution\n::\nType\n{\nD\n},\n \na\n::\nT1\n,\n \nb\n::\nT2\n;\n\n    \nkwargs\n...\n)\n \nwhere\n \n{\nT1\n:\nNumber\n,\n \nT2\n \n:\n \nNumber\n,\n \nD\n:\nDistribution\n}\n\n\n\n\n\n\n\nConstructor for two-parameter distributions\n\n\nUncertainValue\ns are currently implemented for the following two-parameter distributions: \nUniform\n, \nNormal\n, \nBinomial\n, \nBeta\n, \nBetaPrime\n, \nGamma\n, and \nFrechet\n.\n\n\nArguments\n\n\n\n\na\n, \nb\n: Generic parameters whose meaning varies depending   on what \ndistribution\n is provided. See the list below.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions\n.\njl\n.\n\n\n\n\nPrecisely what  \na\n and \nb\n are depends on which distribution is provided.\n\n\n\n\nUncertainValue\n(\nNormal\n,\n \n\u03bc\n,\n \n\u03c3\n)\n returns an \nUncertainScalarNormallyDistributed\n instance.\n\n\nUncertainValue\n(\nUniform\n,\n \nlower\n,\n \nupper\n)\n returns an \nUncertainScalarUniformlyDistributed\n instance.\n\n\nUncertainValue\n(\nBeta\n,\n \n\u03b1\n,\n \n\u03b2\n)\n returns an \nUncertainScalarBetaDistributed\n instance.\n\n\nUncertainValue\n(\nBetaPrime\n,\n \n\u03b1\n,\n \n\u03b2\n)\n returns an \nUncertainScalarBetaPrimeDistributed\n instance.\n\n\nUncertainValue\n(\nGamma\n,\n \n\u03b1\n,\n \n\u03b8\n)\n returns an \nUncertainScalarGammaDistributed\n instance.\n\n\nUncertainValue\n(\nFrechet\n,\n \n\u03b1\n,\n \n\u03b8\n)\n returns an \nUncertainScalarFrechetDistributed\n instance.\n\n\nUncertainValue\n(\nBinomial\n,\n \nn\n,\n \np\n)\n returns an \nUncertainScalarBinomialDistributed\n instance.\n\n\n\n\nKeyword arguments\n\n\n\n\nn\n\u03c3\n: If \ndistribution\n \n:\n \nDistributions\n.\nNormal\n, then how many standard   deviations away from \n\u03bc\n does \nlower\n and \nupper\n (i.e. both, because   they are the same distance away from \n\u03bc\n) represent?\n\n\ntolerance\n: A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   (\nupper\n \n-\n \nlower\n \n \nthreshold\n is required).\n\n\ntrunc_lower\n: Lower truncation bound for distributions with infinite   support. Defaults to \n-\nInf\n.\n\n\ntrunc_upper\n: Upper truncation bound for distributions with infinite   support. Defaults to \nInf\n.\n\n\n\n\nExamples\n\n\nNormal distribution\n\n\nNormal distributions are formed by using the constructor \nUncertainValue\n(\n\u03bc\n,\n \n\u03c3\n,\n \nNormal\n;\n \nkwargs\n...)\n. This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# A normal distribution with mean = 2.3 and standard deviation 0.3.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n)\n\n\n\n# A normal distribution with mean 2.3 and standard deviation 0.3/2.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n,\n \nn\u03c3\n \n=\n \n2\n)\n\n\n\n# A normal distribution with mean 2.3 and standard deviation = 0.3,\n\n\ntruncated\n \nto\n \nthe\n \ninterval\n \n`[1, 3]`\n.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n,\n \ntrunc_lower\n \n=\n \n1.0\n,\n \ntrunc_upper\n \n=\n \n3.0\n)\n\n\n\n\n\n\n\nUniform distribution\n\n\nUniform distributions are formed using the \nUncertainValue\n(\nlower\n,\n \nupper\n,\n \nUniform\n)\n constructor.\n\n\n1\n2\n#  A uniform distribution on `[2, 3]`\n\n\nUncertainValue\n(\n-\n2\n,\n \n3\n,\n \nUniform\n)\n\n\n\n\n\n\n\nsource\n\n\n\n\nThree-parameter distributions\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nUncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\ndistribution\n::\nType\n{\nD\n},\n \na\n::\nT1\n,\n \nb\n::\nT2\n,\n \nc\n::\nT3\n;\n\n    \nkwargs\n...\n)\n \nwhere\n \n{\nT1\n:\nNumber\n,\n \nT2\n:\nNumber\n,\n \nT3\n:\nNumber\n,\n \nD\n:\nDistribution\n}\n\n\n\n\n\n\n\nConstructor for three-parameter distributions\n\n\nCurrently implemented distributions are \nBetaBinomial\n.\n\n\nArguments\n\n\n\n\na\n, \nb\n, \nc\n: Generic parameters whose meaning varies depending   on what \ndistribution\n is provided. See the list below.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions\n.\njl\n.\n\n\n\n\nPrecisely what \na\n, \nb\n and \nc\n are depends on which distribution is provided.\n\n\n\n\nUncertainValue\n(\nBetaBinomial\n,\n \nn\n,\n \n\u03b1\n,\n \n\u03b2\n)\n returns an \nUncertainScalarBetaBinomialDistributed\n instance.\n\n\n\n\nKeyword arguments\n\n\n\n\nn\n\u03c3\n: If \ndistribution\n \n:\n \nDistributions\n.\nNormal\n, then how many standard   deviations away from \n\u03bc\n does \nlower\n and \nupper\n (i.e. both, because   they are the same distance away from \n\u03bc\n) represent?\n\n\ntolerance\n: A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   (\nupper\n \n-\n \nlower\n \n \nthreshold\n is required).\n\n\ntrunc_lower\n: Lower truncation bound for distributions with infinite   support. Defaults to \n-\nInf\n.\n\n\ntrunc_upper\n: Upper truncation bound for distributions with infinite   support. Defaults to \nInf\n.\n\n\n\n\nExamples\n\n\nBetaBinomial distribution\n\n\nNormal distributions are formed by using the constructor \nUncertainValue\n(\n\u03bc\n,\n \n\u03c3\n,\n \nNormal\n;\n \nkwargs\n...)\n. This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).\n\n\n1\n2\n3\n# A beta binomial distribution with n = 100 trials and parameters \u03b1 = 2.3 and\n\n\n# \u03b2 = 5\n\n\nUncertainValue\n(\n100\n,\n \n2.3\n,\n \n5\n,\n \nBetaBinomial\n)\n\n\n\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\n\n\nUniform\n\n\n1\n2\n# Uncertain value generated by a uniform distribution on [-5.0, 5.1].\n\n\nuv\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n-\n5.0\n,\n \n5.1\n)\n\n\n\n\n\n\n\n\nNormal\n\n\n1\n2\n3\n# Uncertain value generated by a normal distribution with parameters \u03bc = -2 and\n\n\n# \u03c3 = 0.5.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n2\n,\n \n0.5\n)\n\n\n\n\n\n\n\n\nGamma\n\n\n1\n2\n3\n# Uncertain value generated by a gamma distribution with parameters \u03b1 = 2.2\n\n\n# and \u03b8 = 3.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2.2\n,\n \n3\n)\n\n\n\n\n\n\n\n\nBeta\n\n\n1\n2\n3\n# Uncertain value generated by a beta distribution with parameters \u03b1 = 1.5\n\n\n# and \u03b2 = 3.5\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n1.5\n,\n \n3.5\n)\n\n\n\n\n\n\n\n\nBetaPrime\n\n\n1\n2\n3\n# Uncertain value generated by a beta prime distribution with parameters \u03b1 = 1.7\n\n\n# and \u03b2 = 3.2\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n1.7\n,\n \n3.2\n)\n\n\n\n\n\n\n\n\nFr\u00e9chet\n\n\n1\n2\n3\n# Uncertain value generated by a Fr\u00e9chet distribution with parameters \u03b1 = 2.1\n\n\n# and \u03b8 = 4\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n2.1\n,\n \n4\n)\n\n\n\n\n\n\n\n\nBinomial\n\n\n1\n2\n3\n# Uncertain value generated by binomial distribution with n = 28 trials and\n\n\n# probability p = 0.2 of success in individual trials.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBinomial\n,\n \n28\n,\n \n0.2\n)\n\n\n\n\n\n\n\n\nBetaBinomial\n\n\n1\n2\n3\n# Creates an uncertain value generated by a beta-binomial distribution with\n\n\n# n = 28 trials, and parameters \u03b1 = 1.5 and \u03b2 = 3.5.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBetaBinomial\n,\n \n28\n,\n \n3.3\n,\n \n4.4\n)", 
            "title": "Theoretical distributions with known parameters"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/#supported_distributions", 
            "text": "Supported distributions are  Uniform ,  Normal ,  Gamma ,  Beta ,  BetaPrime ,  Frechet ,  Binomial ,  BetaBinomial  (more distributions will be added in the future!).", 
            "title": "Supported distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/#constructors", 
            "text": "There are two constructors that creates uncertain values represented by theoretical distributions. Parameters are provided to the constructor in the same order as for constructing the equivalent distributions in  Distributions . jl .  Uncertain values represented by theoretical distributions may be constructed using the two-parameter or three-parameter constructors  UncertainValue ( d :: Type { D } ,   a : Number ,   b : Number )  or  UncertainValue ( d :: Type { D } ,   a : Number ,   b : Number ,   c : Number )  (see below).", 
            "title": "Constructors"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/#two-parameter_distributions", 
            "text": "#  UncertainData . UncertainValues . UncertainValue     Method .  1\n2 UncertainValue ( distribution :: Type { D },   a :: T1 ,   b :: T2 ; \n     kwargs ... )   where   { T1 : Number ,   T2   :   Number ,   D : Distribution }    Constructor for two-parameter distributions  UncertainValue s are currently implemented for the following two-parameter distributions:  Uniform ,  Normal ,  Binomial ,  Beta ,  BetaPrime ,  Gamma , and  Frechet .  Arguments   a ,  b : Generic parameters whose meaning varies depending   on what  distribution  is provided. See the list below.  distribution : A valid univariate distribution from  Distributions . jl .   Precisely what   a  and  b  are depends on which distribution is provided.   UncertainValue ( Normal ,   \u03bc ,   \u03c3 )  returns an  UncertainScalarNormallyDistributed  instance.  UncertainValue ( Uniform ,   lower ,   upper )  returns an  UncertainScalarUniformlyDistributed  instance.  UncertainValue ( Beta ,   \u03b1 ,   \u03b2 )  returns an  UncertainScalarBetaDistributed  instance.  UncertainValue ( BetaPrime ,   \u03b1 ,   \u03b2 )  returns an  UncertainScalarBetaPrimeDistributed  instance.  UncertainValue ( Gamma ,   \u03b1 ,   \u03b8 )  returns an  UncertainScalarGammaDistributed  instance.  UncertainValue ( Frechet ,   \u03b1 ,   \u03b8 )  returns an  UncertainScalarFrechetDistributed  instance.  UncertainValue ( Binomial ,   n ,   p )  returns an  UncertainScalarBinomialDistributed  instance.   Keyword arguments   n \u03c3 : If  distribution   :   Distributions . Normal , then how many standard   deviations away from  \u03bc  does  lower  and  upper  (i.e. both, because   they are the same distance away from  \u03bc ) represent?  tolerance : A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   ( upper   -   lower     threshold  is required).  trunc_lower : Lower truncation bound for distributions with infinite   support. Defaults to  - Inf .  trunc_upper : Upper truncation bound for distributions with infinite   support. Defaults to  Inf .   Examples  Normal distribution  Normal distributions are formed by using the constructor  UncertainValue ( \u03bc ,   \u03c3 ,   Normal ;   kwargs ...) . This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).  1\n2\n3\n4\n5\n6\n7\n8\n9 # A normal distribution with mean = 2.3 and standard deviation 0.3.  UncertainValue ( 2.3 ,   0.3 ,   Normal )  # A normal distribution with mean 2.3 and standard deviation 0.3/2.  UncertainValue ( 2.3 ,   0.3 ,   Normal ,   n\u03c3   =   2 )  # A normal distribution with mean 2.3 and standard deviation = 0.3,  truncated   to   the   interval   `[1, 3]` .  UncertainValue ( 2.3 ,   0.3 ,   Normal ,   trunc_lower   =   1.0 ,   trunc_upper   =   3.0 )    Uniform distribution  Uniform distributions are formed using the  UncertainValue ( lower ,   upper ,   Uniform )  constructor.  1\n2 #  A uniform distribution on `[2, 3]`  UncertainValue ( - 2 ,   3 ,   Uniform )    source", 
            "title": "Two-parameter distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/#three-parameter_distributions", 
            "text": "#  UncertainData . UncertainValues . UncertainValue     Method .  1\n2 UncertainValue ( distribution :: Type { D },   a :: T1 ,   b :: T2 ,   c :: T3 ; \n     kwargs ... )   where   { T1 : Number ,   T2 : Number ,   T3 : Number ,   D : Distribution }    Constructor for three-parameter distributions  Currently implemented distributions are  BetaBinomial .  Arguments   a ,  b ,  c : Generic parameters whose meaning varies depending   on what  distribution  is provided. See the list below.  distribution : A valid univariate distribution from  Distributions . jl .   Precisely what  a ,  b  and  c  are depends on which distribution is provided.   UncertainValue ( BetaBinomial ,   n ,   \u03b1 ,   \u03b2 )  returns an  UncertainScalarBetaBinomialDistributed  instance.   Keyword arguments   n \u03c3 : If  distribution   :   Distributions . Normal , then how many standard   deviations away from  \u03bc  does  lower  and  upper  (i.e. both, because   they are the same distance away from  \u03bc ) represent?  tolerance : A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   ( upper   -   lower     threshold  is required).  trunc_lower : Lower truncation bound for distributions with infinite   support. Defaults to  - Inf .  trunc_upper : Upper truncation bound for distributions with infinite   support. Defaults to  Inf .   Examples  BetaBinomial distribution  Normal distributions are formed by using the constructor  UncertainValue ( \u03bc ,   \u03c3 ,   Normal ;   kwargs ...) . This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).  1\n2\n3 # A beta binomial distribution with n = 100 trials and parameters \u03b1 = 2.3 and  # \u03b2 = 5  UncertainValue ( 100 ,   2.3 ,   5 ,   BetaBinomial )    source", 
            "title": "Three-parameter distributions"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_theoreticaldistributions/#examples", 
            "text": "Uniform  1\n2 # Uncertain value generated by a uniform distribution on [-5.0, 5.1].  uv   =   UncertainValue ( Uniform ,   - 5.0 ,   5.1 )     Normal  1\n2\n3 # Uncertain value generated by a normal distribution with parameters \u03bc = -2 and  # \u03c3 = 0.5.  uv   =   UncertainValue ( Normal ,   - 2 ,   0.5 )     Gamma  1\n2\n3 # Uncertain value generated by a gamma distribution with parameters \u03b1 = 2.2  # and \u03b8 = 3.  uv   =   UncertainValue ( Gamma ,   2.2 ,   3 )     Beta  1\n2\n3 # Uncertain value generated by a beta distribution with parameters \u03b1 = 1.5  # and \u03b2 = 3.5  uv   =   UncertainValue ( Beta ,   1.5 ,   3.5 )     BetaPrime  1\n2\n3 # Uncertain value generated by a beta prime distribution with parameters \u03b1 = 1.7  # and \u03b2 = 3.2  uv   =   UncertainValue ( Beta ,   1.7 ,   3.2 )     Fr\u00e9chet  1\n2\n3 # Uncertain value generated by a Fr\u00e9chet distribution with parameters \u03b1 = 2.1  # and \u03b8 = 4  uv   =   UncertainValue ( Beta ,   2.1 ,   4 )     Binomial  1\n2\n3 # Uncertain value generated by binomial distribution with n = 28 trials and  # probability p = 0.2 of success in individual trials.  uv   =   UncertainValue ( Binomial ,   28 ,   0.2 )     BetaBinomial  1\n2\n3 # Creates an uncertain value generated by a beta-binomial distribution with  # n = 28 trials, and parameters \u03b1 = 1.5 and \u03b2 = 3.5.  uv   =   UncertainValue ( BetaBinomial ,   28 ,   3.3 ,   4.4 )", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_fitted/", 
            "text": "For data values with histograms close to some known distribution, the user may choose to represent the data by fitting a theoretical distribution to the values. This will only work well if the histogram closely resembles a theoretical distribution.\n\n\n\n\nConstructor\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nUncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\nempiricaldata\n::\nAbstractVector\n{\nT\n},\n\n    \nd\n::\nType\n{\nD\n})\n \nwhere\n \n{\nD\n \n:\n \nDistribution\n}\n\n\n\n\n\n\n\nConstructor for empirical distributions.\n\n\nFit a distribution of type \nd\n to the data and use that as the representation of the empirical distribution. Calls \nDistributions\n.\nfit\n behind the scenes.\n\n\nArguments\n\n\n\n\nempiricaldata\n: The data for which to fit the \ndistribution\n.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions\n.\njl\n.\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\n\n\nUniform\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nUniform\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Define an uncertain value by fitting a uniform distribution to the sample.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nNormal\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted normal distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nGamma\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,\n\n\n# \u03b8 = 5.2.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n2.1\n,\n \n5.2\n),\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted gamma distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nIn these examples we're trying to fit the same distribution to our sample as the distribution from which we draw the sample. Thus, we will get good fits. In real applications, make sure to always visually investigate the histogram of your data!\n\n\n\n\nBeware: fitting distributions may lead to nonsensical results!\n\n\nIn a less contrived example, we may try to fit a beta distribution to a sample generated from a gamma distribution.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,\n\n\n# \u03b8 = 5.2.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n2.1\n,\n \n5.2\n),\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted beta distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\nThis is obviously not a good idea. Always visualise your distribution before deciding on which distribution to fit! You won't get any error messages if you try to fit a distribution that does not match your data.\n\n\nIf the data do not follow an obvious theoretical distribution, it is better to use kernel density estimation to define the uncertain value.", 
            "title": "Theoretical distributions with fitted parameters"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_fitted/#constructor", 
            "text": "#  UncertainData . UncertainValues . UncertainValue     Method .  1\n2 UncertainValue ( empiricaldata :: AbstractVector { T }, \n     d :: Type { D })   where   { D   :   Distribution }    Constructor for empirical distributions.  Fit a distribution of type  d  to the data and use that as the representation of the empirical distribution. Calls  Distributions . fit  behind the scenes.  Arguments   empiricaldata : The data for which to fit the  distribution .  distribution : A valid univariate distribution from  Distributions . jl .   source", 
            "title": "Constructor"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_fitted/#examples", 
            "text": "Uniform   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Uniform ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Define an uncertain value by fitting a uniform distribution to the sample.  uv   =   UncertainValue ( Uniform ,   some_sample )     Normal   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Represent the uncertain value by a fitted normal distribution.  uv   =   UncertainValue ( Normal ,   some_sample )     Gamma  1\n2\n3\n4\n5\n6\n7\n8 using   Distributions ,   UncertainData  # Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,  # \u03b8 = 5.2.  some_sample   =   rand ( Gamma ( 2.1 ,   5.2 ),   1000 )  # Represent the uncertain value by a fitted gamma distribution.  uv   =   UncertainValue ( Gamma ,   some_sample )     In these examples we're trying to fit the same distribution to our sample as the distribution from which we draw the sample. Thus, we will get good fits. In real applications, make sure to always visually investigate the histogram of your data!", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_values/uncertainvalues_fitted/#beware_fitting_distributions_may_lead_to_nonsensical_results", 
            "text": "In a less contrived example, we may try to fit a beta distribution to a sample generated from a gamma distribution.  1\n2\n3\n4\n5\n6\n7\n8 using   Distributions ,   UncertainData  # Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,  # \u03b8 = 5.2.  some_sample   =   rand ( Gamma ( 2.1 ,   5.2 ),   1000 )  # Represent the uncertain value by a fitted beta distribution.  uv   =   UncertainValue ( Beta ,   some_sample )    This is obviously not a good idea. Always visualise your distribution before deciding on which distribution to fit! You won't get any error messages if you try to fit a distribution that does not match your data.  If the data do not follow an obvious theoretical distribution, it is better to use kernel density estimation to define the uncertain value.", 
            "title": "Beware: fitting distributions may lead to nonsensical results!"
        }, 
        {
            "location": "/uncertain_values/certainvalue/", 
            "text": "The \nCertainValue\n allows representation of values with no uncertainty. It behaves  just as a scalar, but can be mixed with uncertain values when performing  \nmathematical operations\n and  \nresampling\n. \n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nCertainValue\n \n \nType\n.\n\n\n1\nCertainValue\n\n\n\n\n\n\n\nA simple wrapper type for values with no uncertainty (i.e. represented by a scalar).\n\n\nExamples\n\n\nThe two following ways of constructing values without uncertainty are equivalent. \n\n\n1\nu1\n,\n \nu2\n \n=\n \nCertainValue\n(\n2.2\n),\n \nCertainValue\n(\n6\n)\n\n\n\n\n\n\n\n1\nw1\n,\n \nw2\n \n=\n \nUncertainValue\n(\n2.2\n),\n \nUncertainValue\n(\n6\n)\n\n\n\n\n\n\n\nsource", 
            "title": "Values without uncertainty"
        }, 
        {
            "location": "/uncertain_values/certainvalue/#documentation", 
            "text": "#  UncertainData . UncertainValues . CertainValue     Type .  1 CertainValue    A simple wrapper type for values with no uncertainty (i.e. represented by a scalar).  Examples  The two following ways of constructing values without uncertainty are equivalent.   1 u1 ,   u2   =   CertainValue ( 2.2 ),   CertainValue ( 6 )    1 w1 ,   w2   =   UncertainValue ( 2.2 ),   UncertainValue ( 6 )    source", 
            "title": "Documentation"
        }, 
        {
            "location": "/uncertain_values/populations/", 
            "text": "The \nUncertainScalarPopulation\n type allows representation of an uncertain scalar  represented by a population of values who will be sampled according to a vector of  explicitly provided probabilities. Think of it as an explicit kernel density estimate. \n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nUncertainValues\n.\nUncertainScalarPopulation\n \n \nType\n.\n\n\n1\nUncertainScalarPopulation\n\n\n\n\n\n\n\nAn uncertain value represented by a population for which the probabilities of the values  are dictated by a set of weights. The weights are normalized by default.\n\n\nExamples\n\n\nThe two following ways of constructing populations are equivalent. \n\n\n1\n2\n3\nvalues\n \n=\n \nrand\n(\n1\n:\n50\n,\n \n100\n)\n\n\nweights\n \n=\n \nrand\n(\n100\n)\n\n\npopulation\n \n=\n \nUncertainValue\n(\nvalues\n,\n \nweights\n)\n\n\n\n\n\n\n\n1\n2\n3\nvalues\n \n=\n \nrand\n(\n1\n:\n50\n,\n \n100\n)\n\n\nweights\n \n=\n \nrand\n(\n100\n)\n\n\npopulation\n \n=\n \nUncertainScalarPopulation\n(\nvalues\n,\n \nweights\n)\n\n\n\n\n\n\n\nsource", 
            "title": "Populations"
        }, 
        {
            "location": "/uncertain_values/populations/#documentation", 
            "text": "#  UncertainData . UncertainValues . UncertainScalarPopulation     Type .  1 UncertainScalarPopulation    An uncertain value represented by a population for which the probabilities of the values  are dictated by a set of weights. The weights are normalized by default.  Examples  The two following ways of constructing populations are equivalent.   1\n2\n3 values   =   rand ( 1 : 50 ,   100 )  weights   =   rand ( 100 )  population   =   UncertainValue ( values ,   weights )    1\n2\n3 values   =   rand ( 1 : 50 ,   100 )  weights   =   rand ( 100 )  population   =   UncertainScalarPopulation ( values ,   weights )    source", 
            "title": "Documentation"
        }, 
        {
            "location": "/uncertain_values/merging/", 
            "text": "Because all uncertainties are handled using a resampling approach, it is trivial to  \ncombine\n uncertain values of different types into a single  uncertain value. \n\n\n\n\nWithout weights\n\n\nWhen no weights are provided, the combined value is computed  by resampling each of the \nN\n uncertain values \nn\n/\nN\n times, then combining using kernel density estimation. \n\n\n#\n\n\nUncertainData\n.\ncombine\n \n \nMethod\n.\n\n\n1\n2\ncombine\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n};\n \nn\n \n=\n \n10000\n*\nlength\n(\nuvals\n),\n \n    \nbw\n::\nUnion\n{\nNothing\n,\n \nReal\n}\n \n=\n \nnothing\n)\n\n\n\n\n\n\n\nCombine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in \nuvals\n, \nn\n times  each,  then pooling these draws together. Finally, a kernel density estimate to the final distribution is computed over those draws. \n\n\nThe KDE bandwidth is controlled by \nbw\n. By default, \nbw\n \n=\n \nnothing\n; in this case,  the bandwidth is determined using the \nKernelDensity\n.\ndefault_bandwidth\n function.\n\n\n\n\nTip\n\n\nFor very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.\n\n\n\n\nExample\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1\n,\n \n0.3\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n3.7\n,\n \n0.8\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\ncombine\n(\nuvals\n)\n\n\ncombine\n(\nuvals\n,\n \nn\n \n=\n \n20000\n)\n \n# adjust number of total draws\n\n\n\n\n\n\n\nsource\n\n\nWeights dictating the relative contribution of each  uncertain value into the combined value can also be provided. \ncombine\n works  with \nProbabilityWeights\n, \nAnalyticWeights\n,  \nFrequencyWeights\n and the generic \nWeights\n. \n\n\nBelow shows an example of combining \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nv1\n \n=\n \nUncertainValue\n(\nrand\n(\n1000\n))\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n3.7\n,\n \n0.8\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n]\n\n\n\np\n \n=\n \nplot\n(\ntitle\n \n=\n \nL\ndistributions \\,\\, with \\,\\, overlapping \\,\\, supports\n)\n\n\nplot!\n(\nv1\n,\n \nlabel\n \n=\n \nL\nv_1\n,\n \nls\n \n=\n \n:\ndash\n)\n\n\nplot!\n(\nv2\n,\n \nlabel\n \n=\n \nL\nv_2\n,\n \nls\n \n=\n \n:\ndot\n)\n\n\nvline!\n(\nv3\n.\nvalues\n,\n \nlabel\n \n=\n \nL\nv_3\n)\n \n# plot each possible state as vline\n\n\nplot!\n(\nv4\n,\n \nlabel\n \n=\n \nL\nv_4\n)\n\n\n\npcombined\n \n=\n \nplot\n(\ncombine\n(\nuvals\n),\n \ntitle\n \n=\n \nL\nmerge(v_1, v_2, v_3, v_4)\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n2\n)\n\n\n\nplot\n(\np\n,\n \npcombined\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nlink\n \n=\n \n:\nx\n,\n \nylabel\n \n=\n \nDensity\n)\n\n\n\n\n\n\n\n\n\n\n\nWith weights\n\n\nWeights\n, \nProbabilityWeights\n and  \nAnalyticWeights\n are functionally the same. Either  may be used depending on whether the weights are assigned subjectively or quantitatively.  With \nFrequencyWeights\n, it is possible to control the exact number of draws from each  uncertain value that goes into the draw pool before performing KDE.\n\n\n\n\nProbabilityWeights\n\n\n#\n\n\nUncertainData\n.\ncombine\n \n \nMethod\n.\n\n\n1\n2\n3\ncombine\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n},\n \nweights\n::\nProbabilityWeights\n;\n \n    \nn\n \n=\n \n10000\n*\nlength\n(\nuvals\n),\n \n    \nbw\n::\nUnion\n{\nNothing\n,\n \nReal\n}\n \n=\n \nnothing\n)\n\n\n\n\n\n\n\nCombine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in \nuvals\n proportionally to the provided  relative analytic \nweights\n indicating their relative importance (these are normalised by  default, so don't need to sum to 1), then pooling these draws together. Finally, a kernel  density estimate to the final distribution is computed over the \nn\n total draws.\n\n\nProviding \nProbabilityWeights\n leads to the exact same behaviour as for \nAnalyticWeights\n,  but may be more appropriote when, for example, weights have been determined  quantitatively. \n\n\nThe KDE bandwidth is controlled by \nbw\n. By default, \nbw\n \n=\n \nnothing\n; in this case,  the bandwidth is determined using the \nKernelDensity\n.\ndefault_bandwidth\n function.\n\n\n\n\nTip\n\n\nFor very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.\n\n\n\n\nExample\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1\n,\n \n0.3\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n3.7\n,\n \n0.8\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\n# Two difference syntax options\n\n\ncombine\n(\nuvals\n,\n \nProbabilityWeights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]))\n\n\ncombine\n(\nuvals\n,\n \npweights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]),\n \nn\n \n=\n \n20000\n)\n \n# adjust number of total draws\n\n\n\n\n\n\n\nsource\n\n\nFor example: \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nv1\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\n4\n:\n0.25\n:\n6\n,\n \n1000\n),\n \nbandwidth\n \n=\n \n0.02\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n8\n,\n \n0.4\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\np\n \n=\n \nplot\n(\ntitle\n \n=\n \nL\ndistributions \\,\\, with \\,\\, overlapping \\,\\, supports\n)\n\n\nplot!\n(\nv1\n,\n \nlabel\n \n=\n \nL\nv_1: KDE \\, over \\, empirical \\, distribution\n,\n \nls\n \n=\n \n:\ndash\n)\n\n\nplot!\n(\nv2\n,\n \nlabel\n \n=\n \nL\nv_2: Normal(0.8, 0.4)\n,\n \nls\n \n=\n \n:\ndot\n)\n\n\n# plot each possible state as vline\n\n\nvline!\n(\nv3\n.\nvalues\n,\n \n    \nlabel\n \n=\n \nL\nv_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4]\n)\n \n\nplot!\n(\nv4\n,\n \nlabel\n \n=\n \nL\nv_4: \\, Gamma(8, 0.4)\n)\n\n\n\npcombined\n \n=\n \nplot\n(\n\n    \ncombine\n(\nuvals\n,\n \nProbabilityWeights\n([\n0.1\n,\n \n0.3\n,\n \n0.02\n,\n \n0.5\n]),\n \nn\n \n=\n \n100000\n,\n \nbw\n \n=\n \n0.05\n),\n \n    \ntitle\n \n=\n \nL\ncombine([v_1, v_2, v_3, v_4], ProbabilityWeights([0.1, 0.3, 0.02, 0.5])\n,\n \n    \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n2\n)\n\n\n\nplot\n(\np\n,\n \npcombined\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nsize\n \n=\n \n(\n800\n,\n \n600\n),\n \n    \nlink\n \n=\n \n:\nx\n,\n \n    \nylabel\n \n=\n \nDensity\n,\n\n    \ntickfont\n \n=\n \nfont\n(\n12\n),\n\n    \nlegendfont\n \n=\n \nfont\n(\n8\n),\n \nfg_legend\n \n=\n \n:\ntransparent\n,\n \nbg_legend\n \n=\n \n:\ntransparent\n)\n\n\n\n\n\n\n\n\n\n\n\nAnalyticWeights\n\n\n#\n\n\nUncertainData\n.\ncombine\n \n \nMethod\n.\n\n\n1\n2\n3\ncombine\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n},\n \nweights\n::\nAnalyticWeights\n;\n \n    \nn\n \n=\n \n10000\n*\nlength\n(\nuvals\n),\n \n    \nbw\n::\nUnion\n{\nNothing\n,\n \nReal\n}\n \n=\n \nnothing\n)\n\n\n\n\n\n\n\nCombine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in \nuvals\n proportionally to the provided  relative probability \nweights\n (these are normalised by default, so don't need  to sum to 1), then pooling these draws together. Finally, a kernel density  estimate to the final distribution is computed over the \nn\n total draws.\n\n\nProviding \nAnalyticWeights\n leads to the exact same behaviour as for \nProbabilityWeights\n, but may be more appropriote when relative importance weights are assigned subjectively,  and not based on quantitative evidence.\n\n\nThe KDE bandwidth is controlled by \nbw\n. By default, \nbw\n \n=\n \nnothing\n; in this case,  the bandwidth is determined using the \nKernelDensity\n.\ndefault_bandwidth\n function.\n\n\n\n\nTip\n\n\nFor very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.\n\n\n\n\nExample\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1\n,\n \n0.3\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n3.7\n,\n \n0.8\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\n# Two difference syntax options\n\n\ncombine\n(\nuvals\n,\n \nAnalyticWeights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]))\n\n\ncombine\n(\nuvals\n,\n \naweights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]),\n \nn\n \n=\n \n20000\n)\n \n# adjust number of total draws\n\n\n\n\n\n\n\nsource\n\n\nFor example:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nv1\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\n4\n:\n0.25\n:\n6\n,\n \n1000\n),\n \nbandwidth\n \n=\n \n0.02\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n8\n,\n \n0.4\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\np\n \n=\n \nplot\n(\ntitle\n \n=\n \nL\ndistributions \\,\\, with \\,\\, overlapping \\,\\, supports\n)\n\n\nplot!\n(\nv1\n,\n \nlabel\n \n=\n \nL\nv_1: KDE \\, over \\, empirical \\, distribution\n,\n \nls\n \n=\n \n:\ndash\n)\n\n\nplot!\n(\nv2\n,\n \nlabel\n \n=\n \nL\nv_2: Normal(0.8, 0.4)\n,\n \nls\n \n=\n \n:\ndot\n)\n\n\nvline!\n(\nv3\n.\nvalues\n,\n \nlabel\n \n=\n \nL\nv_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4]\n)\n \n# plot each possible state as vline\n\n\nplot!\n(\nv4\n,\n \nlabel\n \n=\n \nL\nv_4: \\, Gamma(8, 0.4)\n)\n\n\n\npcombined\n \n=\n \nplot\n(\ncombine\n(\nuvals\n,\n \nAnalyticWeights\n([\n0.1\n,\n \n0.3\n,\n \n0.02\n,\n \n0.5\n]),\n \nn\n \n=\n \n100000\n,\n \nbw\n \n=\n \n0.05\n),\n \n    \ntitle\n \n=\n \nL\ncombine([v_1, v_2, v_3, v_4], AnalyticWeights([0.1, 0.3, 0.02, 0.5])\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n2\n)\n\n\n\nplot\n(\np\n,\n \npcombined\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nsize\n \n=\n \n(\n800\n,\n \n600\n),\n \n    \nlink\n \n=\n \n:\nx\n,\n \n    \nylabel\n \n=\n \nDensity\n,\n\n    \ntickfont\n \n=\n \nfont\n(\n12\n),\n\n    \nlegendfont\n \n=\n \nfont\n(\n8\n),\n \nfg_legend\n \n=\n \n:\ntransparent\n,\n \nbg_legend\n \n=\n \n:\ntransparent\n)\n\n\n\n\n\n\n\n\n\n\n\nGeneric Weights\n\n\n#\n\n\nUncertainData\n.\ncombine\n \n \nMethod\n.\n\n\n1\n2\n3\ncombine\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n},\n \nweights\n::\nWeights\n;\n \n    \nn\n \n=\n \n10000\n*\nlength\n(\nuvals\n),\n \n    \nbw\n::\nUnion\n{\nNothing\n,\n \nReal\n}\n \n=\n \nnothing\n)\n\n\n\n\n\n\n\nCombine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in \nuvals\n proportionally to the provided \nweights\n  (these are normalised by default, so don't need to sum to 1), then pooling these draws  together. Finally, a kernel density estimate to the final distribution is computed over  the \nn\n total draws.\n\n\nProviding \nWeights\n leads to the exact same behaviour as for \nProbabilityWeights\n and  \nAnalyticalWeights\n.\n\n\nThe KDE bandwidth is controlled by \nbw\n. By default, \nbw\n \n=\n \nnothing\n; in this case,  the bandwidth is determined using the \nKernelDensity\n.\ndefault_bandwidth\n function.\n\n\n\n\nTip\n\n\nFor very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.\n\n\n\n\nExample\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1\n,\n \n0.3\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n3.7\n,\n \n0.8\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\n# Two difference syntax options\n\n\ncombine\n(\nuvals\n,\n \nWeights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]))\n\n\ncombine\n(\nuvals\n,\n \nweights\n([\n0.2\n,\n \n0.1\n,\n \n0.3\n,\n \n0.2\n]),\n \nn\n \n=\n \n20000\n)\n \n# adjust number of total draws\n\n\n\n\n\n\n\nsource\n\n\nFor example: \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nv1\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\n4\n:\n0.25\n:\n6\n,\n \n1000\n),\n \nbandwidth\n \n=\n \n0.01\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n8\n,\n \n0.4\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\np\n \n=\n \nplot\n(\ntitle\n \n=\n \nL\ndistributions \\,\\, with \\,\\, overlapping \\,\\, supports\n)\n\n\nplot!\n(\nv1\n,\n \nlabel\n \n=\n \nL\nv_1: KDE \\, over \\, empirical \\, distribution\n,\n \nls\n \n=\n \n:\ndash\n)\n\n\nplot!\n(\nv2\n,\n \nlabel\n \n=\n \nL\nv_2: Normal(0.8, 0.4)\n,\n \nls\n \n=\n \n:\ndot\n)\n\n\n# plot each possible state as vline\n\n\nvline!\n(\nv3\n.\nvalues\n,\n \n    \nlabel\n \n=\n \nL\nv_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4]\n)\n \n\nplot!\n(\nv4\n,\n \nlabel\n \n=\n \nL\nv_4: \\, Gamma(8, 0.4)\n)\n\n\n\npcombined\n \n=\n \nplot\n(\ncombine\n(\nuvals\n,\n \nWeights\n([\n0.1\n,\n \n0.15\n,\n \n0.1\n,\n \n0.1\n]),\n \nn\n \n=\n \n100000\n,\n \nbw\n \n=\n \n0.02\n),\n \n    \ntitle\n \n=\n \nL\ncombine([v_1, v_2, v_3, v_4],  Weights([0.1, 0.15, 0.1, 0.1]))\n,\n \n    \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n2\n)\n\n\n\nplot\n(\np\n,\n \npcombined\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nsize\n \n=\n \n(\n800\n,\n \n600\n),\n \n    \nlink\n \n=\n \n:\nx\n,\n \n    \nylabel\n \n=\n \nDensity\n,\n\n    \ntickfont\n \n=\n \nfont\n(\n12\n),\n\n    \nlegendfont\n \n=\n \nfont\n(\n8\n),\n \nfg_legend\n \n=\n \n:\ntransparent\n,\n \nbg_legend\n \n=\n \n:\ntransparent\n)\n\n\n\n\n\n\n\n\n\n\n\nFrequencyWeights\n\n\nUsing \nFrequencyWeights\n, one may specify the number of times each of the uncertain values  should be sampled to form the pooled resampled draws on which the final kernel density  estimate is performed.\n\n\n#\n\n\nUncertainData\n.\ncombine\n \n \nMethod\n.\n\n\n1\n2\ncombine\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n},\n \nweights\n::\nFrequencyWeights\n;\n\n    \nbw\n::\nUnion\n{\nNothing\n,\n \nReal\n}\n \n=\n \nnothing\n)\n\n\n\n\n\n\n\nCombine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in \nuvals\n according to their relative frequencies (the absolute number of draws provided by \nweights\n). Finally, a kernel density  estimate to the final distribution is computed over the \nsum\n(\nweights\n)\n total draws.\n\n\nThe KDE bandwidth is controlled by \nbw\n. By default, \nbw\n \n=\n \nnothing\n; in this case,  the bandwidth is determined using the \nKernelDensity\n.\ndefault_bandwidth\n function.\n\n\n\n\nTip\n\n\nFor very wide and close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.\n\n\n\n\nExample\n\n\nv1 = UncertainValue(Normal, 1, 0.3) v2 = UncertainValue(Normal, 0.8, 0.4) v3 = UncertainValue([rand() for i = 1:3], [0.3, 0.3, 0.4]) v4 = UncertainValue(Normal, 3.7, 0.8) uvals = [v1, v2, v3, v4];\n\n\nTwo difference syntax options\n\n\ncombine(uvals, FrequencyWeights([100, 500, 343, 7000])) combine(uvals, pweights([1410, 550, 223, 801]))\n\n\nsource\n\n\nFor example: \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nv1\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\n4\n:\n0.25\n:\n6\n,\n \n1000\n),\n \nbandwidth\n \n=\n \n0.01\n)\n\n\nv2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0.8\n,\n \n0.4\n)\n\n\nv3\n \n=\n \nUncertainValue\n([\nrand\n()\n \nfor\n \ni\n \n=\n \n1\n:\n3\n],\n \n[\n0.3\n,\n \n0.3\n,\n \n0.4\n])\n\n\nv4\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n8\n,\n \n0.4\n)\n\n\nuvals\n \n=\n \n[\nv1\n,\n \nv2\n,\n \nv3\n,\n \nv4\n];\n\n\n\np\n \n=\n \nplot\n(\ntitle\n \n=\n \nL\ndistributions \\,\\, with \\,\\, overlapping \\,\\, supports\n)\n\n\nplot!\n(\nv1\n,\n \nlabel\n \n=\n \nL\nv_1: KDE \\, over \\, empirical \\, distribution\n,\n \nls\n \n=\n \n:\ndash\n)\n\n\nplot!\n(\nv2\n,\n \nlabel\n \n=\n \nL\nv_2: Normal(0.8, 0.4)\n,\n \nls\n \n=\n \n:\ndot\n)\n\n\n# plot each possible state as vline\n\n\nvline!\n(\nv3\n.\nvalues\n,\n \n    \nlabel\n \n=\n \nL\nv_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4]\n)\n \n\nplot!\n(\nv4\n,\n \nlabel\n \n=\n \nL\nv_4: \\, Gamma(8, 0.4)\n)\n\n\n\npcombined\n \n=\n \nplot\n(\ncombine\n(\nuvals\n,\n \nFrequencyWeights\n([\n10000\n,\n \n20000\n,\n \n3000\n,\n \n5000\n]),\n \nbw\n \n=\n \n0.05\n),\n \n    \ntitle\n \n=\n \nL\ncombine([v_1, v_2, v_3, v_4], FrequencyWeights([10000, 20000, 3000, 5000])\n,\n \n    \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n2\n)\n\n\n\nplot\n(\np\n,\n \npcombined\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nsize\n \n=\n \n(\n800\n,\n \n600\n),\n \n    \nlink\n \n=\n \n:\nx\n,\n \n    \nylabel\n \n=\n \nDensity\n,\n\n    \ntickfont\n \n=\n \nfont\n(\n12\n),\n\n    \nlegendfont\n \n=\n \nfont\n(\n8\n),\n \nfg_legend\n \n=\n \n:\ntransparent\n,\n \nbg_legend\n \n=\n \n:\ntransparent\n)", 
            "title": "Combining"
        }, 
        {
            "location": "/uncertain_values/merging/#without_weights", 
            "text": "When no weights are provided, the combined value is computed  by resampling each of the  N  uncertain values  n / N  times, then combining using kernel density estimation.   #  UncertainData . combine     Method .  1\n2 combine ( uvals :: Vector { AbstractUncertainValue };   n   =   10000 * length ( uvals ),  \n     bw :: Union { Nothing ,   Real }   =   nothing )    Combine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in  uvals ,  n  times  each,  then pooling these draws together. Finally, a kernel density estimate to the final distribution is computed over those draws.   The KDE bandwidth is controlled by  bw . By default,  bw   =   nothing ; in this case,  the bandwidth is determined using the  KernelDensity . default_bandwidth  function.   Tip  For very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.   Example  1\n2\n3\n4\n5\n6\n7\n8 v1   =   UncertainValue ( Normal ,   1 ,   0.3 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Normal ,   3.7 ,   0.8 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  combine ( uvals )  combine ( uvals ,   n   =   20000 )   # adjust number of total draws    source  Weights dictating the relative contribution of each  uncertain value into the combined value can also be provided.  combine  works  with  ProbabilityWeights ,  AnalyticWeights ,   FrequencyWeights  and the generic  Weights .   Below shows an example of combining    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 v1   =   UncertainValue ( rand ( 1000 ))  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Normal ,   3.7 ,   0.8 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ]  p   =   plot ( title   =   L distributions \\,\\, with \\,\\, overlapping \\,\\, supports )  plot! ( v1 ,   label   =   L v_1 ,   ls   =   : dash )  plot! ( v2 ,   label   =   L v_2 ,   ls   =   : dot )  vline! ( v3 . values ,   label   =   L v_3 )   # plot each possible state as vline  plot! ( v4 ,   label   =   L v_4 )  pcombined   =   plot ( combine ( uvals ),   title   =   L merge(v_1, v_2, v_3, v_4) ,   lc   =   : black ,   lw   =   2 )  plot ( p ,   pcombined ,   layout   =   ( 2 ,   1 ),   link   =   : x ,   ylabel   =   Density )", 
            "title": "Without weights"
        }, 
        {
            "location": "/uncertain_values/merging/#with_weights", 
            "text": "Weights ,  ProbabilityWeights  and   AnalyticWeights  are functionally the same. Either  may be used depending on whether the weights are assigned subjectively or quantitatively.  With  FrequencyWeights , it is possible to control the exact number of draws from each  uncertain value that goes into the draw pool before performing KDE.", 
            "title": "With weights"
        }, 
        {
            "location": "/uncertain_values/merging/#probabilityweights", 
            "text": "#  UncertainData . combine     Method .  1\n2\n3 combine ( uvals :: Vector { AbstractUncertainValue },   weights :: ProbabilityWeights ;  \n     n   =   10000 * length ( uvals ),  \n     bw :: Union { Nothing ,   Real }   =   nothing )    Combine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in  uvals  proportionally to the provided  relative analytic  weights  indicating their relative importance (these are normalised by  default, so don't need to sum to 1), then pooling these draws together. Finally, a kernel  density estimate to the final distribution is computed over the  n  total draws.  Providing  ProbabilityWeights  leads to the exact same behaviour as for  AnalyticWeights ,  but may be more appropriote when, for example, weights have been determined  quantitatively.   The KDE bandwidth is controlled by  bw . By default,  bw   =   nothing ; in this case,  the bandwidth is determined using the  KernelDensity . default_bandwidth  function.   Tip  For very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.   Example  1\n2\n3\n4\n5\n6\n7\n8\n9 v1   =   UncertainValue ( Normal ,   1 ,   0.3 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Normal ,   3.7 ,   0.8 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  # Two difference syntax options  combine ( uvals ,   ProbabilityWeights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]))  combine ( uvals ,   pweights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]),   n   =   20000 )   # adjust number of total draws    source  For example:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 v1   =   UncertainValue ( UnivariateKDE ,   rand ( 4 : 0.25 : 6 ,   1000 ),   bandwidth   =   0.02 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Gamma ,   8 ,   0.4 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  p   =   plot ( title   =   L distributions \\,\\, with \\,\\, overlapping \\,\\, supports )  plot! ( v1 ,   label   =   L v_1: KDE \\, over \\, empirical \\, distribution ,   ls   =   : dash )  plot! ( v2 ,   label   =   L v_2: Normal(0.8, 0.4) ,   ls   =   : dot )  # plot each possible state as vline  vline! ( v3 . values ,  \n     label   =   L v_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4] )   plot! ( v4 ,   label   =   L v_4: \\, Gamma(8, 0.4) )  pcombined   =   plot ( \n     combine ( uvals ,   ProbabilityWeights ([ 0.1 ,   0.3 ,   0.02 ,   0.5 ]),   n   =   100000 ,   bw   =   0.05 ),  \n     title   =   L combine([v_1, v_2, v_3, v_4], ProbabilityWeights([0.1, 0.3, 0.02, 0.5]) ,  \n     lc   =   : black ,   lw   =   2 )  plot ( p ,   pcombined ,   layout   =   ( 2 ,   1 ),   size   =   ( 800 ,   600 ),  \n     link   =   : x ,  \n     ylabel   =   Density , \n     tickfont   =   font ( 12 ), \n     legendfont   =   font ( 8 ),   fg_legend   =   : transparent ,   bg_legend   =   : transparent )", 
            "title": "ProbabilityWeights"
        }, 
        {
            "location": "/uncertain_values/merging/#analyticweights", 
            "text": "#  UncertainData . combine     Method .  1\n2\n3 combine ( uvals :: Vector { AbstractUncertainValue },   weights :: AnalyticWeights ;  \n     n   =   10000 * length ( uvals ),  \n     bw :: Union { Nothing ,   Real }   =   nothing )    Combine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in  uvals  proportionally to the provided  relative probability  weights  (these are normalised by default, so don't need  to sum to 1), then pooling these draws together. Finally, a kernel density  estimate to the final distribution is computed over the  n  total draws.  Providing  AnalyticWeights  leads to the exact same behaviour as for  ProbabilityWeights , but may be more appropriote when relative importance weights are assigned subjectively,  and not based on quantitative evidence.  The KDE bandwidth is controlled by  bw . By default,  bw   =   nothing ; in this case,  the bandwidth is determined using the  KernelDensity . default_bandwidth  function.   Tip  For very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.   Example  1\n2\n3\n4\n5\n6\n7\n8\n9 v1   =   UncertainValue ( Normal ,   1 ,   0.3 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Normal ,   3.7 ,   0.8 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  # Two difference syntax options  combine ( uvals ,   AnalyticWeights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]))  combine ( uvals ,   aweights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]),   n   =   20000 )   # adjust number of total draws    source  For example:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 v1   =   UncertainValue ( UnivariateKDE ,   rand ( 4 : 0.25 : 6 ,   1000 ),   bandwidth   =   0.02 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Gamma ,   8 ,   0.4 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  p   =   plot ( title   =   L distributions \\,\\, with \\,\\, overlapping \\,\\, supports )  plot! ( v1 ,   label   =   L v_1: KDE \\, over \\, empirical \\, distribution ,   ls   =   : dash )  plot! ( v2 ,   label   =   L v_2: Normal(0.8, 0.4) ,   ls   =   : dot )  vline! ( v3 . values ,   label   =   L v_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4] )   # plot each possible state as vline  plot! ( v4 ,   label   =   L v_4: \\, Gamma(8, 0.4) )  pcombined   =   plot ( combine ( uvals ,   AnalyticWeights ([ 0.1 ,   0.3 ,   0.02 ,   0.5 ]),   n   =   100000 ,   bw   =   0.05 ),  \n     title   =   L combine([v_1, v_2, v_3, v_4], AnalyticWeights([0.1, 0.3, 0.02, 0.5]) ,   lc   =   : black ,   lw   =   2 )  plot ( p ,   pcombined ,   layout   =   ( 2 ,   1 ),   size   =   ( 800 ,   600 ),  \n     link   =   : x ,  \n     ylabel   =   Density , \n     tickfont   =   font ( 12 ), \n     legendfont   =   font ( 8 ),   fg_legend   =   : transparent ,   bg_legend   =   : transparent )", 
            "title": "AnalyticWeights"
        }, 
        {
            "location": "/uncertain_values/merging/#generic_weights", 
            "text": "#  UncertainData . combine     Method .  1\n2\n3 combine ( uvals :: Vector { AbstractUncertainValue },   weights :: Weights ;  \n     n   =   10000 * length ( uvals ),  \n     bw :: Union { Nothing ,   Real }   =   nothing )    Combine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in  uvals  proportionally to the provided  weights   (these are normalised by default, so don't need to sum to 1), then pooling these draws  together. Finally, a kernel density estimate to the final distribution is computed over  the  n  total draws.  Providing  Weights  leads to the exact same behaviour as for  ProbabilityWeights  and   AnalyticalWeights .  The KDE bandwidth is controlled by  bw . By default,  bw   =   nothing ; in this case,  the bandwidth is determined using the  KernelDensity . default_bandwidth  function.   Tip  For very wide, close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.   Example  1\n2\n3\n4\n5\n6\n7\n8\n9 v1   =   UncertainValue ( Normal ,   1 ,   0.3 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Normal ,   3.7 ,   0.8 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  # Two difference syntax options  combine ( uvals ,   Weights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]))  combine ( uvals ,   weights ([ 0.2 ,   0.1 ,   0.3 ,   0.2 ]),   n   =   20000 )   # adjust number of total draws    source  For example:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 v1   =   UncertainValue ( UnivariateKDE ,   rand ( 4 : 0.25 : 6 ,   1000 ),   bandwidth   =   0.01 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Gamma ,   8 ,   0.4 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  p   =   plot ( title   =   L distributions \\,\\, with \\,\\, overlapping \\,\\, supports )  plot! ( v1 ,   label   =   L v_1: KDE \\, over \\, empirical \\, distribution ,   ls   =   : dash )  plot! ( v2 ,   label   =   L v_2: Normal(0.8, 0.4) ,   ls   =   : dot )  # plot each possible state as vline  vline! ( v3 . values ,  \n     label   =   L v_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4] )   plot! ( v4 ,   label   =   L v_4: \\, Gamma(8, 0.4) )  pcombined   =   plot ( combine ( uvals ,   Weights ([ 0.1 ,   0.15 ,   0.1 ,   0.1 ]),   n   =   100000 ,   bw   =   0.02 ),  \n     title   =   L combine([v_1, v_2, v_3, v_4],  Weights([0.1, 0.15, 0.1, 0.1])) ,  \n     lc   =   : black ,   lw   =   2 )  plot ( p ,   pcombined ,   layout   =   ( 2 ,   1 ),   size   =   ( 800 ,   600 ),  \n     link   =   : x ,  \n     ylabel   =   Density , \n     tickfont   =   font ( 12 ), \n     legendfont   =   font ( 8 ),   fg_legend   =   : transparent ,   bg_legend   =   : transparent )", 
            "title": "Generic Weights"
        }, 
        {
            "location": "/uncertain_values/merging/#frequencyweights", 
            "text": "Using  FrequencyWeights , one may specify the number of times each of the uncertain values  should be sampled to form the pooled resampled draws on which the final kernel density  estimate is performed.  #  UncertainData . combine     Method .  1\n2 combine ( uvals :: Vector { AbstractUncertainValue },   weights :: FrequencyWeights ; \n     bw :: Union { Nothing ,   Real }   =   nothing )    Combine multiple uncertain values into a single uncertain value. This is done by resampling each uncertain value in  uvals  according to their relative frequencies (the absolute number of draws provided by  weights ). Finally, a kernel density  estimate to the final distribution is computed over the  sum ( weights )  total draws.  The KDE bandwidth is controlled by  bw . By default,  bw   =   nothing ; in this case,  the bandwidth is determined using the  KernelDensity . default_bandwidth  function.   Tip  For very wide and close-to-normal distributions, the default bandwidth may work well.  If you're combining very peaked distributions or discrete populations, however,  you may want to lower the bandwidth significantly.   Example  v1 = UncertainValue(Normal, 1, 0.3) v2 = UncertainValue(Normal, 0.8, 0.4) v3 = UncertainValue([rand() for i = 1:3], [0.3, 0.3, 0.4]) v4 = UncertainValue(Normal, 3.7, 0.8) uvals = [v1, v2, v3, v4];  Two difference syntax options  combine(uvals, FrequencyWeights([100, 500, 343, 7000])) combine(uvals, pweights([1410, 550, 223, 801]))  source  For example:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 v1   =   UncertainValue ( UnivariateKDE ,   rand ( 4 : 0.25 : 6 ,   1000 ),   bandwidth   =   0.01 )  v2   =   UncertainValue ( Normal ,   0.8 ,   0.4 )  v3   =   UncertainValue ([ rand ()   for   i   =   1 : 3 ],   [ 0.3 ,   0.3 ,   0.4 ])  v4   =   UncertainValue ( Gamma ,   8 ,   0.4 )  uvals   =   [ v1 ,   v2 ,   v3 ,   v4 ];  p   =   plot ( title   =   L distributions \\,\\, with \\,\\, overlapping \\,\\, supports )  plot! ( v1 ,   label   =   L v_1: KDE \\, over \\, empirical \\, distribution ,   ls   =   : dash )  plot! ( v2 ,   label   =   L v_2: Normal(0.8, 0.4) ,   ls   =   : dot )  # plot each possible state as vline  vline! ( v3 . values ,  \n     label   =   L v_3: \\, Discrete \\, population\\, [1,2,3], w/ \\, weights \\, [0.3, 0.4, 0.4] )   plot! ( v4 ,   label   =   L v_4: \\, Gamma(8, 0.4) )  pcombined   =   plot ( combine ( uvals ,   FrequencyWeights ([ 10000 ,   20000 ,   3000 ,   5000 ]),   bw   =   0.05 ),  \n     title   =   L combine([v_1, v_2, v_3, v_4], FrequencyWeights([10000, 20000, 3000, 5000]) ,  \n     lc   =   : black ,   lw   =   2 )  plot ( p ,   pcombined ,   layout   =   ( 2 ,   1 ),   size   =   ( 800 ,   600 ),  \n     link   =   : x ,  \n     ylabel   =   Density , \n     tickfont   =   font ( 12 ), \n     legendfont   =   font ( 8 ),   fg_legend   =   : transparent ,   bg_legend   =   : transparent )", 
            "title": "FrequencyWeights"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_datasets_overview/", 
            "text": "If dealing with several uncertain values, it may be useful to represent them as an uncertain dataset. This way, one may trivially, for example, compute statistics for a dataset consisting of samples with different types of uncertainties.\n\n\n\n\nUncertain index datasets and data value datasets\n\n\nThere are three main types of uncertain datasets: \n\n\n\n\nUncertainIndexDataset\ns contain uncertain indices.\n\n\nUncertainValueDataset\ns contain uncertain data values.\n\n\nUncertainIndexValueDataset\ns represent datasets for    which both the indices and the data values are uncertain. It uses   \nUncertainIndexDataset\ns to represent the indices and \nUncertainValueDataset\ns   to represent the data values.\n\n\n\n\n\n\nGeneric dataset type\n\n\nThere's also a generic uncertain dataset type for when you don't care about distinguishing  between indices and data values: \n\n\n\n\nUncertainDataset\n contains uncertain indices.", 
            "title": "Overview"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_datasets_overview/#uncertain_index_datasets_and_data_value_datasets", 
            "text": "There are three main types of uncertain datasets:    UncertainIndexDataset s contain uncertain indices.  UncertainValueDataset s contain uncertain data values.  UncertainIndexValueDataset s represent datasets for    which both the indices and the data values are uncertain. It uses    UncertainIndexDataset s to represent the indices and  UncertainValueDataset s   to represent the data values.", 
            "title": "Uncertain index datasets and data value datasets"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_datasets_overview/#generic_dataset_type", 
            "text": "There's also a generic uncertain dataset type for when you don't care about distinguishing  between indices and data values:    UncertainDataset  contains uncertain indices.", 
            "title": "Generic dataset type"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_dataset/", 
            "text": "UncertainDataset\ns is a generic uncertain dataset type that has no explicit index  associated with its uncertain values. \n\n\nIt inherits all the behaviour of \nAbstractUncertainValueDataset\n, but may lack some  functionality that an \nUncertainValueDataset\n has. \n\n\nIf you don't care about distinguishing between  indices and data values, constructing instances of this data type requires five less key  presses than \nUncertainValueDataset\n.\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nUncertainDatasets\n.\nUncertainDataset\n \n \nType\n.\n\n\n1\nUncertainDataset\n\n\n\n\n\n\n\nGeneric dataset containing uncertain values.\n\n\nFields\n\n\n\n\nvalues\n::\nAbstractVector\n{\nAbstractUncertainValue\n}\n: The uncertain values.\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nExample 1: constructing an \nUncertainDataset\n from uncertain values\n\n\nLet's create a random walk and pretend it represents fluctuations in the mean of an observed dataset. Assume that each data point is normally distributed, and that the \ni\ni\n-th observation has standard deviation \n\\sigma_i \\in [0.3, 0.5]\n\\sigma_i \\in [0.3, 0.5]\n.\n\n\nRepresenting these data as an \nUncertainDataset\n is done as follows:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nusing\n \nUncertainData\n,\n \nPlots\n\n\n\n# Create a random walk of 55 steps\n\n\nn\n \n=\n \n55\n\n\nrw\n \n=\n \ncumsum\n(\nrand\n(\nNormal\n(),\n \nn\n))\n\n\n\n# Represent each value of the random walk as an uncertain value and\n\n\n# collect them in an UncertainDataset\n\n\ndist\n \n=\n \nUniform\n(\n0.3\n,\n \n0.5\n)\n\n\nuncertainvals\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nrw\n[\ni\n],\n \nrand\n(\ndist\n))\n \nfor\n \ni\n \n=\n \n1\n:\nn\n]\n\n\nD\n \n=\n \nUncertainDataset\n(\nuncertainvals\n)\n\n\n\n\n\n\n\nBy default, plotting the dataset will plot the median values (only for scatter plots) along with the 33\nrd\n to 67\nth\n percentile range error bars. \n\n\n1\nplot\n(\nD\n)\n\n\n\n\n\n\n\n\n\nYou can customize the error bars by explicitly providing the quantiles:\n\n\n1\nplot\n(\nD\n,\n \n[\n0\n.\n05\n,\n \n0\n.\n95\n])\n\n\n\n\n\n\n\n\n\n\n\nExample 2: mixing different types of uncertain values\n\n\nMixing different types of uncertain values also works. Let's create a dataset of uncertain values constructed in different ways.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nusing\n \nUncertainData\n,\n \nDistributions\n,\n \nPlots\n\n\n\n# Theoretical distributions\n\n\no1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0\n,\n \n0.5\n)\n\n\no2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2\n,\n \n0.3\n)\n\n\no3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n0\n,\n \n4\n)\n\n\n\n# Theoretical distributions fitted to data\n\n\no4\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\nUniform\n(),\n \n100\n))\n\n\no5\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nrand\n(\nGamma\n(\n2\n,\n \n3\n),\n \n5000\n))\n\n\n\n# Kernel density estimated distributions for some more complex data.\n\n\nM1\n \n=\n \nMixtureModel\n([\nNormal\n(\n-\n5\n,\n \n0.5\n),\n \nGamma\n(\n2\n,\n \n5\n),\n \nNormal\n(\n12\n,\n \n0.2\n)])\n\n\nM2\n \n=\n \nMixtureModel\n([\nNormal\n(\n-\n2\n,\n \n0.1\n),\n \nNormal\n(\n1\n,\n \n0.2\n)])\n\n\no6\n \n=\n \nUncertainValue\n(\nrand\n(\nM1\n,\n \n1000\n))\n\n\no7\n \n=\n \nUncertainValue\n(\nrand\n(\nM2\n,\n \n1000\n))\n\n\n\nD\n \n=\n \nUncertainDataset\n([\no1\n,\n \no2\n,\n \no3\n,\n \no4\n,\n \no5\n,\n \no6\n,\n \no7\n])\n\n\n\n\n\n\n\nNow, plot the uncertain dataset.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nusing\n \nPlots\n\n\n# Initialise the plot\n\n\np\n \n=\n \nplot\n(\nlegend\n \n=\n \nfalse\n,\n \nxlabel\n \n=\n \ntime step\n,\n \nylabel\n \n=\n \nvalue\n)\n\n\n\n# Plot the mean of the dataset\n\n\nplot!\n([\nmedian\n(\nD\n[\ni\n])\n \nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nD\n)],\n \nlabel\n \n=\n \nmean\n,\n \nlc\n \n=\n \n:\nblue\n,\n \nlw\n \n=\n \n3\n)\n\n\n\nfor\n \ni\n \n=\n \n1\n:\n200\n\n    \nplot!\n(\np\n,\n \nresample\n(\nD\n),\n \nlw\n \n=\n \n0.4\n,\n \nl\u03b1\n \n=\n \n0.1\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nend\n\n\n\np", 
            "title": "Generic uncertain dataset"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_dataset/#documentation", 
            "text": "#  UncertainData . UncertainDatasets . UncertainDataset     Type .  1 UncertainDataset    Generic dataset containing uncertain values.  Fields   values :: AbstractVector { AbstractUncertainValue } : The uncertain values.   source", 
            "title": "Documentation"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_dataset/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_dataset/#example_1_constructing_an_uncertaindataset_from_uncertain_values", 
            "text": "Let's create a random walk and pretend it represents fluctuations in the mean of an observed dataset. Assume that each data point is normally distributed, and that the  i i -th observation has standard deviation  \\sigma_i \\in [0.3, 0.5] \\sigma_i \\in [0.3, 0.5] .  Representing these data as an  UncertainDataset  is done as follows:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 using   UncertainData ,   Plots  # Create a random walk of 55 steps  n   =   55  rw   =   cumsum ( rand ( Normal (),   n ))  # Represent each value of the random walk as an uncertain value and  # collect them in an UncertainDataset  dist   =   Uniform ( 0.3 ,   0.5 )  uncertainvals   =   [ UncertainValue ( Normal ,   rw [ i ],   rand ( dist ))   for   i   =   1 : n ]  D   =   UncertainDataset ( uncertainvals )    By default, plotting the dataset will plot the median values (only for scatter plots) along with the 33 rd  to 67 th  percentile range error bars.   1 plot ( D )     You can customize the error bars by explicitly providing the quantiles:  1 plot ( D ,   [ 0 . 05 ,   0 . 95 ])", 
            "title": "Example 1: constructing an UncertainDataset from uncertain values"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_dataset/#example_2_mixing_different_types_of_uncertain_values", 
            "text": "Mixing different types of uncertain values also works. Let's create a dataset of uncertain values constructed in different ways.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 using   UncertainData ,   Distributions ,   Plots  # Theoretical distributions  o1   =   UncertainValue ( Normal ,   0 ,   0.5 )  o2   =   UncertainValue ( Normal ,   2 ,   0.3 )  o3   =   UncertainValue ( Uniform ,   0 ,   4 )  # Theoretical distributions fitted to data  o4   =   UncertainValue ( Uniform ,   rand ( Uniform (),   100 ))  o5   =   UncertainValue ( Gamma ,   rand ( Gamma ( 2 ,   3 ),   5000 ))  # Kernel density estimated distributions for some more complex data.  M1   =   MixtureModel ([ Normal ( - 5 ,   0.5 ),   Gamma ( 2 ,   5 ),   Normal ( 12 ,   0.2 )])  M2   =   MixtureModel ([ Normal ( - 2 ,   0.1 ),   Normal ( 1 ,   0.2 )])  o6   =   UncertainValue ( rand ( M1 ,   1000 ))  o7   =   UncertainValue ( rand ( M2 ,   1000 ))  D   =   UncertainDataset ([ o1 ,   o2 ,   o3 ,   o4 ,   o5 ,   o6 ,   o7 ])    Now, plot the uncertain dataset.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 using   Plots  # Initialise the plot  p   =   plot ( legend   =   false ,   xlabel   =   time step ,   ylabel   =   value )  # Plot the mean of the dataset  plot! ([ median ( D [ i ])   for   i   =   1 : length ( D )],   label   =   mean ,   lc   =   : blue ,   lw   =   3 )  for   i   =   1 : 200 \n     plot! ( p ,   resample ( D ),   lw   =   0.4 ,   l\u03b1   =   0.1 ,   lc   =   : black )  end  p", 
            "title": "Example 2: mixing different types of uncertain values"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/", 
            "text": "UncertainIndexDataset\ns is an uncertain dataset type that represents the indices  corresponding to an \nUncertainValueDataset\n.\n\n\nIt is meant to be used for the \nindices\n field in \nUncertainIndexValueDataset\ns instances.\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nUncertainDatasets\n.\nUncertainIndexDataset\n \n \nType\n.\n\n\n1\nUncertainIndices\n\n\n\n\n\n\n\nGeneric dataset containing uncertain indices.\n\n\nFields\n\n\n\n\nindices\n::\nAbstractVector\n{\nAbstractUncertainValue\n}\n: The uncertain values.\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nExample 1: increasing index uncertainty through time\n\n\n\n\nDefining the indices\n\n\nSay we had a dataset of 20 values for which the uncertainties are normally distributed  with increasing standard deviation through time.\n\n\n1\n2\n3\ntime_inds\n \n=\n \n1\n:\n13\n\n\nuvals\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nind\n,\n \nrand\n(\nUniform\n())\n \n+\n \n(\nind\n \n/\n \n6\n))\n \nfor\n \nind\n \nin\n \ntime_inds\n]\n\n\ninds\n \n=\n \nUncertainIndexDataset\n(\nuvals\n)\n\n\n\n\n\n\n\nLet's plot the 33\nrd\n to 67\nth\n percentile range for the indices: \n\n\n1\nplot\n(\ninds\n,\n \n[\n0\n.\n33\n,\n \n0\n.\n67\n])\n\n\n\n\n\n\n\n\n\n\n\nDefining the data\n\n\nLet's define some uncertain values that are associated with the indices. \n\n\n1\n2\n3\n4\n5\n6\nu1\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nrand\n(\nGamma\n(),\n \n500\n))\n\n\nu2\n \n=\n \nUncertainValue\n(\nrand\n(\nMixtureModel\n([\nNormal\n(\n1\n,\n \n0.3\n),\n \nNormal\n(\n0.1\n,\n \n0.1\n)]),\n \n500\n))\n\n\nuvals3\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nrand\n(),\n \nrand\n())\n \nfor\n \ni\n \n=\n \n1\n:\n11\n]\n\n\n\nmeasurements\n \n=\n \n[\nu1\n;\n \nu2\n;\n \nuvals3\n]\n\n\ndatavals\n \n=\n \nUncertainValueDataset\n(\nmeasurements\n)\n\n\n\n\n\n\n\n\n\n\n\nCombinining the indices and values\n\n\nNow, we combine the indices and the corresponding data. \n\n\n1\nd\n \n=\n \nUncertainIndexDataset\n(\ninds\n,\n \ndatavals\n)\n\n\n\n\n\n\n\nPlot the dataset with error bars in both directions, using the 20\nth\n to 80\nth\n percentile  range for the indices and the 33\nrd\n to 67\nth\n percentile range for the data values. \n\n\n1\nplot\n(\nd\n,\n \n[\n0\n.\n2\n,\n \n0\n.\n8\n],\n \n[\n0\n.\n33\n,\n \n0\n.\n67\n])", 
            "title": "Uncertain index dataset"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#documentation", 
            "text": "#  UncertainData . UncertainDatasets . UncertainIndexDataset     Type .  1 UncertainIndices    Generic dataset containing uncertain indices.  Fields   indices :: AbstractVector { AbstractUncertainValue } : The uncertain values.   source", 
            "title": "Documentation"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#example_1_increasing_index_uncertainty_through_time", 
            "text": "", 
            "title": "Example 1: increasing index uncertainty through time"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#defining_the_indices", 
            "text": "Say we had a dataset of 20 values for which the uncertainties are normally distributed  with increasing standard deviation through time.  1\n2\n3 time_inds   =   1 : 13  uvals   =   [ UncertainValue ( Normal ,   ind ,   rand ( Uniform ())   +   ( ind   /   6 ))   for   ind   in   time_inds ]  inds   =   UncertainIndexDataset ( uvals )    Let's plot the 33 rd  to 67 th  percentile range for the indices:   1 plot ( inds ,   [ 0 . 33 ,   0 . 67 ])", 
            "title": "Defining the indices"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#defining_the_data", 
            "text": "Let's define some uncertain values that are associated with the indices.   1\n2\n3\n4\n5\n6 u1   =   UncertainValue ( Gamma ,   rand ( Gamma (),   500 ))  u2   =   UncertainValue ( rand ( MixtureModel ([ Normal ( 1 ,   0.3 ),   Normal ( 0.1 ,   0.1 )]),   500 ))  uvals3   =   [ UncertainValue ( Normal ,   rand (),   rand ())   for   i   =   1 : 11 ]  measurements   =   [ u1 ;   u2 ;   uvals3 ]  datavals   =   UncertainValueDataset ( measurements )", 
            "title": "Defining the data"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_index_dataset/#combinining_the_indices_and_values", 
            "text": "Now, we combine the indices and the corresponding data.   1 d   =   UncertainIndexDataset ( inds ,   datavals )    Plot the dataset with error bars in both directions, using the 20 th  to 80 th  percentile  range for the indices and the 33 rd  to 67 th  percentile range for the data values.   1 plot ( d ,   [ 0 . 2 ,   0 . 8 ],   [ 0 . 33 ,   0 . 67 ])", 
            "title": "Combinining the indices and values"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_value_dataset/", 
            "text": "UncertainValueDataset\ns is an uncertain dataset type that has no explicit index  associated with its uncertain values. This type may come with some extra functionality  that the generic \nUncertainDataset\n type does not support. \n\n\nUse this type when you want to be explicit about the values representing data values, as opposed to \nindices\n. \n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nUncertainDatasets\n.\nUncertainValueDataset\n \n \nType\n.\n\n\n1\nUncertainValueDataset\n\n\n\n\n\n\n\nA dataset of uncertain values.\n\n\nFields\n\n\n\n\nvalues\n::\nAbstractVector\n{\nAbstractUncertainValue\n}\n: The uncertain values. Each value is   represented by an \nAbstractUncertainValue\n.\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nExample 1: constructing an \nUncertainValueDataset\n from uncertain values\n\n\nLet's create a random walk and pretend it represents fluctuations in the mean of an observed dataset. Assume that each data point is normally distributed, and that the \ni\ni\n-th observation has standard deviation \n\\sigma_i \\in [0.3, 0.5]\n\\sigma_i \\in [0.3, 0.5]\n.\n\n\nRepresenting these data as an \nUncertainValueDataset\n is done as follows:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\no1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n0\n,\n \n0.5\n)\n\n\no2\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.0\n,\n \n0.1\n)\n\n\no3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n0\n,\n \n4\n)\n\n\no4\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\n100\n))\n\n\no5\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n4\n,\n \n5\n)\n\n\no6\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n4\n,\n \n5\n)\n\n\no7\n \n=\n \nUncertainValue\n(\nFrechet\n,\n \n1\n,\n \n2\n)\n\n\no8\n \n=\n \nUncertainValue\n(\nBetaPrime\n,\n \n1\n,\n \n2\n)\n\n\no9\n \n=\n \nUncertainValue\n(\nBetaBinomial\n,\n \n10\n,\n \n3\n,\n \n2\n)\n\n\no10\n \n=\n \nUncertainValue\n(\nBinomial\n,\n \n10\n,\n \n0.3\n)\n\n\n\nuvals\n \n=\n \n[\no1\n,\n \no2\n,\n \no3\n,\n \no4\n,\n \no5\n,\n \no6\n,\n \no7\n,\n \no8\n,\n \no9\n,\n \no10\n]\n\n\nd\n \n=\n \nUncertainValueDataset\n(\nuvals\n)\n\n\n\n\n\n\n\nThe built-in plot recipes makes it a breeze to plot the dataset. Here, we'll plot the  20\nth\n to 80\nth\n percentile range error bars. \n\n\n1\nplot\n(\nd\n,\n \n[\n0\n.\n2\n,\n \n0\n.\n8\n])", 
            "title": "Uncertain value dataset"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_value_dataset/#documentation", 
            "text": "#  UncertainData . UncertainDatasets . UncertainValueDataset     Type .  1 UncertainValueDataset    A dataset of uncertain values.  Fields   values :: AbstractVector { AbstractUncertainValue } : The uncertain values. Each value is   represented by an  AbstractUncertainValue .   source", 
            "title": "Documentation"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_value_dataset/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_value_dataset/#example_1_constructing_an_uncertainvaluedataset_from_uncertain_values", 
            "text": "Let's create a random walk and pretend it represents fluctuations in the mean of an observed dataset. Assume that each data point is normally distributed, and that the  i i -th observation has standard deviation  \\sigma_i \\in [0.3, 0.5] \\sigma_i \\in [0.3, 0.5] .  Representing these data as an  UncertainValueDataset  is done as follows:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 o1   =   UncertainValue ( Normal ,   0 ,   0.5 )  o2   =   UncertainValue ( Normal ,   2.0 ,   0.1 )  o3   =   UncertainValue ( Uniform ,   0 ,   4 )  o4   =   UncertainValue ( Uniform ,   rand ( 100 ))  o5   =   UncertainValue ( Beta ,   4 ,   5 )  o6   =   UncertainValue ( Gamma ,   4 ,   5 )  o7   =   UncertainValue ( Frechet ,   1 ,   2 )  o8   =   UncertainValue ( BetaPrime ,   1 ,   2 )  o9   =   UncertainValue ( BetaBinomial ,   10 ,   3 ,   2 )  o10   =   UncertainValue ( Binomial ,   10 ,   0.3 )  uvals   =   [ o1 ,   o2 ,   o3 ,   o4 ,   o5 ,   o6 ,   o7 ,   o8 ,   o9 ,   o10 ]  d   =   UncertainValueDataset ( uvals )    The built-in plot recipes makes it a breeze to plot the dataset. Here, we'll plot the  20 th  to 80 th  percentile range error bars.   1 plot ( d ,   [ 0 . 2 ,   0 . 8 ])", 
            "title": "Example 1: constructing an UncertainValueDataset from uncertain values"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_indexvalue_dataset/", 
            "text": "UncertainIndexValueDataset\ns have uncertainties associated with both the  indices (e.g. time, depth, etc) and the values of the data points.\n\n\n#\n\n\nUncertainData\n.\nUncertainDatasets\n.\nUncertainIndexValueDataset\n \n \nType\n.\n\n\n1\nUncertainIndexValueDataset\n\n\n\n\n\n\n\nA generic dataset type consisting of uncertain values whose indices (time, depth, order, etc...) are also uncertain value.\n\n\nFields\n\n\n\n\nvalues\n::\nT\n where {T \n: AbstractUncertainValueDataset}\n: The uncertain indices.    Will in general be an \nUncertainIndexDataset\n, but does not necessarily have to be.     Each index is represented by an \nAbstractUncertainValue\n.\n\n\nvalues\n::\nUncertainDataset\n: The uncertain values. Each value is   represented by an \nAbstractUncertainValue\n.\n\n\n\n\nsource\n\n\n\n\nExample\n\n\nHere's an example of how to create an uncertain index-value dataset. Let's start by  defining the uncertain data values and collecting them in an \nUncertainValueDataset\n. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nUncertainData\n,\n \nPlots\n \n\ngr\n()\n\n\nr1\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nrand\n(),\n \nrand\n())\n \nfor\n \ni\n \n=\n \n1\n:\n10\n]\n\n\nr2\n \n=\n \nUncertainValue\n(\nrand\n(\n10000\n))\n\n\nr3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\n10000\n))\n\n\nr4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n0.1\n,\n \n0.5\n)\n\n\nr5\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n0.4\n,\n \n0.8\n)\n\n\n\nu_values\n \n=\n \n[\nr1\n;\n \nr2\n;\n \nr3\n;\n \nr4\n;\n \nr5\n]\n\n\nudata\n \n=\n \nUncertainDataset\n(\nu_values\n);\n\n\n\n\n\n\n\nThe values were measures at some time indices by an inaccurate clock, so that the times  of measuring are normally distributed values with fluctuating standard deviations.\n\n\n1\n2\n3\nu_timeindices\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \ni\n,\n \nrand\n(\nUniform\n(\n0\n,\n \n1\n)))\n \n    \nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nudata\n)]\n\n\nuindices\n \n=\n \nUncertainDataset\n(\nu_timeindices\n);\n\n\n\n\n\n\n\nNow, combine the uncertain time indices and measurements into an  \nUncertainIndexValueDataset\n.\n\n\n1\nx\n \n=\n \nUncertainIndexValueDataset\n(\nuindices\n,\n \nudata\n)\n\n\n\n\n\n\n\nThe built-in plot recipes make it easy to visualize the dataset.  By default, plotting the dataset plots the median value of the index and the measurement  (only for scatter plots), along with the 33\nrd\n to 67\nth\n percentile range error bars in both  directions. \n\n\n1\nplot\n(\nx\n)\n\n\n\n\n\n\n\n\n\nYou can also tune the error bars by calling  \nplot\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \nidx_quantiles\n,\n \nval_quantiles\n)\n, explicitly  specifying the quantiles in each direction, like so:\n\n\n1\nplot\n(\nx\n,\n \n[\n0.05\n,\n \n0.95\n],\n \n[\n0.05\n,\n \n0.95\n])", 
            "title": "Uncertain index-value dataset"
        }, 
        {
            "location": "/uncertain_datasets/uncertain_indexvalue_dataset/#example", 
            "text": "Here's an example of how to create an uncertain index-value dataset. Let's start by  defining the uncertain data values and collecting them in an  UncertainValueDataset .    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   UncertainData ,   Plots   gr ()  r1   =   [ UncertainValue ( Normal ,   rand (),   rand ())   for   i   =   1 : 10 ]  r2   =   UncertainValue ( rand ( 10000 ))  r3   =   UncertainValue ( Uniform ,   rand ( 10000 ))  r4   =   UncertainValue ( Normal ,   - 0.1 ,   0.5 )  r5   =   UncertainValue ( Gamma ,   0.4 ,   0.8 )  u_values   =   [ r1 ;   r2 ;   r3 ;   r4 ;   r5 ]  udata   =   UncertainDataset ( u_values );    The values were measures at some time indices by an inaccurate clock, so that the times  of measuring are normally distributed values with fluctuating standard deviations.  1\n2\n3 u_timeindices   =   [ UncertainValue ( Normal ,   i ,   rand ( Uniform ( 0 ,   1 )))  \n     for   i   =   1 : length ( udata )]  uindices   =   UncertainDataset ( u_timeindices );    Now, combine the uncertain time indices and measurements into an   UncertainIndexValueDataset .  1 x   =   UncertainIndexValueDataset ( uindices ,   udata )    The built-in plot recipes make it easy to visualize the dataset.  By default, plotting the dataset plots the median value of the index and the measurement  (only for scatter plots), along with the 33 rd  to 67 th  percentile range error bars in both  directions.   1 plot ( x )     You can also tune the error bars by calling   plot ( udata :: UncertainIndexValueDataset ,   idx_quantiles ,   val_quantiles ) , explicitly  specifying the quantiles in each direction, like so:  1 plot ( x ,   [ 0.05 ,   0.95 ],   [ 0.05 ,   0.95 ])", 
            "title": "Example"
        }, 
        {
            "location": "/resampling/resampling_overview/", 
            "text": "Because uncertain values are represented by  \nsome kind of probability distribution\n, we may trivially resample them by drawing values from their furnishing distributions.\n\n\nIf needed, you may choose to  \nconstrain\n an uncertain value  before resampling, using one of the available  \nsampling constraints\n.\n\n\nThe \nresample\n function is used to resample uncertain values. For detailed instructions on how to sample uncertain values and datasets of uncertain values, see the following pages:\n\n\n\n\nResampling uncertain values\n\n\nResampling uncertain value datasets\n\n\nResampling uncertain index-value datasets", 
            "title": "Overview"
        }, 
        {
            "location": "/sampling_constraints/constrain_uncertain_values/", 
            "text": "Uncertain values may be constrained in various ways, as visualised by the following example.\n\n\n\n\nWhich was generated as follows. Note that the plot recipe normalises the distributions after constraining the uncertain values.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nuval\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2\n,\n \n5\n)\n\n\n\np1\n \n=\n \nplot\n(\nuval\n,\n \nlabel\n \n=\n \nfull support\n,\n \ntitle\n \n=\n \nQuantile truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateQuantiles\n(\n0.2\n,\n \n0.8\n)),\n \nlabel\n \n=\n \nquantile range truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateUpperQuantile\n(\n0.9\n)),\n \nlabel\n \n=\n \nupper quantile truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateLowerQuantile\n(\n0.1\n)),\n \nlabel\n \n=\n \nlower quantile truncation\n)\n\n\n\np2\n \n=\n \nplot\n(\nuval\n,\n \nlabel\n \n=\n \nfull support\n,\n \ntitle\n \n=\n \nValue truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateRange\n(\n2\n,\n \n4\n)),\n \nls\n \n=\n \n:\ndash\n,\n \nlabel\n \n=\n \nrange truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateMaximum\n(\n4.5\n)),\n \nls\n \n=\n \n:\ndash\n,\n \nlabel\n \n=\n \nmaximum value truncation\n)\n\n\nplot!\n(\nconstrain\n(\nuval\n,\n \nTruncateMinimum\n(\n-\n2\n)),\n \nls\n \n=\n \n:\ndash\n,\n \nlabel\n \n=\n \nminimum value truncation\n)\n\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nlink\n \n=\n \n:\nx\n,\n \nxlabel\n \n=\n \nvalue\n,\n \nylabel\n \n=\n \nprobability\n)\n\n\n\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nconstrain\n \n \nMethod\n.\n\n\n1\nconstrain\n(\nuv\n::\nAbstractUncertainValue\n,\n \nconstraint\n::\nSamplingConstraint\n)\n\n\n\n\n\n\n\nApply the \nconstraint\n and truncate the support of the distribution furnishing the uncertain value \nuv\n. Returns a constrained uncertain value.\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\n\n\nTheoretical distribution\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nusing\n \nUncertainData\n,\n \nDistributions\n\n\n\n# Define an uncertain value furnished by a theoretical distribution\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1\n,\n \n0.5\n)\n\n\n\n# Constrain the support of the furnishing distribution using various\n\n\n# constraints\n\n\nuvc_lq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\nuvc_uq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateUpperQuantile\n(\n0.8\n))\n\n\nuvc_q\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateQuantiles\n(\n0.2\n,\n \n0.8\n))\n\n\nuvc_min\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMinimum\n(\n0.5\n))\n\n\nuvc_max\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMaximum\n(\n1.5\n))\n\n\nuvc_range\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateRange\n(\n0.5\n,\n \n1.5\n))\n\n\n\n\n\n\n\n\nTheoretical distribution with fitted parameters\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nusing\n \nUncertainData\n,\n \nDistributions\n\n\n\n# Define an uncertain value furnished by a theoretical distribution with\n\n\n# parameters fitted to empirical data\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n-\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\n\n# Constrain the support of the furnishing distribution using various\n\n\n# constraints\n\n\nuvc_lq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\nuvc_uq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateUpperQuantile\n(\n0.8\n))\n\n\nuvc_q\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateQuantiles\n(\n0.2\n,\n \n0.8\n))\n\n\nuvc_min\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMinimum\n(\n0.5\n))\n\n\nuvc_max\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMaximum\n(\n1.5\n))\n\n\nuvc_range\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateRange\n(\n0.5\n,\n \n1.5\n))\n\n\n\n\n\n\n\n\nKernel density estimated distribution\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Define an uncertain value furnished by a kernel density estimate to the\n\n\n# distribution of the empirical data\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nrand\n(\nUniform\n(\n10\n,\n \n15\n),\n \n1000\n))\n\n\n\n# Constrain the support of the furnishing distribution using various\n\n\n# constraints\n\n\nuvc_lq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\nuvc_uq\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateUpperQuantile\n(\n0.8\n))\n\n\nuvc_q\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateQuantiles\n(\n0.2\n,\n \n0.8\n))\n\n\nuvc_min\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMinimum\n(\n13\n))\n\n\nuvc_max\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateMaximum\n(\n13\n))\n\n\nuvc_range\n \n=\n \nconstrain\n(\nuv\n,\n \nTruncateRange\n(\n11\n,\n \n12\n))", 
            "title": "Constraining uncertain values"
        }, 
        {
            "location": "/sampling_constraints/constrain_uncertain_values/#documentation", 
            "text": "#  UncertainData . SamplingConstraints . constrain     Method .  1 constrain ( uv :: AbstractUncertainValue ,   constraint :: SamplingConstraint )    Apply the  constraint  and truncate the support of the distribution furnishing the uncertain value  uv . Returns a constrained uncertain value.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/sampling_constraints/constrain_uncertain_values/#examples", 
            "text": "Theoretical distribution   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 using   UncertainData ,   Distributions  # Define an uncertain value furnished by a theoretical distribution  uv   =   UncertainValue ( Normal ,   1 ,   0.5 )  # Constrain the support of the furnishing distribution using various  # constraints  uvc_lq   =   constrain ( uv ,   TruncateLowerQuantile ( 0.2 ))  uvc_uq   =   constrain ( uv ,   TruncateUpperQuantile ( 0.8 ))  uvc_q   =   constrain ( uv ,   TruncateQuantiles ( 0.2 ,   0.8 ))  uvc_min   =   constrain ( uv ,   TruncateMinimum ( 0.5 ))  uvc_max   =   constrain ( uv ,   TruncateMaximum ( 1.5 ))  uvc_range   =   constrain ( uv ,   TruncateRange ( 0.5 ,   1.5 ))     Theoretical distribution with fitted parameters   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 using   UncertainData ,   Distributions  # Define an uncertain value furnished by a theoretical distribution with  # parameters fitted to empirical data  uv   =   UncertainValue ( Normal ,   rand ( Normal ( - 1 ,   0.2 ),   1000 ))  # Constrain the support of the furnishing distribution using various  # constraints  uvc_lq   =   constrain ( uv ,   TruncateLowerQuantile ( 0.2 ))  uvc_uq   =   constrain ( uv ,   TruncateUpperQuantile ( 0.8 ))  uvc_q   =   constrain ( uv ,   TruncateQuantiles ( 0.2 ,   0.8 ))  uvc_min   =   constrain ( uv ,   TruncateMinimum ( 0.5 ))  uvc_max   =   constrain ( uv ,   TruncateMaximum ( 1.5 ))  uvc_range   =   constrain ( uv ,   TruncateRange ( 0.5 ,   1.5 ))     Kernel density estimated distribution   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Define an uncertain value furnished by a kernel density estimate to the  # distribution of the empirical data  uv   =   UncertainValue ( UnivariateKDE ,   rand ( Uniform ( 10 ,   15 ),   1000 ))  # Constrain the support of the furnishing distribution using various  # constraints  uvc_lq   =   constrain ( uv ,   TruncateLowerQuantile ( 0.2 ))  uvc_uq   =   constrain ( uv ,   TruncateUpperQuantile ( 0.8 ))  uvc_q   =   constrain ( uv ,   TruncateQuantiles ( 0.2 ,   0.8 ))  uvc_min   =   constrain ( uv ,   TruncateMinimum ( 13 ))  uvc_max   =   constrain ( uv ,   TruncateMaximum ( 13 ))  uvc_range   =   constrain ( uv ,   TruncateRange ( 11 ,   12 ))", 
            "title": "Examples"
        }, 
        {
            "location": "/sampling_constraints/available_constraints/", 
            "text": "The following sampling constraints are available:\n\n\n\n\nGeneric resampling\n\n\nThese constraints may be used in any resampling setting.\n\n\n\n\nStandard deviation\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateStd\n \n \nType\n.\n\n\n1\nTruncateStd\n(\nn\u03c3\n::\nInt\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated at \nn\n\u03c3\n (\nn\n standard deviations).\n\n\nsource\n\n\n\n\nValue range\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateMinimum\n \n \nType\n.\n\n\n1\nTruncateMinimum\n(\nmin\n::\nNumber\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated below at some specified minimum value.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateMaximum\n \n \nType\n.\n\n\n1\nTruncateMaximum\n(\nmax\n::\nNumber\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated above at some specified maximum value.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateRange\n \n \nType\n.\n\n\n1\nTruncateRange\n(\nmin\n::\nNumber\n,\n \nmax\n::\nNumber\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated at some range \n[\nmin\n,\n \nmax\n]\n.\n\n\nsource\n\n\n\n\nQuantile range\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateLowerQuantile\n \n \nType\n.\n\n\n1\nTruncateLowerQuantile\n(\nlower_quantile\n::\nFloat64\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated below at some quantile.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateUpperQuantile\n \n \nType\n.\n\n\n1\nTruncateUpperQuantile\n(\nupper_quantile\n::\nFloat64\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated above at some quantile.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nTruncateQuantiles\n \n \nType\n.\n\n\n1\nTruncateQuantiles\n(\nlower_quantile\n::\nFloat64\n,\n \nupper_quantile\n::\nFloat64\n)\n\n\n\n\n\n\n\nA constraint indicating that the distribution furnishing an uncertain value should be truncated at some quantile quantile \n(\nlower_quantile\n,\n \nupper_quantile\n)\n.\n\n\nsource", 
            "title": "Generic constraints"
        }, 
        {
            "location": "/sampling_constraints/available_constraints/#generic_resampling", 
            "text": "These constraints may be used in any resampling setting.", 
            "title": "Generic resampling"
        }, 
        {
            "location": "/sampling_constraints/available_constraints/#standard_deviation", 
            "text": "#  UncertainData . SamplingConstraints . TruncateStd     Type .  1 TruncateStd ( n\u03c3 :: Int )    A constraint indicating that the distribution furnishing an uncertain value should be truncated at  n \u03c3  ( n  standard deviations).  source", 
            "title": "Standard deviation"
        }, 
        {
            "location": "/sampling_constraints/available_constraints/#value_range", 
            "text": "#  UncertainData . SamplingConstraints . TruncateMinimum     Type .  1 TruncateMinimum ( min :: Number )    A constraint indicating that the distribution furnishing an uncertain value should be truncated below at some specified minimum value.  source  #  UncertainData . SamplingConstraints . TruncateMaximum     Type .  1 TruncateMaximum ( max :: Number )    A constraint indicating that the distribution furnishing an uncertain value should be truncated above at some specified maximum value.  source  #  UncertainData . SamplingConstraints . TruncateRange     Type .  1 TruncateRange ( min :: Number ,   max :: Number )    A constraint indicating that the distribution furnishing an uncertain value should be truncated at some range  [ min ,   max ] .  source", 
            "title": "Value range"
        }, 
        {
            "location": "/sampling_constraints/available_constraints/#quantile_range", 
            "text": "#  UncertainData . SamplingConstraints . TruncateLowerQuantile     Type .  1 TruncateLowerQuantile ( lower_quantile :: Float64 )    A constraint indicating that the distribution furnishing an uncertain value should be truncated below at some quantile.  source  #  UncertainData . SamplingConstraints . TruncateUpperQuantile     Type .  1 TruncateUpperQuantile ( upper_quantile :: Float64 )    A constraint indicating that the distribution furnishing an uncertain value should be truncated above at some quantile.  source  #  UncertainData . SamplingConstraints . TruncateQuantiles     Type .  1 TruncateQuantiles ( lower_quantile :: Float64 ,   upper_quantile :: Float64 )    A constraint indicating that the distribution furnishing an uncertain value should be truncated at some quantile quantile  ( lower_quantile ,   upper_quantile ) .  source", 
            "title": "Quantile range"
        }, 
        {
            "location": "/sampling_constraints/sequential_constraints/", 
            "text": "The following constraints may be used to impose sequential constraints when sampling a  dataset element-wise. \n\n\n\n\nIncreasing\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nStrictlyIncreasing\n \n \nType\n.\n\n\n1\nStrictlyIncreasing\n\n\n\n\n\n\n\nA sampling constraint indicating element-wise sampling of the uncertain values in a dataset, such that the values of the draw are strictly increasing in magnitude.\n\n\nTypically used when there are known, physical constraints on the measurements. For example, geochemical measurements of sediments at different depths of a sediment core  are taken at physically separate depths in the core. Thus, the order of the indices cannot be flipped, and must be strictly decreasing/increasing. \n\n\nsource\n\n\n\n\nDecreasing\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nStrictlyDecreasing\n \n \nType\n.\n\n\n1\nStrictlyDecreasing\n\n\n\n\n\n\n\nA sampling constraint indicating element-wise sampling of the uncertain values in a dataset, such that the values of the draw are strictly decreasing in magnitude.\n\n\nTypically used when there are known, physical constraints on the measurements. For example, geochemical measurements of sediments at different depths of a sediment core  are taken at physically separate depths in the core. Thus, the order of the indices cannot be flipped, and must be strictly decreasing/increasing. \n\n\nsource", 
            "title": "List of sequential constraints"
        }, 
        {
            "location": "/sampling_constraints/sequential_constraints/#increasing", 
            "text": "#  UncertainData . SamplingConstraints . StrictlyIncreasing     Type .  1 StrictlyIncreasing    A sampling constraint indicating element-wise sampling of the uncertain values in a dataset, such that the values of the draw are strictly increasing in magnitude.  Typically used when there are known, physical constraints on the measurements. For example, geochemical measurements of sediments at different depths of a sediment core  are taken at physically separate depths in the core. Thus, the order of the indices cannot be flipped, and must be strictly decreasing/increasing.   source", 
            "title": "Increasing"
        }, 
        {
            "location": "/sampling_constraints/sequential_constraints/#decreasing", 
            "text": "#  UncertainData . SamplingConstraints . StrictlyDecreasing     Type .  1 StrictlyDecreasing    A sampling constraint indicating element-wise sampling of the uncertain values in a dataset, such that the values of the draw are strictly decreasing in magnitude.  Typically used when there are known, physical constraints on the measurements. For example, geochemical measurements of sediments at different depths of a sediment core  are taken at physically separate depths in the core. Thus, the order of the indices cannot be flipped, and must be strictly decreasing/increasing.   source", 
            "title": "Decreasing"
        }, 
        {
            "location": "/sampling_constraints/ordered_sequence_exists/", 
            "text": "There are a few built-in functions to check if your dataset allows the application of  certain \nsequential sampling constraints\n. These functions will check  whether a valid sequence through your dataset exists, so that you can know beforehand  whether a particular resampling scheme is possible to apply to your data.\n\n\n\n\nStrictly increasing sequence\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nstrictly_increasing_sequence_exists\n \n \nFunction\n.\n\n\n1\n2\nstrictly_increasing_sequence_exists\n(\nudata\n::\nAbstractUncertainValueDataset\n;\n \n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nDoes a path through the dataset exist? I.e, check that a strictly  increasing sequence can be found after first  constraining each distribution to the provided quantile range (this  is necessary because some distributions may have infinite support).\n\n\nsource\n\n\n\n\nStrictly decreasing sequence\n\n\n#\n\n\nUncertainData\n.\nSamplingConstraints\n.\nstrictly_decreasing_sequence_exists\n \n \nFunction\n.\n\n\n1\n2\nstrictly_decreasing_sequence_exists\n(\nudata\n::\nAbstractUncertainValueDataset\n;\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nDoes a path through the dataset exist? I.e,  check that a strictly  decreasing sequence can be found after first  constraining each distribution to the provided quantile range (this  is necessary because some distributions may have infinite support).\n\n\nsource", 
            "title": "Existence of sequences"
        }, 
        {
            "location": "/sampling_constraints/ordered_sequence_exists/#strictly_increasing_sequence", 
            "text": "#  UncertainData . SamplingConstraints . strictly_increasing_sequence_exists     Function .  1\n2 strictly_increasing_sequence_exists ( udata :: AbstractUncertainValueDataset ;  \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Does a path through the dataset exist? I.e, check that a strictly  increasing sequence can be found after first  constraining each distribution to the provided quantile range (this  is necessary because some distributions may have infinite support).  source", 
            "title": "Strictly increasing sequence"
        }, 
        {
            "location": "/sampling_constraints/ordered_sequence_exists/#strictly_decreasing_sequence", 
            "text": "#  UncertainData . SamplingConstraints . strictly_decreasing_sequence_exists     Function .  1\n2 strictly_decreasing_sequence_exists ( udata :: AbstractUncertainValueDataset ; \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Does a path through the dataset exist? I.e,  check that a strictly  decreasing sequence can be found after first  constraining each distribution to the provided quantile range (this  is necessary because some distributions may have infinite support).  source", 
            "title": "Strictly decreasing sequence"
        }, 
        {
            "location": "/resampling/resampling_uncertain_values/", 
            "text": "Uncertain values may be resampled by drawing random number from the distributions furnishing them.\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\nresample\n(\nuv\n::\nAbstractUncertainValue\n)\n\n\n\n\n\n\n\nSample the uncertain value once, drawing values from the entire support of the probability  distribution furnishing it.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\nresample\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nSample the uncertain value \nn\n times, drawing values from the entire support of the  probability distribution furnishing it.\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\n\n\nResample once\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\nresample\n(\nuv_theoretical\n)\n\n\nresample\n(\nuv_theoretical_fitted\n)\n\n\nresample\n(\nuv_kde\n)\n\n\n\n\n\n\n\n\nResample n times\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\nn\n \n=\n \n500\n\n\nresample\n(\nuv_theoretical\n,\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nn\n)\n\n\n\n\n\n\n\n\nResampling can also be performed with constraints.\n\n\n\n\nresample\n(\nuv\n::\nAbstractUncertainValue\n,\n \nconstraint\n::\nSamplingConstraint\n)\n   samples the uncertain value once, drawing from a restricted   range of the support of the the probability distribution furnishing it.\n\n\nresample\n(\nuv\n::\nAbstractUncertainValue\n,\n \nconstraint\n::\nSamplingConstraint\n,\n \nn\n::\nInt\n)\n   samples the uncertain value \nn\n times, drawing values from a restricted   range of the support of the the probability distribution furnishing it.\n\n\n\n\nAvailable sampling constraints are:\n\n\n\n\nTruncateStd\n(\nn\n\u03c3\n::\nInt\n)\n\n\nTruncateMinimum\n(\nmin\n::\nNumber\n)\n\n\nTruncateMaximum\n(\nmax\n::\nNumber\n)\n\n\nTruncateRange\n(\nmin\n::\nNumber\n,\n \nmax\n::\nNumber\n)\n\n\nTruncateLowerQuantile\n(\nlower_quantile\n::\nFloat64\n)\n\n\nTruncateUpperQuantile\n(\nupper_quantile\n::\nFloat64\n)\n\n\nTruncateQuantiles\n(\nlower_quantile\n::\nFloat64\n,\n \nupper_quantile\n::\nFloat64\n)\n\n\n\n\nFor full documentation of the constraints, see the  \navailable constraints\n in the menu.\n\n\n\n\n\n\nLower quantile\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value with the restriction that the sampled\n\n\n# values must be higher than the 0.2-th quantile of the distribution\n\n\n# furnishing the value.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateLowerQuantile\n(\n0.2\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateLowerQuantile\n(\n0.2\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateLowerQuantile\n(\n0.2\n))\n\n\n\n\n\n\n\n\nUpper quantile\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value  with the restriction that the sampled\n\n\n# values must be lower than the 0.95-th quantile of the distribution\n\n\n# furnishing the value.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateUpperQuantile\n(\n0.95\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateUpperQuantile\n(\n0.95\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateUpperQuantile\n(\n0.95\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateUpperQuantile\n(\n0.95\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateUpperQuantile\n(\n0.95\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateUpperQuantile\n(\n0.95\n))\n\n\n\n\n\n\n\n\nQuantile range\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value with the restriction that the sampled\n\n\n# values must be within the (0.025, 0.975) quantile range.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateQuantiles\n(\n0.025\n,\n \n0.975\n))\n\n\n\n\n\n\n\n\nMinimum\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value with the restriction that the sampled\n\n\n# values have -2 as a lower bound.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateMinimum\n(\n-\n2\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateMinimum\n(\n-\n2\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateMinimum\n(\n-\n2\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateMinimum\n(\n-\n2\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateMinimum\n(\n-\n2\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateMinimum\n(\n-\n2\n))\n\n\n\n\n\n\n\n\nMaximum\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value with the restriction that the sampled\n\n\n# values have 3 as an upper bound.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateMaximum\n(\n3\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateMaximum\n(\n3\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateMaximum\n(\n3\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateMaximum\n(\n3\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateMaximum\n(\n3\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateMaximum\n(\n3\n))\n\n\n\n\n\n\n\n\nRange\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some uncertain values\n\n\nuv_theoretical\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n4\n,\n \n0.2\n)\n\n\nuv_theoretical_fitted\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nrand\n(\nNormal\n(\n1\n,\n \n0.2\n),\n \n1000\n))\n\n\nuv_kde\n \n=\n \nUncertainValue\n(\nrand\n(\nGamma\n(\n4\n,\n \n5\n),\n \n1000\n))\n\n\n\n# Resample the uncertain value with the restriction that the sampled\n\n\n# values must have values on the interval [-1, 1]. We first sample once,\n\n\n# then 50 times.\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n))\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n))\n\n\nresample\n(\nuv_kde\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n))\n\n\n\nn\n \n=\n \n100\n\n\nresample\n(\nuv_theoretical\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n),\n \nn\n)\n\n\nresample\n(\nuv_theoretical_fitted\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n),\n \nn\n)\n\n\nresample\n(\nuv_kde\n,\n \nTruncateRange\n(\n-\n1\n,\n \n1\n))", 
            "title": "Resampling uncertain values"
        }, 
        {
            "location": "/resampling/resampling_uncertain_values/#documentation", 
            "text": "#  UncertainData . Resampling . resample     Method .  1 resample ( uv :: AbstractUncertainValue )    Sample the uncertain value once, drawing values from the entire support of the probability  distribution furnishing it.  source  #  UncertainData . Resampling . resample     Method .  1 resample ( uv :: AbstractUncertainValue ,   n :: Int )    Sample the uncertain value  n  times, drawing values from the entire support of the  probability distribution furnishing it.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/resampling/resampling_uncertain_values/#examples", 
            "text": "Resample once   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  resample ( uv_theoretical )  resample ( uv_theoretical_fitted )  resample ( uv_kde )     Resample n times   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  n   =   500  resample ( uv_theoretical ,   n )  resample ( uv_theoretical_fitted ,   n )  resample ( uv_kde ,   n )     Resampling can also be performed with constraints.   resample ( uv :: AbstractUncertainValue ,   constraint :: SamplingConstraint )    samples the uncertain value once, drawing from a restricted   range of the support of the the probability distribution furnishing it.  resample ( uv :: AbstractUncertainValue ,   constraint :: SamplingConstraint ,   n :: Int )    samples the uncertain value  n  times, drawing values from a restricted   range of the support of the the probability distribution furnishing it.   Available sampling constraints are:   TruncateStd ( n \u03c3 :: Int )  TruncateMinimum ( min :: Number )  TruncateMaximum ( max :: Number )  TruncateRange ( min :: Number ,   max :: Number )  TruncateLowerQuantile ( lower_quantile :: Float64 )  TruncateUpperQuantile ( upper_quantile :: Float64 )  TruncateQuantiles ( lower_quantile :: Float64 ,   upper_quantile :: Float64 )   For full documentation of the constraints, see the   available constraints  in the menu.    Lower quantile   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value with the restriction that the sampled  # values must be higher than the 0.2-th quantile of the distribution  # furnishing the value.  resample ( uv_theoretical ,   TruncateLowerQuantile ( 0.2 ))  resample ( uv_theoretical_fitted ,   TruncateLowerQuantile ( 0.2 ))  resample ( uv_kde ,   TruncateLowerQuantile ( 0.2 ))  n   =   100  resample ( uv_theoretical ,   TruncateLowerQuantile ( 0.2 ),   n )  resample ( uv_theoretical_fitted ,   TruncateLowerQuantile ( 0.2 ),   n )  resample ( uv_kde ,   TruncateLowerQuantile ( 0.2 ))     Upper quantile   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value  with the restriction that the sampled  # values must be lower than the 0.95-th quantile of the distribution  # furnishing the value.  resample ( uv_theoretical ,   TruncateUpperQuantile ( 0.95 ))  resample ( uv_theoretical_fitted ,   TruncateUpperQuantile ( 0.95 ))  resample ( uv_kde ,   TruncateUpperQuantile ( 0.95 ))  n   =   100  resample ( uv_theoretical ,   TruncateUpperQuantile ( 0.95 ),   n )  resample ( uv_theoretical_fitted ,   TruncateUpperQuantile ( 0.95 ),   n )  resample ( uv_kde ,   TruncateUpperQuantile ( 0.95 ))     Quantile range   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value with the restriction that the sampled  # values must be within the (0.025, 0.975) quantile range.  resample ( uv_theoretical ,   TruncateQuantiles ( 0.025 ,   0.975 ))  resample ( uv_theoretical_fitted ,   TruncateQuantiles ( 0.025 ,   0.975 ))  resample ( uv_kde ,   TruncateQuantiles ( 0.025 ,   0.975 ))  n   =   100  resample ( uv_theoretical ,   TruncateQuantiles ( 0.025 ,   0.975 ),   n )  resample ( uv_theoretical_fitted ,   TruncateQuantiles ( 0.025 ,   0.975 ),   n )  resample ( uv_kde ,   TruncateQuantiles ( 0.025 ,   0.975 ))     Minimum   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value with the restriction that the sampled  # values have -2 as a lower bound.  resample ( uv_theoretical ,   TruncateMinimum ( - 2 ))  resample ( uv_theoretical_fitted ,   TruncateMinimum ( - 2 ))  resample ( uv_kde ,   TruncateMinimum ( - 2 ))  n   =   100  resample ( uv_theoretical ,   TruncateMinimum ( - 2 ),   n )  resample ( uv_theoretical_fitted ,   TruncateMinimum ( - 2 ),   n )  resample ( uv_kde ,   TruncateMinimum ( - 2 ))     Maximum   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value with the restriction that the sampled  # values have 3 as an upper bound.  resample ( uv_theoretical ,   TruncateMaximum ( 3 ))  resample ( uv_theoretical_fitted ,   TruncateMaximum ( 3 ))  resample ( uv_kde ,   TruncateMaximum ( 3 ))  n   =   100  resample ( uv_theoretical ,   TruncateMaximum ( 3 ),   n )  resample ( uv_theoretical_fitted ,   TruncateMaximum ( 3 ),   n )  resample ( uv_kde ,   TruncateMaximum ( 3 ))     Range   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 using   Distributions ,   UncertainData  # Generate some uncertain values  uv_theoretical   =   UncertainValue ( Normal ,   4 ,   0.2 )  uv_theoretical_fitted   =   UncertainValue ( Normal ,   rand ( Normal ( 1 ,   0.2 ),   1000 ))  uv_kde   =   UncertainValue ( rand ( Gamma ( 4 ,   5 ),   1000 ))  # Resample the uncertain value with the restriction that the sampled  # values must have values on the interval [-1, 1]. We first sample once,  # then 50 times.  resample ( uv_theoretical ,   TruncateRange ( - 1 ,   1 ))  resample ( uv_theoretical_fitted ,   TruncateRange ( - 1 ,   1 ))  resample ( uv_kde ,   TruncateRange ( - 1 ,   1 ))  n   =   100  resample ( uv_theoretical ,   TruncateRange ( - 1 ,   1 ),   n )  resample ( uv_theoretical_fitted ,   TruncateRange ( - 1 ,   1 ),   n )  resample ( uv_kde ,   TruncateRange ( - 1 ,   1 ))", 
            "title": "Examples"
        }, 
        {
            "location": "/resampling/resampling_uncertain_datasets/", 
            "text": "Uncertain datasets are resampled by element-wise sampling the furnishing distributions  of the uncertain values in the dataset. \n\n\nYou may sample the dataset as it is, or apply  \nsampling constraints\n that limit the  support of the individual data value distributions.\n\n\nNote: for datasets where both indices and values are uncertain, see  \nuncertain index-value datasets\n.\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\nresample\n(\nuv\n::\nUncertainDataset\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nResample an uncertain value dataset in an element-wise manner. \n\n\nDraws values from the entire support of the furnishing distributions.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\nresample\n(\nuv\n::\nUncertainDataset\n,\n \nn\n::\nInt\n)\n \n-\n \nVector\n{\nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample \nn\n realizations of an uncertain value dataset in an element-wise manner. \n\n\nDraws values from the entire support of the furnishing distributions.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}})\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nResample an uncertain value dataset in an element-wise manner. \n\n\nEnforces the provided sampling \nconstraint\n(s) to each of the data values, possibly  truncating the support of the furnishing distributions from which values are drawn.\n\n\nIf a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to the data values.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n\n    \nn\n::\nInt\n)\n \n-\n \nVector\n{\nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample \nn\n realizations of an uncertain value dataset in an element-wise manner. \n\n\nEnforces the provided sampling \nconstraint\n(s) to each of the data values, possibly  truncating the support of the furnishing distributions from which values are drawn.\n\n\nIf a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to the data values.\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nResampling with sampling constraints\n\n\nConsider the following example where we had a bunch of different measurements. \n\n\nThe first ten measurements (\nr1\n) are normally distributed values with mean \n\u03bc\n \n=\n \n0\n \n\u00b1\n \n0\n.\n4\n  and standard deviation \n\u03c3\n \n=\n \n0\n.\n5\n \n\u00b1\n \n0\n.\n1\n. The next measurement \nr2\n is actually a sample  consisting of 9850 replicates. Upon plotting it, we see that it has some complex  distribution which  we have to estimate using a kernel density approach (calling  \nUncertainValue\n without any additional argument triggers kernel density estimation).  Next, we have distribution \nr3\n that upon plotting looks uniform, so we approximate it by a  uniform distribution. Finally, the last two uncertain values \nr4\n and \nr5\n are represented  by a normal and a gamma distribution with known parameters.\n\n\nTo plot these data, we gather them in an \nUncertainDataset\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\ndist1\n \n=\n \nUniform\n(\n-\n0.4\n,\n \n0.4\n)\n\n\ndist2\n \n=\n \nUniform\n(\n-\n0.1\n,\n \n0.1\n)\n\n\nr1\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \n0\n \n+\n \nrand\n(\ndist\n),\n \n0.5\n \n+\n \nrand\n(\ndist2\n))\n \nfor\n \ni\n \n=\n \n1\n:\n10\n]\n\n \n# now drawn from a uniform distribution, but simulates \n\n\nr2\n \n=\n \nUncertainValue\n(\nrand\n(\n9850\n))\n\n\nr3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\n10000\n))\n\n\nr4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n0.1\n,\n \n0.5\n)\n\n\nr5\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n0.4\n,\n \n0.8\n)\n\n\n\nuvals\n \n=\n \n[\nr1\n;\n \nr2\n;\n \nr3\n;\n \nr4\n;\n \nr5\n]\n\n\nudata\n \n=\n \nUncertainDataset\n(\nuvals\n);\n\n\n\n\n\n\n\nBy default, the plot recipe for uncertain datasets will plot the median value with the  33\nrd\n to 67\nth\n percentile range (roughly equivalent to a one standard deviation for  normally distributed values). You may change the percentile range by providing a two-element vector to the plot function.\n\n\nLet's demonstrate this by creating a function that plots the uncertain values with  errors bars covering the 0.1\nst\n to 99.9\nth\n, the 5\nth\n to 95\nth\n, and the 33\nrd\n to 67\nth\n percentile  ranges. The function will also take a sampling constraint, then resample the dataset  a number of times and plot the individual realizations as lines. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nusing\n \nUncertainData\n,\n \nPlots\n\n\n\nfunction\n \nresample_plot\n(\ndata\n,\n \nsampling_constraint\n;\n \nn_resample_draws\n \n=\n \n40\n)\n \n    \np\n \n=\n \nplot\n(\nlw\n \n=\n \n0.5\n)\n\n    \nscatter!\n(\ndata\n,\n \n[\n0.001\n,\n \n0.999\n],\n \nseriescolor\n \n=\n \n:\nblack\n)\n\n    \nscatter!\n(\ndata\n,\n \n[\n0.05\n,\n \n0.95\n],\n \nseriescolor\n \n=\n \n:\nred\n)\n\n    \nscatter!\n(\ndata\n,\n \n[\n0.33\n,\n \n0.67\n],\n \nseriescolor\n \n=\n \n:\ngreen\n)\n\n\n    \nplot!\n(\nresample\n(\ndata\n,\n \nsampling_constraint\n,\n \nn_resample_draws\n),\n \n        \nlc\n \n=\n \n:\nblack\n,\n \nlw\n \n=\n \n0.3\n,\n \nl\u03b1\n \n=\n \n0.5\n)\n\n    \nreturn\n \np\n\n\nend\n\n\n\n# Now, resample using some different constraints and compare the plots\n\n\np1\n \n=\n \nresample_plot\n(\nudata\n,\n \nNoConstraint\n())\n\n\ntitle!\n(\nNo constraints\n)\n\n\np2\n \n=\n \nresample_plot\n(\nudata\n,\n \nTruncateQuantiles\n(\n0.05\n,\n \n0.95\n))\n\n\ntitle!\n(\n5th to 95th quantile range\n)\n\n\np3\n \n=\n \nresample_plot\n(\nudata\n,\n \nTruncateQuantiles\n(\n0.33\n,\n \n0.67\n))\n\n\ntitle!\n(\n33th to 67th quantile range\n)\n\n\np4\n \n=\n \nresample_plot\n(\nudata\n,\n \nTruncateMaximum\n(\n0.7\n))\n\n\ntitle!\n(\nTruncate at maximum value = 0.7\n)\n\n\n\nplot\n(\np1\n,\n \np2\n,\n \np3\n,\n \np4\n,\n \nlayout\n \n=\n \n(\n4\n,\n \n1\n),\n \ntitlefont\n \n=\n \nfont\n(\n8\n))\n\n\n\n\n\n\n\nThis produces the following plot:\n\n\n\n\n\n\nWhat happens when applying invalid constraints to a dataset?\n\n\nIn the example above, the resampling worked fine because all the constraints were  applicable to the data. However, it could happen that the constraint is not applicable  to all uncertain values in the dataset. For example, applying a \nTruncateMaximum\n(\n2\n)\n  constraint to an uncertain value \nu\n defined by \nu\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n4\n,\n \n5\n)\n would  not work, because the support of \nu\n would be empty after applying the constraint.\n\n\nTo check if a constraint yields a nonempty truncated uncertain value, use the  \nsupport_intersection\n function. If the result of \n`\nsupport_intersection\n(\nuval1\n,\n \nuval2\n)\n  for two uncertain values \nuval1\n and \nuval2\n is the empty set \n\u2205\n, then you'll run into  trouble.\n\n\nTo check for such cases for an entire dataset, you can use the  \nverify_constraints\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \nconstraint\n::\nSamplingConstraint\n)\n  function. It will apply the constraint to each value and return the indices of the values  for which applying the constraint would result in a furnishing distribution whose support  is the empty set.", 
            "title": "Resampling uncertain datasets"
        }, 
        {
            "location": "/resampling/resampling_uncertain_datasets/#documentation", 
            "text": "#  UncertainData . Resampling . resample     Method .  1 resample ( uv :: UncertainDataset )   -   Vector { Float64 }    Resample an uncertain value dataset in an element-wise manner.   Draws values from the entire support of the furnishing distributions.  source  #  UncertainData . Resampling . resample     Method .  1 resample ( uv :: UncertainDataset ,   n :: Int )   -   Vector { Vector { Float64 }}    Resample  n  realizations of an uncertain value dataset in an element-wise manner.   Draws values from the entire support of the furnishing distributions.  source  #  UncertainData . Resampling . resample     Method .  1\n2 resample ( udata :: AbstractUncertainValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }})   -   Vector { Float64 }    Resample an uncertain value dataset in an element-wise manner.   Enforces the provided sampling  constraint (s) to each of the data values, possibly  truncating the support of the furnishing distributions from which values are drawn.  If a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to the data values.  source  #  UncertainData . Resampling . resample     Method .  1\n2\n3 resample ( udata :: AbstractUncertainValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }}, \n     n :: Int )   -   Vector { Vector { Float64 }}    Resample  n  realizations of an uncertain value dataset in an element-wise manner.   Enforces the provided sampling  constraint (s) to each of the data values, possibly  truncating the support of the furnishing distributions from which values are drawn.  If a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to the data values.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/resampling/resampling_uncertain_datasets/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/resampling/resampling_uncertain_datasets/#resampling_with_sampling_constraints", 
            "text": "Consider the following example where we had a bunch of different measurements.   The first ten measurements ( r1 ) are normally distributed values with mean  \u03bc   =   0   \u00b1   0 . 4   and standard deviation  \u03c3   =   0 . 5   \u00b1   0 . 1 . The next measurement  r2  is actually a sample  consisting of 9850 replicates. Upon plotting it, we see that it has some complex  distribution which  we have to estimate using a kernel density approach (calling   UncertainValue  without any additional argument triggers kernel density estimation).  Next, we have distribution  r3  that upon plotting looks uniform, so we approximate it by a  uniform distribution. Finally, the last two uncertain values  r4  and  r5  are represented  by a normal and a gamma distribution with known parameters.  To plot these data, we gather them in an  UncertainDataset .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 dist1   =   Uniform ( - 0.4 ,   0.4 )  dist2   =   Uniform ( - 0.1 ,   0.1 )  r1   =   [ UncertainValue ( Normal ,   0   +   rand ( dist ),   0.5   +   rand ( dist2 ))   for   i   =   1 : 10 ] \n  # now drawn from a uniform distribution, but simulates   r2   =   UncertainValue ( rand ( 9850 ))  r3   =   UncertainValue ( Uniform ,   rand ( 10000 ))  r4   =   UncertainValue ( Normal ,   - 0.1 ,   0.5 )  r5   =   UncertainValue ( Gamma ,   0.4 ,   0.8 )  uvals   =   [ r1 ;   r2 ;   r3 ;   r4 ;   r5 ]  udata   =   UncertainDataset ( uvals );    By default, the plot recipe for uncertain datasets will plot the median value with the  33 rd  to 67 th  percentile range (roughly equivalent to a one standard deviation for  normally distributed values). You may change the percentile range by providing a two-element vector to the plot function.  Let's demonstrate this by creating a function that plots the uncertain values with  errors bars covering the 0.1 st  to 99.9 th , the 5 th  to 95 th , and the 33 rd  to 67 th  percentile  ranges. The function will also take a sampling constraint, then resample the dataset  a number of times and plot the individual realizations as lines.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 using   UncertainData ,   Plots  function   resample_plot ( data ,   sampling_constraint ;   n_resample_draws   =   40 )  \n     p   =   plot ( lw   =   0.5 ) \n     scatter! ( data ,   [ 0.001 ,   0.999 ],   seriescolor   =   : black ) \n     scatter! ( data ,   [ 0.05 ,   0.95 ],   seriescolor   =   : red ) \n     scatter! ( data ,   [ 0.33 ,   0.67 ],   seriescolor   =   : green ) \n\n     plot! ( resample ( data ,   sampling_constraint ,   n_resample_draws ),  \n         lc   =   : black ,   lw   =   0.3 ,   l\u03b1   =   0.5 ) \n     return   p  end  # Now, resample using some different constraints and compare the plots  p1   =   resample_plot ( udata ,   NoConstraint ())  title! ( No constraints )  p2   =   resample_plot ( udata ,   TruncateQuantiles ( 0.05 ,   0.95 ))  title! ( 5th to 95th quantile range )  p3   =   resample_plot ( udata ,   TruncateQuantiles ( 0.33 ,   0.67 ))  title! ( 33th to 67th quantile range )  p4   =   resample_plot ( udata ,   TruncateMaximum ( 0.7 ))  title! ( Truncate at maximum value = 0.7 )  plot ( p1 ,   p2 ,   p3 ,   p4 ,   layout   =   ( 4 ,   1 ),   titlefont   =   font ( 8 ))    This produces the following plot:", 
            "title": "Resampling with sampling constraints"
        }, 
        {
            "location": "/resampling/resampling_uncertain_datasets/#what_happens_when_applying_invalid_constraints_to_a_dataset", 
            "text": "In the example above, the resampling worked fine because all the constraints were  applicable to the data. However, it could happen that the constraint is not applicable  to all uncertain values in the dataset. For example, applying a  TruncateMaximum ( 2 )   constraint to an uncertain value  u  defined by  u   =   UncertainValue ( Uniform ,   4 ,   5 )  would  not work, because the support of  u  would be empty after applying the constraint.  To check if a constraint yields a nonempty truncated uncertain value, use the   support_intersection  function. If the result of  ` support_intersection ( uval1 ,   uval2 )   for two uncertain values  uval1  and  uval2  is the empty set  \u2205 , then you'll run into  trouble.  To check for such cases for an entire dataset, you can use the   verify_constraints ( udata :: AbstractUncertainValueDataset ,   constraint :: SamplingConstraint )   function. It will apply the constraint to each value and return the indices of the values  for which applying the constraint would result in a furnishing distribution whose support  is the empty set.", 
            "title": "What happens when applying invalid constraints to a dataset?"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/", 
            "text": "Resampling \nUncertainIndexValueDataset\ns is done in the same manner as for uncertain  values and \nUncertainDatasets\n. \n\n\nSee also the list of  \navailable sampling constraints\n.\n\n\n\n\nDocumentation\n\n\n\n\nNo constraints\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n)\n \n-\n \nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample an uncertain index-value dataset in an element-wise manner.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nn\n::\nInt\n)\n \n-\n \nVector\n{\nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}}\n\n\n\n\n\n\n\nResample \nn\n realizations an uncertain index-value dataset in an element-wise manner. \n\n\nsource\n\n\n\n\nSame constraint to both indices and data values\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}})\n \n-\n \nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample an uncertain index-value dataset in an element-wise manner. \n\n\nEnforces the provided sampling \nconstraint\n to all uncertain values in the dataset, both  indices and data values.\n\n\nIf a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to both the indices and the data values.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n\n    \nn\n::\nInt\n)\n \n-\n \nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample \nn\n realizations of an uncertain index-value dataset in an element-wise manner. \n\n\nEnforces the provided sampling \nconstraint\n to all uncertain values in the dataset, both  indices and data values.\n\n\nIf a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to both the indices and the data values.\n\n\nsource\n\n\n\n\nDifferent constraints to indices and data values\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nconstraint_idxs\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n \n    \nconstraint_vals\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}})\n \n-\n \nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}\n\n\n\n\n\n\n\nResample an uncertain index-value dataset in an element-wise manner. \n\n\nEnforces separate sampling constraints to the indices and to the data values. \n\n\nIf a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\n4\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nconstraint_idxs\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n \n    \nconstraint_vals\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n\n    \nn\n::\nInt\n)\n \n-\n \nVector\n{\nTuple\n{\nVector\n{\nFloat64\n},\n \nVector\n{\nFloat64\n}}}\n\n\n\n\n\n\n\nResample \nn\n realizations of an uncertain index-value dataset in an element-wise manner. \n\n\nEnforces separate sampling constraints to the indices and to the data values. \n\n\nIf a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise.\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nSame constraint for all uncertain values\n\n\nFirst, let's define some data to work on.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nusing\n \nUncertainData\n,\n \nPlots\n \n\ngr\n()\n\n\nr1\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nrand\n(),\n \nrand\n())\n \nfor\n \ni\n \n=\n \n1\n:\n10\n]\n\n\nr2\n \n=\n \nUncertainValue\n(\nrand\n(\n10000\n))\n\n\nr3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\n10000\n))\n\n\nr4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n0.1\n,\n \n0.5\n)\n\n\nr5\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n0.4\n,\n \n0.8\n)\n\n\n\nu_values\n \n=\n \n[\nr1\n;\n \nr2\n;\n \nr3\n;\n \nr4\n;\n \nr5\n]\n\n\nu_timeindices\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \ni\n,\n \nrand\n(\nUniform\n(\n0\n,\n \n1\n)))\n \nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nu_values\n)]\n\n\nuindices\n \n=\n \nUncertainDataset\n(\nu_timeindices\n);\n\n\nudata\n \n=\n \nUncertainDataset\n(\nu_values\n);\n\n\n\n# Now, gather uncertain indices and uncertain data values\n\n\nx\n \n=\n \nUncertainIndexValueDataset\n(\nuindices\n,\n \nudata\n)\n\n\n\n\n\n\n\nBy default, the plot recipe shows the median and 33\nrd\n to 67\nth\n percentile range error bars.  Let's use the default plot recipe, and add some line plots with resampled realizations  of the dataset. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\np\n \n=\n \nplot\n(\nx\n)\n \n\n\nfor\n \ni\n \n=\n \n1\n:\n100\n\n    \ns\n \n=\n \nresample\n(\nx\n,\n \nTruncateQuantiles\n(\n0.33\n,\n \n0.67\n),\n \nTruncateQuantiles\n(\n0.33\n,\n \n0.67\n))\n\n    \nscatter!\n(\np\n,\n \ns\n[\n1\n],\n \ns\n[\n2\n],\n \nlabel\n \n=\n \n,\n \nlw\n \n=\n \n0.3\n,\n \nl\u03b1\n \n=\n \n0.1\n,\n \nlc\n \n=\n \n:\nblack\n,\n\n            \nmc\n \n=\n \n:\nblack\n,\n \nms\n \n=\n \n0.5\n,\n \nm\u03b1\n \n=\n \n0.4\n)\n\n    \nplot!\n(\np\n,\n \ns\n[\n1\n],\n \ns\n[\n2\n],\n \nlabel\n \n=\n \n,\n \nlw\n \n=\n \n0.3\n,\n \nl\u03b1\n \n=\n \n0.1\n,\n \nlc\n \n=\n \n:\nblack\n,\n\n            \nmc\n \n=\n \n:\nblack\n,\n \nms\n \n=\n \n0.5\n,\n \nm\u03b1\n \n=\n \n0.4\n)\n\n\nend\n\n\np\n\n\n\n\n\n\n\n\n\nThis would of course also work with any other sampling constraint that is valid for your  dataset. Let's demonstrate with a few more examples.\n\n\n\n\nDifferent constraints for indices and data values\n\n\nLet's say that we want to treat the uncertainties of the indices (time, in this case)  separately from the uncertainties of the data values. \n\n\nFirst, let's define a dataset to work on.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nusing\n \nUncertainData\n,\n \nPlots\n \n\ngr\n()\n\n\nr1\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \nrand\n(),\n \nrand\n())\n \nfor\n \ni\n \n=\n \n1\n:\n10\n]\n\n\nr2\n \n=\n \nUncertainValue\n(\nrand\n(\n10000\n))\n\n\nr3\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nrand\n(\n10000\n))\n\n\nr4\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n0.1\n,\n \n0.5\n)\n\n\nr5\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n0.4\n,\n \n0.8\n)\n\n\n\nu_values\n \n=\n \n[\nr1\n;\n \nr2\n;\n \nr3\n;\n \nr4\n;\n \nr5\n]\n\n\nu_timeindices\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \ni\n,\n \nrand\n(\nUniform\n(\n0\n,\n \n1\n)))\n \nfor\n \ni\n \n=\n \n1\n:\nlength\n(\nu_values\n)]\n\n\nuindices\n \n=\n \nUncertainDataset\n(\nu_timeindices\n);\n\n\nudata\n \n=\n \nUncertainDataset\n(\nu_values\n);\n\n\n\n# Now, gather uncertain indices and uncertain data values\n\n\nx\n \n=\n \nUncertainIndexValueDataset\n(\nuindices\n,\n \nudata\n)\n\n\n\n\n\n\n\nLet's pretend every 2\nnd\n time index has many outliers which we don't trust, so we restrict  resampling of those values to the 30\nth\n to 70\nth\n percentile range. For the remaining time  indices, there are some outliers outliers, but these are concentrated at the lower end of  the distributions, so we'll resample by truncating the furnishing distributions below at  the 10\nth\n percentile. \n\n\nFor the data values, we pretend that the same applies: every 2\nnd\n value has a bunch of  outliers, so we restrict the support of the distributions of those uncertain values to  1.5 standard deviations around the mean. For the remaining data values, we'll resample  from the the 20\nth\n to 80\nth\n percentile range.\n\n\nNow, define the constraints as described:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Define the constraints\n\n\nn_vals\n \n=\n \nlength\n(\nx\n)\n\n\n\nindex_constraints\n \n=\n \nVector\n{\nSamplingConstraint\n}(\nundef\n,\n \nn_vals\n)\n\n\nvalue_constraints\n \n=\n \nVector\n{\nSamplingConstraint\n}(\nundef\n,\n \nn_vals\n)\n\n\n\nfor\n \ni\n \n=\n \n1\n:\nn_vals\n\n    \nif\n \ni\n \n%\n \n2\n \n==\n \n0\n\n        \nindex_constraints\n[\ni\n]\n \n=\n \nTruncateQuantiles\n(\n0.3\n,\n \n0.7\n)\n\n        \nvalue_constraints\n[\ni\n]\n \n=\n \nTruncateStd\n(\n1.5\n)\n\n    \nelse\n\n        \nindex_constraints\n[\ni\n]\n \n=\n \nTruncateLowerQuantile\n(\n0.1\n)\n\n        \nvalue_constraints\n[\ni\n]\n \n=\n \nTruncateQuantiles\n(\n0.2\n,\n \n0.8\n)\n  \n    \nend\n\n\nend\n\n\n\n\n\n\n\nFinally, plot the realizations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Resample a bunch of times and plot the realizations both as lines as scatter points\n\n\np\n \n=\n \nplot\n(\nxlabel\n \n=\n \nIndex\n,\n \nylabel\n \n=\n \nValue\n)\n\n\nfor\n \ni\n \n=\n \n1\n:\n500\n\n    \ns\n \n=\n \nresample\n(\nx\n,\n \nindex_constraints\n,\n \nvalue_constraints\n)\n\n    \nscatter!\n(\np\n,\n \ns\n[\n1\n],\n \ns\n[\n2\n],\n \nlabel\n \n=\n \n,\n \nlw\n \n=\n \n0.3\n,\n \nl\u03b1\n \n=\n \n0.1\n,\n \nlc\n \n=\n \n:\nblack\n,\n\n            \nmc\n \n=\n \n:\nblack\n,\n \nms\n \n=\n \n0.5\n,\n \nm\u03b1\n \n=\n \n0.4\n)\n\n    \nplot!\n(\np\n,\n \ns\n[\n1\n],\n \ns\n[\n2\n],\n \nlabel\n \n=\n \n,\n \nlw\n \n=\n \n0.3\n,\n \nl\u03b1\n \n=\n \n0.1\n,\n \nlc\n \n=\n \n:\nblack\n,\n\n            \nmc\n \n=\n \n:\nblack\n,\n \nms\n \n=\n \n0.5\n,\n \nm\u03b1\n \n=\n \n0.4\n)\n\n\nend\n\n\np", 
            "title": "Resampling uncertain index-value datasets"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#no_constraints", 
            "text": "#  UncertainData . Resampling . resample     Method .  1 resample ( udata :: UncertainIndexValueDataset )   -   Tuple { Vector { Float64 },   Vector { Float64 }}    Resample an uncertain index-value dataset in an element-wise manner.  source  #  UncertainData . Resampling . resample     Method .  1\n2 resample ( udata :: UncertainIndexValueDataset ,  \n     n :: Int )   -   Vector { Tuple { Vector { Float64 },   Vector { Float64 }}}    Resample  n  realizations an uncertain index-value dataset in an element-wise manner.   source", 
            "title": "No constraints"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#same_constraint_to_both_indices_and_data_values", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2 resample ( udata :: UncertainIndexValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }})   -   Tuple { Vector { Float64 },   Vector { Float64 }}    Resample an uncertain index-value dataset in an element-wise manner.   Enforces the provided sampling  constraint  to all uncertain values in the dataset, both  indices and data values.  If a single constraint is provided, then that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to both the indices and the data values.  source  #  UncertainData . Resampling . resample     Method .  1\n2\n3 resample ( udata :: UncertainIndexValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }}, \n     n :: Int )   -   Tuple { Vector { Float64 },   Vector { Float64 }}    Resample  n  realizations of an uncertain index-value dataset in an element-wise manner.   Enforces the provided sampling  constraint  to all uncertain values in the dataset, both  indices and data values.  If a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise to both the indices and the data values.  source", 
            "title": "Same constraint to both indices and data values"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#different_constraints_to_indices_and_data_values", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2\n3 resample ( udata :: UncertainIndexValueDataset ,  \n     constraint_idxs :: Union { SamplingConstraint ,   Vector { SamplingConstraint }},  \n     constraint_vals :: Union { SamplingConstraint ,   Vector { SamplingConstraint }})   -   Tuple { Vector { Float64 },   Vector { Float64 }}    Resample an uncertain index-value dataset in an element-wise manner.   Enforces separate sampling constraints to the indices and to the data values.   If a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise.  source  #  UncertainData . Resampling . resample     Method .  1\n2\n3\n4 resample ( udata :: UncertainIndexValueDataset ,  \n     constraint_idxs :: Union { SamplingConstraint ,   Vector { SamplingConstraint }},  \n     constraint_vals :: Union { SamplingConstraint ,   Vector { SamplingConstraint }}, \n     n :: Int )   -   Vector { Tuple { Vector { Float64 },   Vector { Float64 }}}    Resample  n  realizations of an uncertain index-value dataset in an element-wise manner.   Enforces separate sampling constraints to the indices and to the data values.   If a single constraint is provided, that constraint will be applied to all values. If a  vector of constraints (as many as there are values) is provided, then the constraints are  applied element-wise.  source", 
            "title": "Different constraints to indices and data values"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#same_constraint_for_all_uncertain_values", 
            "text": "First, let's define some data to work on.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 using   UncertainData ,   Plots   gr ()  r1   =   [ UncertainValue ( Normal ,   rand (),   rand ())   for   i   =   1 : 10 ]  r2   =   UncertainValue ( rand ( 10000 ))  r3   =   UncertainValue ( Uniform ,   rand ( 10000 ))  r4   =   UncertainValue ( Normal ,   - 0.1 ,   0.5 )  r5   =   UncertainValue ( Gamma ,   0.4 ,   0.8 )  u_values   =   [ r1 ;   r2 ;   r3 ;   r4 ;   r5 ]  u_timeindices   =   [ UncertainValue ( Normal ,   i ,   rand ( Uniform ( 0 ,   1 )))   for   i   =   1 : length ( u_values )]  uindices   =   UncertainDataset ( u_timeindices );  udata   =   UncertainDataset ( u_values );  # Now, gather uncertain indices and uncertain data values  x   =   UncertainIndexValueDataset ( uindices ,   udata )    By default, the plot recipe shows the median and 33 rd  to 67 th  percentile range error bars.  Let's use the default plot recipe, and add some line plots with resampled realizations  of the dataset.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 p   =   plot ( x )   for   i   =   1 : 100 \n     s   =   resample ( x ,   TruncateQuantiles ( 0.33 ,   0.67 ),   TruncateQuantiles ( 0.33 ,   0.67 )) \n     scatter! ( p ,   s [ 1 ],   s [ 2 ],   label   =   ,   lw   =   0.3 ,   l\u03b1   =   0.1 ,   lc   =   : black , \n             mc   =   : black ,   ms   =   0.5 ,   m\u03b1   =   0.4 ) \n     plot! ( p ,   s [ 1 ],   s [ 2 ],   label   =   ,   lw   =   0.3 ,   l\u03b1   =   0.1 ,   lc   =   : black , \n             mc   =   : black ,   ms   =   0.5 ,   m\u03b1   =   0.4 )  end  p     This would of course also work with any other sampling constraint that is valid for your  dataset. Let's demonstrate with a few more examples.", 
            "title": "Same constraint for all uncertain values"
        }, 
        {
            "location": "/resampling/resampling_uncertain_indexvalue_datasets/#different_constraints_for_indices_and_data_values", 
            "text": "Let's say that we want to treat the uncertainties of the indices (time, in this case)  separately from the uncertainties of the data values.   First, let's define a dataset to work on.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 using   UncertainData ,   Plots   gr ()  r1   =   [ UncertainValue ( Normal ,   rand (),   rand ())   for   i   =   1 : 10 ]  r2   =   UncertainValue ( rand ( 10000 ))  r3   =   UncertainValue ( Uniform ,   rand ( 10000 ))  r4   =   UncertainValue ( Normal ,   - 0.1 ,   0.5 )  r5   =   UncertainValue ( Gamma ,   0.4 ,   0.8 )  u_values   =   [ r1 ;   r2 ;   r3 ;   r4 ;   r5 ]  u_timeindices   =   [ UncertainValue ( Normal ,   i ,   rand ( Uniform ( 0 ,   1 )))   for   i   =   1 : length ( u_values )]  uindices   =   UncertainDataset ( u_timeindices );  udata   =   UncertainDataset ( u_values );  # Now, gather uncertain indices and uncertain data values  x   =   UncertainIndexValueDataset ( uindices ,   udata )    Let's pretend every 2 nd  time index has many outliers which we don't trust, so we restrict  resampling of those values to the 30 th  to 70 th  percentile range. For the remaining time  indices, there are some outliers outliers, but these are concentrated at the lower end of  the distributions, so we'll resample by truncating the furnishing distributions below at  the 10 th  percentile.   For the data values, we pretend that the same applies: every 2 nd  value has a bunch of  outliers, so we restrict the support of the distributions of those uncertain values to  1.5 standard deviations around the mean. For the remaining data values, we'll resample  from the the 20 th  to 80 th  percentile range.  Now, define the constraints as described:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Define the constraints  n_vals   =   length ( x )  index_constraints   =   Vector { SamplingConstraint }( undef ,   n_vals )  value_constraints   =   Vector { SamplingConstraint }( undef ,   n_vals )  for   i   =   1 : n_vals \n     if   i   %   2   ==   0 \n         index_constraints [ i ]   =   TruncateQuantiles ( 0.3 ,   0.7 ) \n         value_constraints [ i ]   =   TruncateStd ( 1.5 ) \n     else \n         index_constraints [ i ]   =   TruncateLowerQuantile ( 0.1 ) \n         value_constraints [ i ]   =   TruncateQuantiles ( 0.2 ,   0.8 )   \n     end  end    Finally, plot the realizations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Resample a bunch of times and plot the realizations both as lines as scatter points  p   =   plot ( xlabel   =   Index ,   ylabel   =   Value )  for   i   =   1 : 500 \n     s   =   resample ( x ,   index_constraints ,   value_constraints ) \n     scatter! ( p ,   s [ 1 ],   s [ 2 ],   label   =   ,   lw   =   0.3 ,   l\u03b1   =   0.1 ,   lc   =   : black , \n             mc   =   : black ,   ms   =   0.5 ,   m\u03b1   =   0.4 ) \n     plot! ( p ,   s [ 1 ],   s [ 2 ],   label   =   ,   lw   =   0.3 ,   l\u03b1   =   0.1 ,   lc   =   : black , \n             mc   =   : black ,   ms   =   0.5 ,   m\u03b1   =   0.4 )  end  p", 
            "title": "Different constraints for indices and data values"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/", 
            "text": "In addition to the  \ngeneric sampling constraints\n,  you may impose sequential sampling constraints when resampling an uncertain dataset. \n\n\n\n\nIs a particular constraint applicable?\n\n\nNot all \nsequential sampling constraints\n  may be applicable to your dataset. Use  \nthese functions\n to check whether a  particular constraint is possible to apply to your dataset. \n\n\n\n\nSyntax\n\n\n\n\nSequential constraint only\n\n\nA dataset may be sampling imposing a sequential sampling constraint, but leaving the  furnishing distributions untouched otherwise.\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nsequential_constraint\n::\nSequentialSamplingConstraint\n;\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nResample a dataset by imposing a sequential sampling constraint. \n\n\nBefore drawing the realization, all furnishing distributions are truncated to the provided  \nquantiles\n range. This is to avoid problems in case some distributions have infinite  support.\n\n\nsource\n\n\n\n\nRegular constraint(s) + sequential constraint\n\n\nAnother option is to first impose constraints on the furnishing distributions, then  applying the sequential sampling constraint.\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\n4\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n \n    \nsequential_constraint\n::\nSequentialSamplingConstraint\n;\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nResample a dataset by first imposing regular sampling constraints on the furnishing  distributions, then applying a sequential sampling constraint. \n\n\nBefore drawing the realization, all furnishing distributions are truncated to the provided  \nquantiles\n range. This is to avoid problems in case some distributions have infinite  support.\n\n\nsource\n\n\n\n\nList of sequential resampling schemes\n\n\n\n\nStrictlyIncreasing\n sequences.\n\n\nStrictlyDecreasing\n sequences.", 
            "title": "Overview"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/#is_a_particular_constraint_applicable", 
            "text": "Not all  sequential sampling constraints   may be applicable to your dataset. Use   these functions  to check whether a  particular constraint is possible to apply to your dataset.", 
            "title": "Is a particular constraint applicable?"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/#syntax", 
            "text": "", 
            "title": "Syntax"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/#sequential_constraint_only", 
            "text": "A dataset may be sampling imposing a sequential sampling constraint, but leaving the  furnishing distributions untouched otherwise.  #  UncertainData . Resampling . resample     Method .  1\n2\n3 resample ( udata :: AbstractUncertainValueDataset ,  \n     sequential_constraint :: SequentialSamplingConstraint ; \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Resample a dataset by imposing a sequential sampling constraint.   Before drawing the realization, all furnishing distributions are truncated to the provided   quantiles  range. This is to avoid problems in case some distributions have infinite  support.  source", 
            "title": "Sequential constraint only"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/#regular_constraints_sequential_constraint", 
            "text": "Another option is to first impose constraints on the furnishing distributions, then  applying the sequential sampling constraint.  #  UncertainData . Resampling . resample     Method .  1\n2\n3\n4 resample ( udata :: AbstractUncertainValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }},  \n     sequential_constraint :: SequentialSamplingConstraint ; \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Resample a dataset by first imposing regular sampling constraints on the furnishing  distributions, then applying a sequential sampling constraint.   Before drawing the realization, all furnishing distributions are truncated to the provided   quantiles  range. This is to avoid problems in case some distributions have infinite  support.  source", 
            "title": "Regular constraint(s) + sequential constraint"
        }, 
        {
            "location": "/resampling/sequential/resampling_uncertaindatasets_sequential/#list_of_sequential_resampling_schemes", 
            "text": "StrictlyIncreasing  sequences.  StrictlyDecreasing  sequences.", 
            "title": "List of sequential resampling schemes"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/", 
            "text": "The default constructor for a strictly increasing sequential sampling constraint is  \nStrictlyIncreasing\n. To specify how the sequence is sampled, provide an  \nOrderedSamplingAlgorithm\n as an argument to the constructor.\n\n\n\n\nCompatible ordering algorithms\n\n\n\n\nStrictlyIncreasing\n(\nStartToEnd\n())\n (the default)\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\n4\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n\n    \nsequential_constraint\n::\nStrictlyIncreasing\n{\nOrderedSamplingAlgorithm\n};\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nDraw a sequence of values strictly increasing in magnitude from the dataset, sampling  each of the furnishing distributions once each, after first truncating the supports  of the values in the dataset using the provided \nconstraint\ns.\n\n\nArguments:\n\n\n\n\nudata\n: An uncertain dataset.\n\n\nconstraint\n: Sampling constraint(s) to apply to each of the values in the dataset    before drawing the sequence of increasing values.\n\n\nsequential_constraint\n: An instance of a \nStrictlyIncreasing\n sequential    sampling constraint. For example, \nStrictlyIncreasing\n(\nStartToEnd\n())\n indicates    that a strictly increasing sequence should be created in one go from start to    finish (as opposed to chunking the data set first, then gluing partial sequences    together afterwards).\n\n\nquantiles\n: A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    increasing values. This deals with distributions with infinite support.\n\n\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\nresample\n(\nudata\n::\nDT\n,\n \nsequential_constraint\n::\nStrictlyIncreasing\n{\nT\n};\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n \nwhere\n \n{\nDT\n \n:\n \nAbstractUncertainValueDataset\n,\n \nT\n \n:\n \nStartToEnd\n}\n\n\n\n\n\n\n\nElement-wise resample the uncertain values in the dataset such that each preceding value  is strictly less in magnitude than the next one. \n\n\nArguments:\n\n\n\n\nudata\n: An uncertain dataset.\n\n\nsequential_constraint\n: An instance of a \nStrictlyIncreasing\n sequential    sampling constraint.\n\n\nordered_sampling_alg\n: An instance of a \nStartToEnd\n ordered    sampling constraint, indicating that the sequence of increasing values should    be created in one go from start to finish (as opposed to chunking the data set    first, then gluing partial sequences together afterwards).\n\n\nquantiles\n: A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    increasing values. This deals with distributions with infinite support.\n\n\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nExample 1: strictly increasing sequences\n\n\nLet's compare how the realizations look for the situation where no sequential sampling constraint is imposed versus enforcing strictly increasing sequences.\n\n\nWe start by creating some uncertain data with increasing magnitude and zero overlap between  values, so we're guaranteed that a strictly increasing sequence through the dataset exists.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nusing\n \nUncertainData\n,\n \nPlots\n \n\n\n\nN\n \n=\n \n10\n\n\nu_timeindices\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \ni\n,\n \nrand\n(\nUniform\n(\n0.1\n,\n \n2\n)))\n \nfor\n \ni\n \n=\n \n1\n:\nN\n]\n\n\nu\n \n=\n \nUncertainDataset\n(\nu_timeindices\n)\n\n\n\np_increasing\n \n=\n \nplot\n(\nu\n,\n \n[\n0.0001\n,\n \n0.9999\n],\n \nlegend\n \n=\n \nfalse\n,\n\n    \nxlabel\n \n=\n \nindex\n,\n \nylabel\n \n=\n \nvalue\n)\n\n\np_regular\n \n=\n \nplot\n(\nu\n,\n \n[\n0.0001\n,\n \n0.9999\n],\n \nlegend\n \n=\n \nfalse\n,\n\n    \nylabel\n \n=\n \nvalue\n,\n \nxaxis\n \n=\n \nfalse\n)\n\n\n\nfor\n \ni\n \n=\n \n1\n:\n1000\n\n    \nplot!\n(\np_increasing\n,\n \nresample\n(\nu\n,\n \nStrictlyIncreasing\n()),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.1\n)\n\n    \nplot!\n(\np_regular\n,\n \nresample\n(\nu\n),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.2\n)\n\n\nend\n \n\n\nplot\n(\np_regular\n,\n \np_increasing\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nlink\n \n=\n \n:\nx\n,\n \nsize\n \n=\n \n(\n400\n,\n \n500\n))\n\n\n\n\n\n\n\n\n\nValues of the realizations where strictly increasing sequences are imposed clearly are  limited by the next values in the dataset. For the regular sampling, however, realizations  jump wildly, with both positive and negative first differences.\n\n\n\n\nExample 2: regular constraints + strictly increasing sequences\n\n\nYou may also combine regular sampling constraints with sequential resampling schemes.  Here's one example. We use the same data as in example 1 above, but when drawing increasing  sequences, we only resample from within one standard deviation around the mean.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\np_increasing\n \n=\n \nplot\n(\nu\n,\n \n[\n0.0001\n,\n \n0.9999\n],\n \nlegend\n \n=\n \nfalse\n,\n\n    \nxlabel\n \n=\n \nindex\n,\n \nylabel\n \n=\n \nvalue\n)\n\n\np_regular\n \n=\n \nplot\n(\nu\n,\n \n[\n0.0001\n,\n \n0.9999\n],\n \nlegend\n \n=\n \nfalse\n,\n\n    \nylabel\n \n=\n \nvalue\n,\n \nxaxis\n \n=\n \nfalse\n)\n\n\n\nfor\n \ni\n \n=\n \n1\n:\n1000\n\n    \nplot!\n(\np_increasing\n,\n \nresample\n(\nu\n,\n \nTruncateStd\n(\n1\n),\n \nStrictlyIncreasing\n()),\n \nlw\n \n=\n \n0.2\n,\n \n        \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.1\n)\n\n    \nplot!\n(\np_regular\n,\n \nresample\n(\nu\n),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.2\n)\n\n\nend\n \n\n\nplot\n(\np_regular\n,\n \np_increasing\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n),\n \nlink\n \n=\n \n:\nx\n,\n \nsize\n \n=\n \n(\n400\n,\n \n500\n))", 
            "title": "Strictly increasing"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/#compatible_ordering_algorithms", 
            "text": "StrictlyIncreasing ( StartToEnd ())  (the default)", 
            "title": "Compatible ordering algorithms"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/#documentation", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2\n3\n4 resample ( udata :: AbstractUncertainValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }}, \n     sequential_constraint :: StrictlyIncreasing { OrderedSamplingAlgorithm }; \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Draw a sequence of values strictly increasing in magnitude from the dataset, sampling  each of the furnishing distributions once each, after first truncating the supports  of the values in the dataset using the provided  constraint s.  Arguments:   udata : An uncertain dataset.  constraint : Sampling constraint(s) to apply to each of the values in the dataset    before drawing the sequence of increasing values.  sequential_constraint : An instance of a  StrictlyIncreasing  sequential    sampling constraint. For example,  StrictlyIncreasing ( StartToEnd ())  indicates    that a strictly increasing sequence should be created in one go from start to    finish (as opposed to chunking the data set first, then gluing partial sequences    together afterwards).  quantiles : A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    increasing values. This deals with distributions with infinite support.   source  #  UncertainData . Resampling . resample     Method .  1\n2 resample ( udata :: DT ,   sequential_constraint :: StrictlyIncreasing { T }; \n     quantiles   =   [ 0.0001 ,   0.9999 ])   where   { DT   :   AbstractUncertainValueDataset ,   T   :   StartToEnd }    Element-wise resample the uncertain values in the dataset such that each preceding value  is strictly less in magnitude than the next one.   Arguments:   udata : An uncertain dataset.  sequential_constraint : An instance of a  StrictlyIncreasing  sequential    sampling constraint.  ordered_sampling_alg : An instance of a  StartToEnd  ordered    sampling constraint, indicating that the sequence of increasing values should    be created in one go from start to finish (as opposed to chunking the data set    first, then gluing partial sequences together afterwards).  quantiles : A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    increasing values. This deals with distributions with infinite support.   source", 
            "title": "Documentation"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/#example_1_strictly_increasing_sequences", 
            "text": "Let's compare how the realizations look for the situation where no sequential sampling constraint is imposed versus enforcing strictly increasing sequences.  We start by creating some uncertain data with increasing magnitude and zero overlap between  values, so we're guaranteed that a strictly increasing sequence through the dataset exists.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 using   UncertainData ,   Plots   N   =   10  u_timeindices   =   [ UncertainValue ( Normal ,   i ,   rand ( Uniform ( 0.1 ,   2 )))   for   i   =   1 : N ]  u   =   UncertainDataset ( u_timeindices )  p_increasing   =   plot ( u ,   [ 0.0001 ,   0.9999 ],   legend   =   false , \n     xlabel   =   index ,   ylabel   =   value )  p_regular   =   plot ( u ,   [ 0.0001 ,   0.9999 ],   legend   =   false , \n     ylabel   =   value ,   xaxis   =   false )  for   i   =   1 : 1000 \n     plot! ( p_increasing ,   resample ( u ,   StrictlyIncreasing ()),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.1 ) \n     plot! ( p_regular ,   resample ( u ),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.2 )  end   plot ( p_regular ,   p_increasing ,   layout   =   ( 2 ,   1 ),   link   =   : x ,   size   =   ( 400 ,   500 ))     Values of the realizations where strictly increasing sequences are imposed clearly are  limited by the next values in the dataset. For the regular sampling, however, realizations  jump wildly, with both positive and negative first differences.", 
            "title": "Example 1: strictly increasing sequences"
        }, 
        {
            "location": "/resampling/sequential/strictly_increasing/#example_2_regular_constraints_strictly_increasing_sequences", 
            "text": "You may also combine regular sampling constraints with sequential resampling schemes.  Here's one example. We use the same data as in example 1 above, but when drawing increasing  sequences, we only resample from within one standard deviation around the mean.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 p_increasing   =   plot ( u ,   [ 0.0001 ,   0.9999 ],   legend   =   false , \n     xlabel   =   index ,   ylabel   =   value )  p_regular   =   plot ( u ,   [ 0.0001 ,   0.9999 ],   legend   =   false , \n     ylabel   =   value ,   xaxis   =   false )  for   i   =   1 : 1000 \n     plot! ( p_increasing ,   resample ( u ,   TruncateStd ( 1 ),   StrictlyIncreasing ()),   lw   =   0.2 ,  \n         lc   =   : black ,   l\u03b1   =   0.1 ) \n     plot! ( p_regular ,   resample ( u ),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.2 )  end   plot ( p_regular ,   p_increasing ,   layout   =   ( 2 ,   1 ),   link   =   : x ,   size   =   ( 400 ,   500 ))", 
            "title": "Example 2: regular constraints + strictly increasing sequences"
        }, 
        {
            "location": "/resampling/sequential/strictly_decreasing/", 
            "text": "The default constructor for a strictly decreasing sequential sampling constraint is  \nStrictlyDecreasing\n. To specify how the sequence is sampled, provide an  \nOrderedSamplingAlgorithm\n as an argument to the constructor.\n\n\n\n\nDocumentation\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\n4\nresample\n(\nudata\n::\nAbstractUncertainValueDataset\n,\n \n    \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}},\n\n    \nsequential_constraint\n::\nStrictlyDecreasing\n{\nOrderedSamplingAlgorithm\n};\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n\n\n\n\n\n\n\nDraw a sequence of values strictly decreasing in magnitude from the dataset, sampling  each of the furnishing distributions once each, after first truncating the supports  of the values in the dataset using the provided \nconstraint\ns.\n\n\nArguments:\n\n\n\n\nudata\n: An uncertain dataset.\n\n\nconstraint\n: Sampling constraint(s) to apply to each of the values in the dataset\n\n\n\n\nbefore drawing the sequence of decreasing values.\n\n\n\n\nsequential_constraint\n: An instance of a \nStrictlyDecreasing\n sequential\n\n\n\n\nsampling constraint. For example, \nStrictlyDecreasing\n(\nStartToEnd\n())\n indicates  that a strictly decreasing sequence should be created in one go from start to  finish (as opposed to chunking the data set first, then gluing partial sequences  together afterwards).\n\n\n\n\nquantiles\n: A two-element vector representing a quantile range which is used\n\n\n\n\nto truncate the supports values in the dataset before drawing the sequence of  decreasing values. This deals with distributions with infinite support.\n\n\nsource\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\nresample\n(\nudata\n::\nDT\n,\n \nsequential_constraint\n::\nStrictlyDecreasing\n{\nT\n};\n\n    \nquantiles\n \n=\n \n[\n0.0001\n,\n \n0.9999\n])\n \nwhere\n \n{\nDT\n \n:\n \nAbstractUncertainValueDataset\n,\n \nT\n \n:\n \nStartToEnd\n}\n\n\n\n\n\n\n\nElement-wise resample the uncertain values in the dataset such that each preceding value  is strictly larger in magnitude than the next one. \n\n\nArguments:\n\n\n\n\nudata\n: An uncertain dataset.\n\n\nsequential_constraint\n: An instance of a \nStrictlyDecreasing\n sequential    sampling constraint.\n\n\nordered_sampling_alg\n: An instance of a \nStartToEnd\n ordered    sampling constraint, indicating that the sequence of decreasing values should    be created in one go from start to finish (as opposed to chunking the data set    first, then gluing partial sequences together afterwards).\n\n\nquantiles\n: A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    decreasing values. This deals with distributions with infinite support.\n\n\n\n\nsource\n\n\n\n\nCompatible ordering algorithms\n\n\n\n\nStrictlyDecreasing\n(\nStartToEnd\n())\n (the default)\n\n\n\n\n\n\nExamples\n\n\n\n\nExample: Strictly decreasing sequences + regular constraints\n\n\nWe'll start by creating some uncertain data with decreasing magnitude and just minor  overlap between values, so we're reasonably sure we can create strictly decreasing sequences.\n\n\n1\n2\n3\n4\nusing\n \nUncertainData\n,\n \nPlots\n \n\nN\n \n=\n \n20\n\n\nu_timeindices\n \n=\n \n[\nUncertainValue\n(\nNormal\n,\n \ni\n,\n \nrand\n(\nUniform\n(\n0.1\n,\n \n)))\n \nfor\n \ni\n \n=\n \nN\n:-\n1\n:\n1\n]\n\n\nu\n \n=\n \nUncertainDataset\n(\nu_timeindices\n)\n\n\n\n\n\n\n\nNow, we'll create three different plots. In all plots, we plot the 0.00001th to 0.99999\nth\n  (black) and 33\nrd\n to 67\nth\n (red) percentile range error bars. For the first plot, we'll  resample the data without any constraints. For the second plot, we'll resample without  imposing any constraints on the furnishing distirbution, but enforcing strictly decreasing sequences when drawing realizations. For the third plot, we'll first truncate all  furnishing distributions to their 33\nrd\n to 67\nth\n percentile range, then draw realizations  whose consecutively value are strictly decreasing in magnitude.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n# Plot the data with 0.00001th to 0.99999th error bars in both directions\n\n\nqs\n \n=\n \n[\n0.0001\n,\n \n0.9999\n]\n\n\np_noconstraint\n \n=\n \nplot\n(\nu\n,\n \nqs\n,\n \nlegend\n \n=\n \nfalse\n,\n \nxaxis\n \n=\n \nfalse\n,\n\n    \ntitle\n \n=\n \nNoConstraint()\n)\n \n\np_decreasing\n \n=\n \nplot\n(\nu\n,\n \nqs\n,\n \nlegend\n \n=\n \nfalse\n,\n \nxaxis\n \n=\n \nfalse\n,\n \n    \ntitle\n \n=\n \nStrictlyDecreasing()\n)\n\n\np_decreasing_constraint\n \n=\n \nplot\n(\nu\n,\n \nqs\n,\n \nlegend\n \n=\n \nfalse\n,\n \nxaxis\n \n=\n \nfalse\n,\n\n    \ntitle\n \n=\n \nTruncateQuantiles(0.33, 0.67) + StriclyDecreasing()\n)\n\n\n\n# Add 33rd to 67th percentile range error bars to all plots. \n\n\nplot!\n(\np_noconstraint\n,\n \nu\n,\n \n[\n0.33\n,\n \n0.67\n],\n \nmsc\n \n=\n \n:\nred\n)\n\n\nplot!\n(\np_decreasing\n,\n \nu\n,\n \n[\n0.33\n,\n \n0.67\n],\n \nmsc\n \n=\n \n:\nred\n)\n\n\nplot!\n(\np_decreasing_constraint\n,\n \nu\n,\n \n[\n0.33\n,\n \n0.67\n],\n \nmsc\n \n=\n \n:\nred\n)\n\n\n\nfor\n \ni\n \n=\n \n1\n:\n300\n\n    \nplot!\n(\np_noconstraint\n,\n \nresample\n(\nu\n,\n \nNoConstraint\n()),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.2\n)\n\n    \nplot!\n(\np_decreasing\n,\n \nresample\n(\nu\n,\n \nStrictlyDecreasing\n()),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.1\n)\n\n    \nplot!\n(\np_decreasing_constraint\n,\n \nresample\n(\nu\n,\n \nTruncateQuantiles\n(\n0.33\n,\n \n0.67\n),\n \nStrictlyDecreasing\n()),\n \nlw\n \n=\n \n0.2\n,\n \nlc\n \n=\n \n:\nblack\n,\n \nl\u03b1\n \n=\n \n0.1\n)\n\n\nend\n \n\n\nplot\n(\np_noconstraint\n,\n \np_decreasing\n,\n \np_decreasing_constraint\n,\n \nlink\n \n=\n \n:\nx\n,\n\n    \nlayout\n \n=\n \n(\n3\n,\n \n1\n),\n \nsize\n \n=\n \n(\n300\n,\n \n600\n),\n \ntitlefont\n \n=\n \nfont\n(\n8\n))", 
            "title": "Strictly decreasing"
        }, 
        {
            "location": "/resampling/sequential/strictly_decreasing/#documentation", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2\n3\n4 resample ( udata :: AbstractUncertainValueDataset ,  \n     constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }}, \n     sequential_constraint :: StrictlyDecreasing { OrderedSamplingAlgorithm }; \n     quantiles   =   [ 0.0001 ,   0.9999 ])    Draw a sequence of values strictly decreasing in magnitude from the dataset, sampling  each of the furnishing distributions once each, after first truncating the supports  of the values in the dataset using the provided  constraint s.  Arguments:   udata : An uncertain dataset.  constraint : Sampling constraint(s) to apply to each of the values in the dataset   before drawing the sequence of decreasing values.   sequential_constraint : An instance of a  StrictlyDecreasing  sequential   sampling constraint. For example,  StrictlyDecreasing ( StartToEnd ())  indicates  that a strictly decreasing sequence should be created in one go from start to  finish (as opposed to chunking the data set first, then gluing partial sequences  together afterwards).   quantiles : A two-element vector representing a quantile range which is used   to truncate the supports values in the dataset before drawing the sequence of  decreasing values. This deals with distributions with infinite support.  source  #  UncertainData . Resampling . resample     Method .  1\n2 resample ( udata :: DT ,   sequential_constraint :: StrictlyDecreasing { T }; \n     quantiles   =   [ 0.0001 ,   0.9999 ])   where   { DT   :   AbstractUncertainValueDataset ,   T   :   StartToEnd }    Element-wise resample the uncertain values in the dataset such that each preceding value  is strictly larger in magnitude than the next one.   Arguments:   udata : An uncertain dataset.  sequential_constraint : An instance of a  StrictlyDecreasing  sequential    sampling constraint.  ordered_sampling_alg : An instance of a  StartToEnd  ordered    sampling constraint, indicating that the sequence of decreasing values should    be created in one go from start to finish (as opposed to chunking the data set    first, then gluing partial sequences together afterwards).  quantiles : A two-element vector representing a quantile range which is used    to truncate the supports values in the dataset before drawing the sequence of    decreasing values. This deals with distributions with infinite support.   source", 
            "title": "Documentation"
        }, 
        {
            "location": "/resampling/sequential/strictly_decreasing/#compatible_ordering_algorithms", 
            "text": "StrictlyDecreasing ( StartToEnd ())  (the default)", 
            "title": "Compatible ordering algorithms"
        }, 
        {
            "location": "/resampling/sequential/strictly_decreasing/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/resampling/sequential/strictly_decreasing/#example_strictly_decreasing_sequences_regular_constraints", 
            "text": "We'll start by creating some uncertain data with decreasing magnitude and just minor  overlap between values, so we're reasonably sure we can create strictly decreasing sequences.  1\n2\n3\n4 using   UncertainData ,   Plots   N   =   20  u_timeindices   =   [ UncertainValue ( Normal ,   i ,   rand ( Uniform ( 0.1 ,   )))   for   i   =   N :- 1 : 1 ]  u   =   UncertainDataset ( u_timeindices )    Now, we'll create three different plots. In all plots, we plot the 0.00001th to 0.99999 th   (black) and 33 rd  to 67 th  (red) percentile range error bars. For the first plot, we'll  resample the data without any constraints. For the second plot, we'll resample without  imposing any constraints on the furnishing distirbution, but enforcing strictly decreasing sequences when drawing realizations. For the third plot, we'll first truncate all  furnishing distributions to their 33 rd  to 67 th  percentile range, then draw realizations  whose consecutively value are strictly decreasing in magnitude.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 # Plot the data with 0.00001th to 0.99999th error bars in both directions  qs   =   [ 0.0001 ,   0.9999 ]  p_noconstraint   =   plot ( u ,   qs ,   legend   =   false ,   xaxis   =   false , \n     title   =   NoConstraint() )   p_decreasing   =   plot ( u ,   qs ,   legend   =   false ,   xaxis   =   false ,  \n     title   =   StrictlyDecreasing() )  p_decreasing_constraint   =   plot ( u ,   qs ,   legend   =   false ,   xaxis   =   false , \n     title   =   TruncateQuantiles(0.33, 0.67) + StriclyDecreasing() )  # Add 33rd to 67th percentile range error bars to all plots.   plot! ( p_noconstraint ,   u ,   [ 0.33 ,   0.67 ],   msc   =   : red )  plot! ( p_decreasing ,   u ,   [ 0.33 ,   0.67 ],   msc   =   : red )  plot! ( p_decreasing_constraint ,   u ,   [ 0.33 ,   0.67 ],   msc   =   : red )  for   i   =   1 : 300 \n     plot! ( p_noconstraint ,   resample ( u ,   NoConstraint ()),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.2 ) \n     plot! ( p_decreasing ,   resample ( u ,   StrictlyDecreasing ()),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.1 ) \n     plot! ( p_decreasing_constraint ,   resample ( u ,   TruncateQuantiles ( 0.33 ,   0.67 ),   StrictlyDecreasing ()),   lw   =   0.2 ,   lc   =   : black ,   l\u03b1   =   0.1 )  end   plot ( p_noconstraint ,   p_decreasing ,   p_decreasing_constraint ,   link   =   : x , \n     layout   =   ( 3 ,   1 ),   size   =   ( 300 ,   600 ),   titlefont   =   font ( 8 ))", 
            "title": "Example: Strictly decreasing sequences + regular constraints"
        }, 
        {
            "location": "/resampling/sequential/resampling_indexvalue_sequential/", 
            "text": "Resampling syntax\n\n\n\n\nManually resampling\n\n\nBecause both the indices and the values of \nUncertainIndexValueDataset\ns are  datasets themselves, you could manually resample them by accessing the \nindices\n and  \nvalues\n fields. This gives you full control of the resampling. \n\n\nThere are some built-in sampling routines you could use instead if you use cases are simple.\n\n\n\n\nBuilt-in resampling methods\n\n\nSequential constraints are always interpreted as belonging to the indices of an  \nuncertain index-value dataset\n.  Therefore, when using the built-in function to resample an index-value dataset, you can use  the same syntax as for any other  \nuncertain value dataset\n, but provide an additional sequential constraint after the regular constraints. The  order of arguments is always 1) regular constraints, then 2) the sequential constraint.\n\n\nThe following examples illustrates the syntax. Assume \nudata\n is an  \nUncertainIndexValueDataset\n instance. Then\n\n\n\n\nresample\n(\nudata\n,\n \nStrictlyIncreasing\n())\n enforces the sequential constraint only to the    indices, applying no constraint(s) on the furnishing distributions of either the    indices nor the values of the dataset.\n\n\nresample\n(\nudata\n,\n \nTruncateQuantile\n(\n0\n.\n1\n,\n \n0\n.\n9\n),\n \nStrictlyIncreasing\n())\n applies the truncating    constraint both the indices and the values, then enforces the sequential constraint    on the indices.\n\n\nresample\n(\nudata\n,\n \nTruncateStd\n(\n2\n),\n \nTruncateQuantile\n(\n0\n.\n1\n,\n \n0\n.\n9\n),\n \nStrictlyIncreasing\n())\n    applies separate truncating constraints to the indices and to the values, then    enforces the sequential constraint on the indices.\n\n\nresample\n(\nudata\n,\n \nNoConstraint\n(),\n \nTruncateQuantile\n(\n0\n.\n1\n,\n \n0\n.\n9\n),\n \nStrictlyIncreasing\n())\n does    the same as above, but \nNoConstraint\n()\n indicates that no constraints are applied to    the indices prior to drawing the sequential realization of the indices.\n\n\n\n\nOf course, like for uncertain value datasets, you can also apply individual constraints to  each index and each value in the dataset, by providing a vector of constraints instead  of a single constraint.\n\n\nCurrently implemented sequential constraints: \n\n\n\n\nStrictlyIncreasing\n\n\nStrictlyDecreasing", 
            "title": "Index-value datasets"
        }, 
        {
            "location": "/resampling/sequential/resampling_indexvalue_sequential/#resampling_syntax", 
            "text": "", 
            "title": "Resampling syntax"
        }, 
        {
            "location": "/resampling/sequential/resampling_indexvalue_sequential/#manually_resampling", 
            "text": "Because both the indices and the values of  UncertainIndexValueDataset s are  datasets themselves, you could manually resample them by accessing the  indices  and   values  fields. This gives you full control of the resampling.   There are some built-in sampling routines you could use instead if you use cases are simple.", 
            "title": "Manually resampling"
        }, 
        {
            "location": "/resampling/sequential/resampling_indexvalue_sequential/#built-in_resampling_methods", 
            "text": "Sequential constraints are always interpreted as belonging to the indices of an   uncertain index-value dataset .  Therefore, when using the built-in function to resample an index-value dataset, you can use  the same syntax as for any other   uncertain value dataset , but provide an additional sequential constraint after the regular constraints. The  order of arguments is always 1) regular constraints, then 2) the sequential constraint.  The following examples illustrates the syntax. Assume  udata  is an   UncertainIndexValueDataset  instance. Then   resample ( udata ,   StrictlyIncreasing ())  enforces the sequential constraint only to the    indices, applying no constraint(s) on the furnishing distributions of either the    indices nor the values of the dataset.  resample ( udata ,   TruncateQuantile ( 0 . 1 ,   0 . 9 ),   StrictlyIncreasing ())  applies the truncating    constraint both the indices and the values, then enforces the sequential constraint    on the indices.  resample ( udata ,   TruncateStd ( 2 ),   TruncateQuantile ( 0 . 1 ,   0 . 9 ),   StrictlyIncreasing ())     applies separate truncating constraints to the indices and to the values, then    enforces the sequential constraint on the indices.  resample ( udata ,   NoConstraint (),   TruncateQuantile ( 0 . 1 ,   0 . 9 ),   StrictlyIncreasing ())  does    the same as above, but  NoConstraint ()  indicates that no constraints are applied to    the indices prior to drawing the sequential realization of the indices.   Of course, like for uncertain value datasets, you can also apply individual constraints to  each index and each value in the dataset, by providing a vector of constraints instead  of a single constraint.  Currently implemented sequential constraints:    StrictlyIncreasing  StrictlyDecreasing", 
            "title": "Built-in resampling methods"
        }, 
        {
            "location": "/resampling/interpolation/interpolation/", 
            "text": "Interpolations.jl\n is used for basic  interpolation. It supports many different types of interpolation when data are evenly  spaced, and gridded interpolation for unevenly spaced data. \n\n\n\n\nSupported interpolations\n\n\nFor now, \nUncertainData\n implements linear interpolation for uncertain  dataset realizations. \n\n\n\n\nUncertain index-value datasets\n\n\nDatasets with uncertain indices (hence, the indices are almost always unevenly spaced), can only be interpolated using \nlinear interpolation\n.", 
            "title": "Overview"
        }, 
        {
            "location": "/resampling/interpolation/interpolation/#supported_interpolations", 
            "text": "For now,  UncertainData  implements linear interpolation for uncertain  dataset realizations.", 
            "title": "Supported interpolations"
        }, 
        {
            "location": "/resampling/interpolation/interpolation/#uncertain_index-value_datasets", 
            "text": "Datasets with uncertain indices (hence, the indices are almost always unevenly spaced), can only be interpolated using  linear interpolation .", 
            "title": "Uncertain index-value datasets"
        }, 
        {
            "location": "/resampling/interpolation/gridded/", 
            "text": "Grids\n\n\n#\n\n\nUncertainData\n.\nInterpolationAndGrids\n.\nRegularGrid\n \n \nType\n.\n\n\n1\nRegularGrid\n\n\n\n\n\n\n\nFields\n\n\n\n\nmin\n: The minimum value of the grid.\n\n\nmax\n: The maximum value of the grid.\n\n\nstep\n: The interval size.\n\n\nextrapolation_bc\n: The extrapolation condition. Can also be NaN.\n\n\n\n\nsource\n\n\n\n\nSyntax\n\n\n\n\nUncertain index-value datasets\n\n\nThe following methods are available for the interpolating of a realization of an uncertain index-value dataset: \n\n\n\n\nNo constraints\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \ngrid\n::\nInterpolationGrid\n;\n\n    \ntrunc\n::\nTruncateQuantiles\n \n=\n \nTruncateQuantiles\n(\n0.001\n,\n \n0.999\n))\n\n\n\n\n\n\n\nDraw a realization of \nudata\n, then interpolate the data values to \ngrid\n. \n\n\nTo avoid very large spans of interpolation, the uncertain indices are truncated to some  large quantile range. Values are not truncated. \n\n\nsource\n\n\n\n\nSequential constraints\n\n\n#\n\n\nUncertainData\n.\nResampling\n.\nresample\n \n \nMethod\n.\n\n\n1\n2\n3\n4\nresample\n(\nudata\n::\nUncertainIndexValueDataset\n,\n \n    \nsequential_constraint\n::\nSequentialSamplingConstraint\n,\n\n    \ngrid\n::\nInterpolationGrid\n;\n\n    \ntrunc\n::\nTruncateQuantiles\n \n=\n \nTruncateQuantiles\n(\n0.001\n,\n \n0.999\n))\n\n\n\n\n\n\n\nDraw a realization of \nudata\n, enforcing a \nsequential_constraint\n on the indices. Then, interpolate the values of the realization to the provided grid of indices (\ngrid\n). \n\n\nTo avoid very large spans of interpolation, the uncertain indices are truncated to some  large quantile range. Values are not truncated.  \n\n\nsource", 
            "title": "Gridded interpolation"
        }, 
        {
            "location": "/resampling/interpolation/gridded/#grids", 
            "text": "#  UncertainData . InterpolationAndGrids . RegularGrid     Type .  1 RegularGrid    Fields   min : The minimum value of the grid.  max : The maximum value of the grid.  step : The interval size.  extrapolation_bc : The extrapolation condition. Can also be NaN.   source", 
            "title": "Grids"
        }, 
        {
            "location": "/resampling/interpolation/gridded/#syntax", 
            "text": "", 
            "title": "Syntax"
        }, 
        {
            "location": "/resampling/interpolation/gridded/#uncertain_index-value_datasets", 
            "text": "The following methods are available for the interpolating of a realization of an uncertain index-value dataset:", 
            "title": "Uncertain index-value datasets"
        }, 
        {
            "location": "/resampling/interpolation/gridded/#no_constraints", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2\n3 resample ( udata :: UncertainIndexValueDataset ,  \n     grid :: InterpolationGrid ; \n     trunc :: TruncateQuantiles   =   TruncateQuantiles ( 0.001 ,   0.999 ))    Draw a realization of  udata , then interpolate the data values to  grid .   To avoid very large spans of interpolation, the uncertain indices are truncated to some  large quantile range. Values are not truncated.   source", 
            "title": "No constraints"
        }, 
        {
            "location": "/resampling/interpolation/gridded/#sequential_constraints", 
            "text": "#  UncertainData . Resampling . resample     Method .  1\n2\n3\n4 resample ( udata :: UncertainIndexValueDataset ,  \n     sequential_constraint :: SequentialSamplingConstraint , \n     grid :: InterpolationGrid ; \n     trunc :: TruncateQuantiles   =   TruncateQuantiles ( 0.001 ,   0.999 ))    Draw a realization of  udata , enforcing a  sequential_constraint  on the indices. Then, interpolate the values of the realization to the provided grid of indices ( grid ).   To avoid very large spans of interpolation, the uncertain indices are truncated to some  large quantile range. Values are not truncated.    source", 
            "title": "Sequential constraints"
        }, 
        {
            "location": "/mathematics/elementary_operations/", 
            "text": "Elementary mathematical operations\n\n\nElementary mathematical operations (\n+\n, \n-\n, \n*\n, and \n/\n) between arbitrary  uncertain values of different types and scalars are supported. \n\n\n\n\nSyntax\n\n\nBecause elementary operations should work on arbitrary uncertain values, a resampling  approach is used to perform the mathematical operations. All mathematical  operations thus return a vector containing the results of repeated element-wise operations  (where each element is a resampled draw from the furnishing distribution(s) of the  uncertain value(s)). \n\n\nThe default number of realizations is set to \n10000\n. This allows calling \nuval1\n \n+\n \nuval2\n  for two uncertain values \nuval1\n and \nuval2\n. If you need to tune the number of resample  draws to \nn\n, use the \n+\n(\nuval1\n,\n \nuval2\n,\n \nn\n)\n syntax. \n\n\n\n\nFuture improvements\n\n\nIn the future, elementary operations might be improved for certain combinations of uncertain  values where exact expressions for error propagation are now, for example using the  machinery in \nMeasurements\n.\njl\n for normally distributed values.\n\n\n\n\nSupported operations\n\n\n\n\nAddition\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for pairs of uncertain values. \n\n\nComputes the element-wise sum between for a default of \nn\n \n=\n \n10000\n realizations of \na\n and  \nb\n, then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise sums.\n\n\nUse the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for pairs of uncertain values. \n\n\nComputes the element-wise sum between \na\n and \nb\n for \nn\n realizations of \na\n and \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.\n\n\nCall this function using the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for between scalars and uncertain values. \n\n\nComputes the element-wise sum between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.\n\n\nUse the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise sum between \na\n and \nb\n for \nn\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.\n\n\nCall this function using the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for between uncertain values and scalars. \n\n\nComputes the element-wise sum between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.\n\n\nUse the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n+\n \n \nMethod\n.\n\n\n1\nBase\n.:+\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nAddition operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise sum between \na\n and \nb\n for \nn\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.\n\n\nCall this function using the \n+\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n\n\nSubtraction\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for pairs of uncertain values. \n\n\nComputes the element-wise differences between for a default of \nn\n \n=\n \n30000\n realizations of \na\n and  \nb\n, then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise differences.\n\n\nUse the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for pairs of uncertain values. \n\n\nComputes the element-wise differences between \na\n and \nb\n for \nn\n realizations of \na\n and \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.\n\n\nCall this function using the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for between scalars and uncertain values. \n\n\nComputes the element-wise differences between \na\n and \nb\n for a default of \nn\n \n=\n \n30000\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.\n\n\nUse the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise differences between \na\n and \nb\n for \nn\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.\n\n\nCall this function using the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for between uncertain values and scalars. \n\n\nComputes the element-wise differences between \na\n and \nb\n for a default of \nn\n \n=\n \n30000\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.\n\n\nUse the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:-\n \n \nMethod\n.\n\n\n1\nBase\n.:-\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nSubtraction operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise differences between \na\n and \nb\n for \nn\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.\n\n\nCall this function using the \n-\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n\n\nMultiplication\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for pairs of uncertain values. \n\n\nComputes the element-wise products between for a default of \nn\n \n=\n \n10000\n realizations of \na\n and  \nb\n, then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise products.\n\n\nUse the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for pairs of uncertain values. \n\n\nComputes the element-wise products between \na\n and \nb\n for \nn\n realizations of \na\n and \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.\n\n\nCall this function using the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for between scalars and uncertain values. \n\n\nComputes the element-wise products between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.\n\n\nUse the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise products between \na\n and \nb\n for \nn\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.\n\n\nCall this function using the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for between uncertain values and scalars. \n\n\nComputes the element-wise products between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.\n\n\nUse the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n*\n \n \nMethod\n.\n\n\n1\nBase\n.:*\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nMultiplication operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise products between \na\n and \nb\n for \nn\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.\n\n\nCall this function using the \n*\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n\n\nDivision\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for pairs of uncertain values. \n\n\nComputes the element-wise quotients between for a default of \nn\n \n=\n \n10000\n realizations of \na\n and  \nb\n, then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise quotients.\n\n\nUse the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for pairs of uncertain values. \n\n\nComputes the element-wise quotients between \na\n and \nb\n for \nn\n realizations of \na\n and \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.\n\n\nCall this function using the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for between scalars and uncertain values. \n\n\nComputes the element-wise quotients between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.\n\n\nUse the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nReal\n,\n \nb\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise quotients between \na\n and \nb\n for \nn\n realizations of \nb\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.\n\n\nCall this function using the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for between uncertain values and scalars. \n\n\nComputes the element-wise quotients between \na\n and \nb\n for a default of \nn\n \n=\n \n10000\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.\n\n\nUse the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax to tune the number (\nn\n) of draws.\n\n\nsource\n\n\n#\n\n\nBase\n.:\n/\n \n \nMethod\n.\n\n\n1\nBase\n.:/\n(\na\n::\nAbstractUncertainValue\n,\n \nb\n::\nReal\n,\n \nn\n::\nInt\n)\n \n-\n \nUncertainValue\n\n\n\n\n\n\n\nDivision operator for scalar-uncertain value pairs. \n\n\nComputes the element-wise quotients between \na\n and \nb\n for \nn\n realizations of \na\n, then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.\n\n\nCall this function using the \n/\n(\na\n,\n \nb\n,\n \nn\n)\n syntax.\n\n\nsource\n\n\n\n\nSpecial cases\n\n\n\n\nCertainValue\ns\n\n\nPerforming elementary operations with \nCertainValue\ns behaves as for scalars.", 
            "title": "Elementary operations"
        }, 
        {
            "location": "/mathematics/elementary_operations/#elementary_mathematical_operations", 
            "text": "Elementary mathematical operations ( + ,  - ,  * , and  / ) between arbitrary  uncertain values of different types and scalars are supported.", 
            "title": "Elementary mathematical operations"
        }, 
        {
            "location": "/mathematics/elementary_operations/#syntax", 
            "text": "Because elementary operations should work on arbitrary uncertain values, a resampling  approach is used to perform the mathematical operations. All mathematical  operations thus return a vector containing the results of repeated element-wise operations  (where each element is a resampled draw from the furnishing distribution(s) of the  uncertain value(s)).   The default number of realizations is set to  10000 . This allows calling  uval1   +   uval2   for two uncertain values  uval1  and  uval2 . If you need to tune the number of resample  draws to  n , use the  + ( uval1 ,   uval2 ,   n )  syntax.", 
            "title": "Syntax"
        }, 
        {
            "location": "/mathematics/elementary_operations/#future_improvements", 
            "text": "In the future, elementary operations might be improved for certain combinations of uncertain  values where exact expressions for error propagation are now, for example using the  machinery in  Measurements . jl  for normally distributed values.", 
            "title": "Future improvements"
        }, 
        {
            "location": "/mathematics/elementary_operations/#supported_operations", 
            "text": "", 
            "title": "Supported operations"
        }, 
        {
            "location": "/mathematics/elementary_operations/#addition", 
            "text": "#  Base .: +     Method .  1 Base .:+ ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue )   -   UncertainValue    Addition operator for pairs of uncertain values.   Computes the element-wise sum between for a default of  n   =   10000  realizations of  a  and   b , then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise sums.  Use the  + ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: +     Method .  1 Base .:+ ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Addition operator for pairs of uncertain values.   Computes the element-wise sum between  a  and  b  for  n  realizations of  a  and  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.  Call this function using the  + ( a ,   b ,   n )  syntax.  source  #  Base .: +     Method .  1 Base .:+ ( a :: Real ,   b :: AbstractUncertainValue )   -   UncertainValue    Addition operator for between scalars and uncertain values.   Computes the element-wise sum between  a  and  b  for a default of  n   =   10000  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.  Use the  + ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: +     Method .  1 Base .:+ ( a :: Real ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Addition operator for scalar-uncertain value pairs.   Computes the element-wise sum between  a  and  b  for  n  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.  Call this function using the  + ( a ,   b ,   n )  syntax.  source  #  Base .: +     Method .  1 Base .:+ ( a :: AbstractUncertainValue ,   b :: Real )   -   UncertainValue    Addition operator for between uncertain values and scalars.   Computes the element-wise sum between  a  and  b  for a default of  n   =   10000  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.  Use the  + ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: +     Method .  1 Base .:+ ( a :: AbstractUncertainValue ,   b :: Real ,   n :: Int )   -   UncertainValue    Addition operator for scalar-uncertain value pairs.   Computes the element-wise sum between  a  and  b  for  n  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise sums.  Call this function using the  + ( a ,   b ,   n )  syntax.  source", 
            "title": "Addition"
        }, 
        {
            "location": "/mathematics/elementary_operations/#subtraction", 
            "text": "#  Base .:-     Method .  1 Base .:- ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue )   -   UncertainValue    Subtraction operator for pairs of uncertain values.   Computes the element-wise differences between for a default of  n   =   30000  realizations of  a  and   b , then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise differences.  Use the  - ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .:-     Method .  1 Base .:- ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Subtraction operator for pairs of uncertain values.   Computes the element-wise differences between  a  and  b  for  n  realizations of  a  and  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.  Call this function using the  - ( a ,   b ,   n )  syntax.  source  #  Base .:-     Method .  1 Base .:- ( a :: Real ,   b :: AbstractUncertainValue )   -   UncertainValue    Subtraction operator for between scalars and uncertain values.   Computes the element-wise differences between  a  and  b  for a default of  n   =   30000  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.  Use the  - ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .:-     Method .  1 Base .:- ( a :: Real ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Subtraction operator for scalar-uncertain value pairs.   Computes the element-wise differences between  a  and  b  for  n  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.  Call this function using the  - ( a ,   b ,   n )  syntax.  source  #  Base .:-     Method .  1 Base .:- ( a :: AbstractUncertainValue ,   b :: Real )   -   UncertainValue    Subtraction operator for between uncertain values and scalars.   Computes the element-wise differences between  a  and  b  for a default of  n   =   30000  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.  Use the  - ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .:-     Method .  1 Base .:- ( a :: AbstractUncertainValue ,   b :: Real ,   n :: Int )   -   UncertainValue    Subtraction operator for scalar-uncertain value pairs.   Computes the element-wise differences between  a  and  b  for  n  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise differences.  Call this function using the  - ( a ,   b ,   n )  syntax.  source", 
            "title": "Subtraction"
        }, 
        {
            "location": "/mathematics/elementary_operations/#multiplication", 
            "text": "#  Base .: *     Method .  1 Base .:* ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue )   -   UncertainValue    Multiplication operator for pairs of uncertain values.   Computes the element-wise products between for a default of  n   =   10000  realizations of  a  and   b , then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise products.  Use the  * ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: *     Method .  1 Base .:* ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Multiplication operator for pairs of uncertain values.   Computes the element-wise products between  a  and  b  for  n  realizations of  a  and  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.  Call this function using the  * ( a ,   b ,   n )  syntax.  source  #  Base .: *     Method .  1 Base .:* ( a :: Real ,   b :: AbstractUncertainValue )   -   UncertainValue    Multiplication operator for between scalars and uncertain values.   Computes the element-wise products between  a  and  b  for a default of  n   =   10000  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.  Use the  * ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: *     Method .  1 Base .:* ( a :: Real ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Multiplication operator for scalar-uncertain value pairs.   Computes the element-wise products between  a  and  b  for  n  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.  Call this function using the  * ( a ,   b ,   n )  syntax.  source  #  Base .: *     Method .  1 Base .:* ( a :: AbstractUncertainValue ,   b :: Real )   -   UncertainValue    Multiplication operator for between uncertain values and scalars.   Computes the element-wise products between  a  and  b  for a default of  n   =   10000  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.  Use the  * ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: *     Method .  1 Base .:* ( a :: AbstractUncertainValue ,   b :: Real ,   n :: Int )   -   UncertainValue    Multiplication operator for scalar-uncertain value pairs.   Computes the element-wise products between  a  and  b  for  n  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise products.  Call this function using the  * ( a ,   b ,   n )  syntax.  source", 
            "title": "Multiplication"
        }, 
        {
            "location": "/mathematics/elementary_operations/#division", 
            "text": "#  Base .: /     Method .  1 Base .:/ ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue )   -   UncertainValue    Division operator for pairs of uncertain values.   Computes the element-wise quotients between for a default of  n   =   10000  realizations of  a  and   b , then returns an uncertain value based on a kernel density estimate to the distribution  of the element-wise quotients.  Use the  / ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: /     Method .  1 Base .:/ ( a :: AbstractUncertainValue ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Division operator for pairs of uncertain values.   Computes the element-wise quotients between  a  and  b  for  n  realizations of  a  and  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.  Call this function using the  / ( a ,   b ,   n )  syntax.  source  #  Base .: /     Method .  1 Base .:/ ( a :: Real ,   b :: AbstractUncertainValue )   -   UncertainValue    Division operator for between scalars and uncertain values.   Computes the element-wise quotients between  a  and  b  for a default of  n   =   10000  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.  Use the  / ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: /     Method .  1 Base .:/ ( a :: Real ,   b :: AbstractUncertainValue ,   n :: Int )   -   UncertainValue    Division operator for scalar-uncertain value pairs.   Computes the element-wise quotients between  a  and  b  for  n  realizations of  b , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.  Call this function using the  / ( a ,   b ,   n )  syntax.  source  #  Base .: /     Method .  1 Base .:/ ( a :: AbstractUncertainValue ,   b :: Real )   -   UncertainValue    Division operator for between uncertain values and scalars.   Computes the element-wise quotients between  a  and  b  for a default of  n   =   10000  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.  Use the  / ( a ,   b ,   n )  syntax to tune the number ( n ) of draws.  source  #  Base .: /     Method .  1 Base .:/ ( a :: AbstractUncertainValue ,   b :: Real ,   n :: Int )   -   UncertainValue    Division operator for scalar-uncertain value pairs.   Computes the element-wise quotients between  a  and  b  for  n  realizations of  a , then returns an uncertain value based on a kernel density estimate to the  distribution of the element-wise quotients.  Call this function using the  / ( a ,   b ,   n )  syntax.  source", 
            "title": "Division"
        }, 
        {
            "location": "/mathematics/elementary_operations/#special_cases", 
            "text": "", 
            "title": "Special cases"
        }, 
        {
            "location": "/mathematics/elementary_operations/#certainvalues", 
            "text": "Performing elementary operations with  CertainValue s behaves as for scalars.", 
            "title": "CertainValues"
        }, 
        {
            "location": "/mathematics/trig_functions/", 
            "text": "Trigonometric functions\n\n\nTrigonometric functions are supported for arbitrary uncertain values of different types. Like for \nelementary operations\n, a resampling approach is  used for the computations.\n\n\n\n\nSyntax\n\n\nBecause elementary operations should work on arbitrary uncertain values, a resampling  approach is used to perform the mathematical operations. All mathematical  operations thus return a vector containing the results of repeated element-wise operations  (where each element is a resampled draw from the furnishing distribution(s) of the  uncertain value(s)). \n\n\nEach trigonometric function comes in two versions. \n\n\n\n\nThe first syntax allows skipping providing the number of draws, which is set to 10000 by default    (e.g. \ncos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n.\n\n\nUsing the second syntax, you have to explicitly provide the number of draws (e.g. \ncos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n).\n\n\n\n\n\n\nPossible errors\n\n\nBeware: if the support of the funishing distribution for an uncertain value lies partly  outside the domain of the function, you risk encountering errors.\n\n\n\n\nSupported trigonometric functions\n\n\n\n\nSine\n\n\n#\n\n\nBase\n.\nsin\n \n \nMethod\n.\n\n\n1\nBase\n.\nsin\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the sine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nsin\n \n \nMethod\n.\n\n\n1\nBase\n.\nsin\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the sine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsind\n \n \nMethod\n.\n\n\n1\nBase\n.\nsind\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the sine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsind\n \n \nMethod\n.\n\n\n1\nBase\n.\nsind\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the sine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise sine for \nn\n realizations.\n\n\nsource\n\n\n1\n2\nBase\n.\nsinh\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n)\n\n\nBase\n.\nsinh\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\n\n\nCosine\n\n\n#\n\n\nBase\n.\ncos\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ncos\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncosd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncosd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ncosh\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cosine of the uncertain value \nx\n. Computes the element-wise hyperbolic cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ncosh\n \n \nMethod\n.\n\n\n1\nBase\n.\ncos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cosine of the uncertain value \nx\n. Computes the element-wise hyperbolic cosine for \nn\n realizations.\n\n\nsource\n\n\n\n\nTangent\n\n\n#\n\n\nBase\n.\natan\n \n \nMethod\n.\n\n\n1\nBase\n.\natan\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse tangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\natan\n \n \nMethod\n.\n\n\n1\nBase\n.\natan\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse tangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\natand\n \n \nMethod\n.\n\n\n1\nBase\n.\natand\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse tangent of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\natand\n \n \nMethod\n.\n\n\n1\nBase\n.\natand\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse tangent of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\natanh\n \n \nMethod\n.\n\n\n1\nBase\n.\natanh\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hypoerbolic tangent of the uncertain value \nx\n. Computes the element-wise inverse hypoerbolic tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\natanh\n \n \nMethod\n.\n\n\n1\nBase\n.\natanh\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hypoerbolic tangent of the uncertain value \nx\n. Computes the element-wise inverse hypoerbolic tangent for \nn\n realizations.\n\n\nsource\n\n\n\n\nReciprocal trig functions\n\n\n\n\nCosecant\n\n\n#\n\n\nBase\n.\nMath\n.\ncsc\n \n \nMethod\n.\n\n\n1\nBase\n.\ncsc\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosecant of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncsc\n \n \nMethod\n.\n\n\n1\nBase\n.\ncsc\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosecant of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncscd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncscd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosecant of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncscd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncscd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cosecant of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncsch\n \n \nMethod\n.\n\n\n1\nBase\n.\ncscd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cosecant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise hyperbolic cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncsch\n \n \nMethod\n.\n\n\n1\nBase\n.\ncscd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cosecant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise hyperbolic cosecant for \nn\n realizations.\n\n\nsource\n\n\n\n\nSecant\n\n\n#\n\n\nBase\n.\nMath\n.\nsec\n \n \nMethod\n.\n\n\n1\nBase\n.\nsec\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the secant of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsec\n \n \nMethod\n.\n\n\n1\nBase\n.\nsec\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the secant of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsecd\n \n \nMethod\n.\n\n\n1\nBase\n.\nsecd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the secant of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsecd\n \n \nMethod\n.\n\n\n1\nBase\n.\nsecd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the secant of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsech\n \n \nMethod\n.\n\n\n1\nBase\n.\nsech\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic secant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsech\n \n \nMethod\n.\n\n\n1\nBase\n.\nsech\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic secant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n\n\nCotangent\n\n\n#\n\n\nBase\n.\nMath\n.\ncot\n \n \nMethod\n.\n\n\n1\nBase\n.\ncot\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cotangent of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cotangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncot\n \n \nMethod\n.\n\n\n1\nBase\n.\ncot\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cotangent of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise cotangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncotd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncotd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cotangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cotangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncotd\n \n \nMethod\n.\n\n\n1\nBase\n.\ncotd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the cotangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise cotangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncoth\n \n \nMethod\n.\n\n\n1\nBase\n.\ncoth\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cotangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise hyperbolic cotangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncoth\n \n \nMethod\n.\n\n\n1\nBase\n.\ncoth\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic cotangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise hyperbolic cotangent for \nn\n realizations.\n\n\nsource\n\n\n\n\nInverse trig functions\n\n\n\n\nSine\n\n\n#\n\n\nBase\n.\nasin\n \n \nMethod\n.\n\n\n1\nBase\n.\nasin\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse sine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise inverse sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nasin\n \n \nMethod\n.\n\n\n1\nBase\n.\nasin\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse sine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise inverse sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasind\n \n \nMethod\n.\n\n\n1\nBase\n.\nasind\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse sine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise inverse sine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasind\n \n \nMethod\n.\n\n\n1\nBase\n.\nasind\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse sine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise inverse sine for \nn\n realizations.\n\n\nsource\n\n\n1\n2\nBase\n.\nasinh\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n)\n\n\nBase\n.\nasinh\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\n\n\nCosine\n\n\n#\n\n\nBase\n.\nacos\n \n \nMethod\n.\n\n\n1\nBase\n.\nacos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise inverse cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nacos\n \n \nMethod\n.\n\n\n1\nBase\n.\nacos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosine of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise inverse cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacosd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacosd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise inverse cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacosd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacosd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosine of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise inverse cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nacosh\n \n \nMethod\n.\n\n\n1\nBase\n.\nacosh\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cosine of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic cosine for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nacosh\n \n \nMethod\n.\n\n\n1\nBase\n.\nacosh\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cosine of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic cosine for \nn\n realizations.\n\n\nsource\n\n\n\n\nTangent\n\n\n#\n\n\nBase\n.\ntan\n \n \nMethod\n.\n\n\n1\nBase\n.\ntan\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the tangent of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ntan\n \n \nMethod\n.\n\n\n1\nBase\n.\ntan\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the tangent of the uncertain value \nx\n, where \nx\n is in radians. Computes the  element-wise tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ntand\n \n \nMethod\n.\n\n\n1\nBase\n.\ntand\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the tangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ntand\n \n \nMethod\n.\n\n\n1\nBase\n.\ntand\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the tangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the  element-wise tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ntanh\n \n \nMethod\n.\n\n\n1\nBase\n.\ntanh\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic tangent of the uncertain value \nx\n. Computes the element-wise hyperbolic tangent for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\ntanh\n \n \nMethod\n.\n\n\n1\nBase\n.\ntanh\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the hyperbolic tangent of the uncertain value \nx\n.  Computes the element-wise hyperbolic tangent for \nn\n realizations.\n\n\nsource\n\n\n\n\nInverse cosecant\n\n\n#\n\n\nBase\n.\nMath\n.\nacsc\n \n \nMethod\n.\n\n\n1\nBase\n.\nacsc\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosecant of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacsc\n \n \nMethod\n.\n\n\n1\nBase\n.\nacsc\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosecant of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacscd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacscd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosecant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacscd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacscd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cosecant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacsch\n \n \nMethod\n.\n\n\n1\nBase\n.\nacscd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cosecant of the uncertain value \nx\n. Computes the element-wise inverse hypoerbolic cosecant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacsch\n \n \nMethod\n.\n\n\n1\nBase\n.\nacscd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cosecant of the uncertain value \nx\n. Computes the element-wise inverse hypoerbolic cosecant for \nn\n realizations.\n\n\nsource\n\n\n\n\nInverse secant\n\n\n#\n\n\nBase\n.\nMath\n.\nasec\n \n \nMethod\n.\n\n\n1\nBase\n.\nasec\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse secant of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasec\n \n \nMethod\n.\n\n\n1\nBase\n.\nasec\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse secant of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasecd\n \n \nMethod\n.\n\n\n1\nBase\n.\nasecd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse secant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasecd\n \n \nMethod\n.\n\n\n1\nBase\n.\nasecd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse secant of the uncertain value \nx\n, where \nx\n is in degrees.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasech\n \n \nMethod\n.\n\n\n1\nBase\n.\nasech\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic secant of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nasech\n \n \nMethod\n.\n\n\n1\nBase\n.\nasech\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic secant of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n\n\nInverse cotangent\n\n\n#\n\n\nBase\n.\nMath\n.\nacot\n \n \nMethod\n.\n\n\n1\nBase\n.\nacot\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cotangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacot\n \n \nMethod\n.\n\n\n1\nBase\n.\nacot\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cotangent of the uncertain value \nx\n, where \nx\n is in radians.  Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacotd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacotd\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cotangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacotd\n \n \nMethod\n.\n\n\n1\nBase\n.\nacotd\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse cotangent of the uncertain value \nx\n, where \nx\n is in degrees. Computes the element-wise inverse secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacoth\n \n \nMethod\n.\n\n\n1\nBase\n.\nacoth\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cotangent of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nacoth\n \n \nMethod\n.\n\n\n1\nBase\n.\nacoth\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\n\n\nCompute the inverse hyperbolic cotangent of the uncertain value \nx\n. Computes the element-wise inverse hyperbolic secant for \nn\n realizations.\n\n\nsource\n\n\n\n\nOther trig functions\n\n\n#\n\n\nBase\n.\nMath\n.\nsincos\n \n \nMethod\n.\n\n\n1\nBase\n.\nsincos\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nSimultaneously compute the sine and cosine of the uncertain value \nx\n, where \nx\n is in  radians. Computes the element-wise \nsincos\n for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsincos\n \n \nMethod\n.\n\n\n1\nBase\n.\nsincos\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nSimultaneously compute the sine and cosine of the uncertain value \nx\n, where \nx\n is in  radians. Computes the element-wise \nsincos\n for \nn\n realizations.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsinc\n \n \nMethod\n.\n\n\n1\nBase\n.\nsinc\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nIn an element-wise manner for \nn\n realizations of the uncertain value \nx\n, compute  \n\\sin(\\pi x) / (\\pi x)\n\\sin(\\pi x) / (\\pi x)\n if \nx \\neq 0\nx \\neq 0\n, and \n1\n1\n if \nx = 0\nx = 0\n.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsinc\n \n \nMethod\n.\n\n\n1\nBase\n.\nsinc\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\sin(\\pi x) / (\\pi x)\n\\sin(\\pi x) / (\\pi x)\n if \nx \\neq 0\nx \\neq 0\n, and \n1\n1\n if \nx = 0\nx = 0\n element-wise  over \nn\n realizations of the uncertain value \nx\n. \n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsinpi\n \n \nMethod\n.\n\n\n1\nBase\n.\nsinpi\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\sin(\\pi x)\n\\sin(\\pi x)\n more accurately than \nsin\n(\npi\n*\nx\n)\n, especially for large \nx\n,  in an element-wise over \nn\n realizations of the uncertain value \nx\n. \n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\nsinpi\n \n \nMethod\n.\n\n\n1\nBase\n.\nsinpi\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\sin(\\pi x)\n\\sin(\\pi x)\n more accurately than \nsin\n(\npi\n*\nx\n)\n, especially for large \nx\n,  in an element-wise over \nn\n realizations of the uncertain value \nx\n. \n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncosc\n \n \nMethod\n.\n\n\n1\nBase\n.\ncosc\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)\n\\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)\n if \nx \\neq 0\nx \\neq 0\n, and \n0\n0\n if \nx = 0\nx = 0\n, in an element-wise manner over \nn\n realizations of the uncertain value \nx\n. \n\n\nThis is the derivative of \nsinc\n(\nx\n)\n.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncosc\n \n \nMethod\n.\n\n\n1\nBase\n.\ncosc\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)\n\\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)\n if \nx \\neq 0\nx \\neq 0\n, and \n0\n0\n if \nx = 0\nx = 0\n, in an element-wise manner over \nn\n realizations of the uncertain value \nx\n. \n\n\nThis is the derivative of \nsinc\n(\nx\n)\n.\n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncospi\n \n \nMethod\n.\n\n\n1\nBase\n.\ncospi\n(\nx\n::\nAbstractUncertainValue\n;\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\cos(\\pi x)\n\\cos(\\pi x)\n more accurately than \ncos\n(\npi\n*\nx\n)\n, especially for large \nx\n,  in an element-wise over \nn\n realizations of the uncertain value \nx\n. \n\n\nsource\n\n\n#\n\n\nBase\n.\nMath\n.\ncospi\n \n \nMethod\n.\n\n\n1\nBase\n.\ncospi\n(\nx\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n10000\n)\n\n\n\n\n\n\n\nCompute \n\\cos(\\pi x)\n\\cos(\\pi x)\n more accurately than \ncos\n(\npi\n*\nx\n)\n, especially for large \nx\n,  in an element-wise over \nn\n realizations of the uncertain value \nx\n. \n\n\nsource", 
            "title": "Trigonometric functions"
        }, 
        {
            "location": "/mathematics/trig_functions/#trigonometric_functions", 
            "text": "Trigonometric functions are supported for arbitrary uncertain values of different types. Like for  elementary operations , a resampling approach is  used for the computations.", 
            "title": "Trigonometric functions"
        }, 
        {
            "location": "/mathematics/trig_functions/#syntax", 
            "text": "Because elementary operations should work on arbitrary uncertain values, a resampling  approach is used to perform the mathematical operations. All mathematical  operations thus return a vector containing the results of repeated element-wise operations  (where each element is a resampled draw from the furnishing distribution(s) of the  uncertain value(s)).   Each trigonometric function comes in two versions.    The first syntax allows skipping providing the number of draws, which is set to 10000 by default    (e.g.  cos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 ) .  Using the second syntax, you have to explicitly provide the number of draws (e.g.  cos ( x :: AbstractUncertainValue ,   n :: Int ) ).", 
            "title": "Syntax"
        }, 
        {
            "location": "/mathematics/trig_functions/#possible_errors", 
            "text": "Beware: if the support of the funishing distribution for an uncertain value lies partly  outside the domain of the function, you risk encountering errors.", 
            "title": "Possible errors"
        }, 
        {
            "location": "/mathematics/trig_functions/#supported_trigonometric_functions", 
            "text": "", 
            "title": "Supported trigonometric functions"
        }, 
        {
            "location": "/mathematics/trig_functions/#sine", 
            "text": "#  Base . sin     Method .  1 Base . sin ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the sine of the uncertain value  x , where  x  is in radians. Computes the  element-wise sine for  n  realizations.  source  #  Base . sin     Method .  1 Base . sin ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the sine of the uncertain value  x , where  x  is in radians. Computes the  element-wise sine for  n  realizations.  source  #  Base . Math . sind     Method .  1 Base . sind ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the sine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise sine for  n  realizations.  source  #  Base . Math . sind     Method .  1 Base . sind ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the sine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise sine for  n  realizations.  source  1\n2 Base . sinh ( x :: AbstractUncertainValue ;   n :: Int )  Base . sinh ( x :: AbstractUncertainValue ,   n :: Int )", 
            "title": "Sine"
        }, 
        {
            "location": "/mathematics/trig_functions/#cosine", 
            "text": "#  Base . cos     Method .  1 Base . cos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosine of the uncertain value  x , where  x  is in radians. Computes the  element-wise cosine for  n  realizations.  source  #  Base . cos     Method .  1 Base . cos ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosine of the uncertain value  x , where  x  is in radians. Computes the  element-wise cosine for  n  realizations.  source  #  Base . Math . cosd     Method .  1 Base . cos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosine for  n  realizations.  source  #  Base . Math . cosd     Method .  1 Base . cos ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosine for  n  realizations.  source  #  Base . cosh     Method .  1 Base . cos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cosine of the uncertain value  x . Computes the element-wise hyperbolic cosine for  n  realizations.  source  #  Base . cosh     Method .  1 Base . cos ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cosine of the uncertain value  x . Computes the element-wise hyperbolic cosine for  n  realizations.  source", 
            "title": "Cosine"
        }, 
        {
            "location": "/mathematics/trig_functions/#tangent", 
            "text": "#  Base . atan     Method .  1 Base . atan ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse tangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse tangent for  n  realizations.  source  #  Base . atan     Method .  1 Base . atan ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse tangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse tangent for  n  realizations.  source  #  Base . Math . atand     Method .  1 Base . atand ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse tangent of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse tangent for  n  realizations.  source  #  Base . Math . atand     Method .  1 Base . atand ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse tangent of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse tangent for  n  realizations.  source  #  Base . atanh     Method .  1 Base . atanh ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hypoerbolic tangent of the uncertain value  x . Computes the element-wise inverse hypoerbolic tangent for  n  realizations.  source  #  Base . atanh     Method .  1 Base . atanh ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hypoerbolic tangent of the uncertain value  x . Computes the element-wise inverse hypoerbolic tangent for  n  realizations.  source", 
            "title": "Tangent"
        }, 
        {
            "location": "/mathematics/trig_functions/#reciprocal_trig_functions", 
            "text": "", 
            "title": "Reciprocal trig functions"
        }, 
        {
            "location": "/mathematics/trig_functions/#cosecant", 
            "text": "#  Base . Math . csc     Method .  1 Base . csc ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosecant of the uncertain value  x , where  x  is in radians. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . csc     Method .  1 Base . csc ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosecant of the uncertain value  x , where  x  is in radians. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . cscd     Method .  1 Base . cscd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosecant of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . cscd     Method .  1 Base . cscd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cosecant of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . csch     Method .  1 Base . cscd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cosecant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise hyperbolic cosecant for  n  realizations.  source  #  Base . Math . csch     Method .  1 Base . cscd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cosecant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise hyperbolic cosecant for  n  realizations.  source", 
            "title": "Cosecant"
        }, 
        {
            "location": "/mathematics/trig_functions/#secant", 
            "text": "#  Base . Math . sec     Method .  1 Base . sec ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the secant of the uncertain value  x , where  x  is in radians. Computes the  element-wise secant for  n  realizations.  source  #  Base . Math . sec     Method .  1 Base . sec ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the secant of the uncertain value  x , where  x  is in radians. Computes the  element-wise secant for  n  realizations.  source  #  Base . Math . secd     Method .  1 Base . secd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the secant of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . secd     Method .  1 Base . secd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the secant of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cosecant for  n  realizations.  source  #  Base . Math . sech     Method .  1 Base . sech ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic secant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise hyperbolic secant for  n  realizations.  source  #  Base . Math . sech     Method .  1 Base . sech ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic secant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise hyperbolic secant for  n  realizations.  source", 
            "title": "Secant"
        }, 
        {
            "location": "/mathematics/trig_functions/#cotangent", 
            "text": "#  Base . Math . cot     Method .  1 Base . cot ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cotangent of the uncertain value  x , where  x  is in radians. Computes the  element-wise cotangent for  n  realizations.  source  #  Base . Math . cot     Method .  1 Base . cot ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cotangent of the uncertain value  x , where  x  is in radians. Computes the  element-wise cotangent for  n  realizations.  source  #  Base . Math . cotd     Method .  1 Base . cotd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cotangent of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cotangent for  n  realizations.  source  #  Base . Math . cotd     Method .  1 Base . cotd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the cotangent of the uncertain value  x , where  x  is in degrees. Computes the  element-wise cotangent for  n  realizations.  source  #  Base . Math . coth     Method .  1 Base . coth ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cotangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise hyperbolic cotangent for  n  realizations.  source  #  Base . Math . coth     Method .  1 Base . coth ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic cotangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise hyperbolic cotangent for  n  realizations.  source", 
            "title": "Cotangent"
        }, 
        {
            "location": "/mathematics/trig_functions/#inverse_trig_functions", 
            "text": "", 
            "title": "Inverse trig functions"
        }, 
        {
            "location": "/mathematics/trig_functions/#sine_1", 
            "text": "#  Base . asin     Method .  1 Base . asin ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse sine of the uncertain value  x , where  x  is in radians. Computes the  element-wise inverse sine for  n  realizations.  source  #  Base . asin     Method .  1 Base . asin ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse sine of the uncertain value  x , where  x  is in radians. Computes the  element-wise inverse sine for  n  realizations.  source  #  Base . Math . asind     Method .  1 Base . asind ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse sine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise inverse sine for  n  realizations.  source  #  Base . Math . asind     Method .  1 Base . asind ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse sine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise inverse sine for  n  realizations.  source  1\n2 Base . asinh ( x :: AbstractUncertainValue ;   n :: Int )  Base . asinh ( x :: AbstractUncertainValue ,   n :: Int )", 
            "title": "Sine"
        }, 
        {
            "location": "/mathematics/trig_functions/#cosine_1", 
            "text": "#  Base . acos     Method .  1 Base . acos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosine of the uncertain value  x , where  x  is in radians. Computes the  element-wise inverse cosine for  n  realizations.  source  #  Base . acos     Method .  1 Base . acos ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosine of the uncertain value  x , where  x  is in radians. Computes the  element-wise inverse cosine for  n  realizations.  source  #  Base . Math . acosd     Method .  1 Base . acosd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise inverse cosine for  n  realizations.  source  #  Base . Math . acosd     Method .  1 Base . acosd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosine of the uncertain value  x , where  x  is in degrees. Computes the  element-wise inverse cosine for  n  realizations.  source  #  Base . acosh     Method .  1 Base . acosh ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cosine of the uncertain value  x . Computes the element-wise inverse hyperbolic cosine for  n  realizations.  source  #  Base . acosh     Method .  1 Base . acosh ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cosine of the uncertain value  x . Computes the element-wise inverse hyperbolic cosine for  n  realizations.  source", 
            "title": "Cosine"
        }, 
        {
            "location": "/mathematics/trig_functions/#tangent_1", 
            "text": "#  Base . tan     Method .  1 Base . tan ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the tangent of the uncertain value  x , where  x  is in radians. Computes the  element-wise tangent for  n  realizations.  source  #  Base . tan     Method .  1 Base . tan ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the tangent of the uncertain value  x , where  x  is in radians. Computes the  element-wise tangent for  n  realizations.  source  #  Base . Math . tand     Method .  1 Base . tand ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the tangent of the uncertain value  x , where  x  is in degrees. Computes the  element-wise tangent for  n  realizations.  source  #  Base . Math . tand     Method .  1 Base . tand ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the tangent of the uncertain value  x , where  x  is in degrees. Computes the  element-wise tangent for  n  realizations.  source  #  Base . tanh     Method .  1 Base . tanh ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic tangent of the uncertain value  x . Computes the element-wise hyperbolic tangent for  n  realizations.  source  #  Base . tanh     Method .  1 Base . tanh ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the hyperbolic tangent of the uncertain value  x .  Computes the element-wise hyperbolic tangent for  n  realizations.  source", 
            "title": "Tangent"
        }, 
        {
            "location": "/mathematics/trig_functions/#inverse_cosecant", 
            "text": "#  Base . Math . acsc     Method .  1 Base . acsc ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosecant of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse cosecant for  n  realizations.  source  #  Base . Math . acsc     Method .  1 Base . acsc ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosecant of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse cosecant for  n  realizations.  source  #  Base . Math . acscd     Method .  1 Base . acscd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosecant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse cosecant for  n  realizations.  source  #  Base . Math . acscd     Method .  1 Base . acscd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cosecant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse cosecant for  n  realizations.  source  #  Base . Math . acsch     Method .  1 Base . acscd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cosecant of the uncertain value  x . Computes the element-wise inverse hypoerbolic cosecant for  n  realizations.  source  #  Base . Math . acsch     Method .  1 Base . acscd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cosecant of the uncertain value  x . Computes the element-wise inverse hypoerbolic cosecant for  n  realizations.  source", 
            "title": "Inverse cosecant"
        }, 
        {
            "location": "/mathematics/trig_functions/#inverse_secant", 
            "text": "#  Base . Math . asec     Method .  1 Base . asec ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse secant of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . asec     Method .  1 Base . asec ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse secant of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . asecd     Method .  1 Base . asecd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse secant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . asecd     Method .  1 Base . asecd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse secant of the uncertain value  x , where  x  is in degrees.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . asech     Method .  1 Base . asech ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic secant of the uncertain value  x . Computes the element-wise inverse hyperbolic secant for  n  realizations.  source  #  Base . Math . asech     Method .  1 Base . asech ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic secant of the uncertain value  x . Computes the element-wise inverse hyperbolic secant for  n  realizations.  source", 
            "title": "Inverse secant"
        }, 
        {
            "location": "/mathematics/trig_functions/#inverse_cotangent", 
            "text": "#  Base . Math . acot     Method .  1 Base . acot ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cotangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . acot     Method .  1 Base . acot ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cotangent of the uncertain value  x , where  x  is in radians.  Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . acotd     Method .  1 Base . acotd ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cotangent of the uncertain value  x , where  x  is in degrees. Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . acotd     Method .  1 Base . acotd ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse cotangent of the uncertain value  x , where  x  is in degrees. Computes the element-wise inverse secant for  n  realizations.  source  #  Base . Math . acoth     Method .  1 Base . acoth ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cotangent of the uncertain value  x . Computes the element-wise inverse hyperbolic secant for  n  realizations.  source  #  Base . Math . acoth     Method .  1 Base . acoth ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )   -   Vector { Float64 }    Compute the inverse hyperbolic cotangent of the uncertain value  x . Computes the element-wise inverse hyperbolic secant for  n  realizations.  source", 
            "title": "Inverse cotangent"
        }, 
        {
            "location": "/mathematics/trig_functions/#other_trig_functions", 
            "text": "#  Base . Math . sincos     Method .  1 Base . sincos ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    Simultaneously compute the sine and cosine of the uncertain value  x , where  x  is in  radians. Computes the element-wise  sincos  for  n  realizations.  source  #  Base . Math . sincos     Method .  1 Base . sincos ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )    Simultaneously compute the sine and cosine of the uncertain value  x , where  x  is in  radians. Computes the element-wise  sincos  for  n  realizations.  source  #  Base . Math . sinc     Method .  1 Base . sinc ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    In an element-wise manner for  n  realizations of the uncertain value  x , compute   \\sin(\\pi x) / (\\pi x) \\sin(\\pi x) / (\\pi x)  if  x \\neq 0 x \\neq 0 , and  1 1  if  x = 0 x = 0 .  source  #  Base . Math . sinc     Method .  1 Base . sinc ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )    Compute  \\sin(\\pi x) / (\\pi x) \\sin(\\pi x) / (\\pi x)  if  x \\neq 0 x \\neq 0 , and  1 1  if  x = 0 x = 0  element-wise  over  n  realizations of the uncertain value  x .   source  #  Base . Math . sinpi     Method .  1 Base . sinpi ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    Compute  \\sin(\\pi x) \\sin(\\pi x)  more accurately than  sin ( pi * x ) , especially for large  x ,  in an element-wise over  n  realizations of the uncertain value  x .   source  #  Base . Math . sinpi     Method .  1 Base . sinpi ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    Compute  \\sin(\\pi x) \\sin(\\pi x)  more accurately than  sin ( pi * x ) , especially for large  x ,  in an element-wise over  n  realizations of the uncertain value  x .   source  #  Base . Math . cosc     Method .  1 Base . cosc ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    Compute  \\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2) \\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)  if  x \\neq 0 x \\neq 0 , and  0 0  if  x = 0 x = 0 , in an element-wise manner over  n  realizations of the uncertain value  x .   This is the derivative of  sinc ( x ) .  source  #  Base . Math . cosc     Method .  1 Base . cosc ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )    Compute  \\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2) \\cos(\\pi x) / x - \\sin(\\pi x) / (\\pi x^2)  if  x \\neq 0 x \\neq 0 , and  0 0  if  x = 0 x = 0 , in an element-wise manner over  n  realizations of the uncertain value  x .   This is the derivative of  sinc ( x ) .  source  #  Base . Math . cospi     Method .  1 Base . cospi ( x :: AbstractUncertainValue ;   n :: Int   =   10000 )    Compute  \\cos(\\pi x) \\cos(\\pi x)  more accurately than  cos ( pi * x ) , especially for large  x ,  in an element-wise over  n  realizations of the uncertain value  x .   source  #  Base . Math . cospi     Method .  1 Base . cospi ( x :: AbstractUncertainValue ,   n :: Int   =   10000 )    Compute  \\cos(\\pi x) \\cos(\\pi x)  more accurately than  cos ( pi * x ) , especially for large  x ,  in an element-wise over  n  realizations of the uncertain value  x .   source", 
            "title": "Other trig functions"
        }, 
        {
            "location": "/uncertain_statistics/core_stats/core_statistics/", 
            "text": "This package implements many of the statistical algorithms in \nStatsBase\n for uncertain  values and uncertain datasets.\n\n\nThe syntax for calling the algorithms is the same as in \nStatsBase\n, but the functions here accept an additional positional argument \nn\n. This additional  argument controls how many times the uncertain values are resampled to compute the  statistics. For theoretical distributions, both with known and fitted parameters, some of  the stats functions may be called without the \nn\n argument.\n\n\n\n\nStatistics of single uncertain values\n\n\n#\n\n\nStatistics\n.\nmean\n \n \nMethod\n.\n\n\n1\nmean\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the mean of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nmedian\n \n \nMethod\n.\n\n\n1\nmedian\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the median of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nmiddle\n \n \nMethod\n.\n\n\n1\nmiddle\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the middle of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nstd\n \n \nMethod\n.\n\n\n1\nstd\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the standard deviation of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nvar\n \n \nMethod\n.\n\n\n1\nvariance\n(\nuv\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the variance of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nquantile\n \n \nMethod\n.\n\n\n1\nquantile\n(\nuv\n::\nAbstractUncertainValue\n,\n \nq\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the quantile(s) \nq\n of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n\n\nStatistics on datasets of uncertain values\n\n\nThe following statistics are available for uncertain datasets (collections of uncertain values).\n\n\n#\n\n\nStatistics\n.\nmean\n \n \nMethod\n.\n\n\n1\nmean\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nComputes the element-wise mean of a dataset of uncertain values. Takes the mean of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nmedian\n \n \nMethod\n.\n\n\n1\nmedian\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nComputes the element-wise median of a dataset of uncertain values. Takes the median of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nmiddle\n \n \nMethod\n.\n\n\n1\nmiddle\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n)\n\n\n\n\n\n\n\nCompute the middle of \nn\n realisations of an \nAbstractUncertainValueDataset\n.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nstd\n \n \nMethod\n.\n\n\n1\nstd\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...\n)\n\n\n\n\n\n\n\nComputes the element-wise standard deviation of a dataset of uncertain values. Takes the standard deviation of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nvar\n \n \nMethod\n.\n\n\n1\nvar\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...\n)\n\n\n\n\n\n\n\nComputes the element-wise sample variance of a dataset of uncertain values. Takes the sample variance of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\nquantile\n \n \nMethod\n.\n\n\n1\nquantile\n(\nd\n::\nAbstractUncertainValueDataset\n,\n \np\n,\n \nn\n::\nInt\n;\n \nkwargs\n...\n)\n\n\n\n\n\n\n\nCompute element-wise quantile(s) \np\nof a dataset consisting of uncertain values. Takes the quantiles of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\ncov\n \n \nMethod\n.\n\n\n1\ncov\n(\nd1\n::\nAbstractUncertainValueDataset\n,\n \nd2\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...\n)\n\n\n\n\n\n\n\nCompute the covariance between two \nAbstractUncertainValueDataset\ns by realising both datasets \nn\n times.\n\n\nsource\n\n\n#\n\n\nStatistics\n.\ncor\n \n \nMethod\n.\n\n\n1\ncor\n(\nd1\n::\nAbstractUncertainValueDataset\n,\n \nd2\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...\n)\n\n\n\n\n\n\n\nCompute the Pearson correlation between two \nAbstractUncertainValueDataset\ns by realising both datasets \nn\n times.\n\n\nsource", 
            "title": "Core statistics"
        }, 
        {
            "location": "/uncertain_statistics/core_stats/core_statistics/#statistics_of_single_uncertain_values", 
            "text": "#  Statistics . mean     Method .  1 mean ( uv :: AbstractUncertainValue ,   n :: Int )    Compute the mean of an uncertain value over an  n -draw sample of it.  source  #  Statistics . median     Method .  1 median ( uv :: AbstractUncertainValue ,   n :: Int )    Compute the median of an uncertain value over an  n -draw sample of it.  source  #  Statistics . middle     Method .  1 middle ( uv :: AbstractUncertainValue ,   n :: Int )    Compute the middle of an uncertain value over an  n -draw sample of it.  source  #  Statistics . std     Method .  1 std ( uv :: AbstractUncertainValue ,   n :: Int )    Compute the standard deviation of an uncertain value over an  n -draw sample of it.  source  #  Statistics . var     Method .  1 variance ( uv :: AbstractUncertainValue ,   n :: Int )    Compute the variance of an uncertain value over an  n -draw sample of it.  source  #  Statistics . quantile     Method .  1 quantile ( uv :: AbstractUncertainValue ,   q ,   n :: Int )    Compute the quantile(s)  q  of an uncertain value over an  n -draw sample of it.  source", 
            "title": "Statistics of single uncertain values"
        }, 
        {
            "location": "/uncertain_statistics/core_stats/core_statistics/#statistics_on_datasets_of_uncertain_values", 
            "text": "The following statistics are available for uncertain datasets (collections of uncertain values).  #  Statistics . mean     Method .  1 mean ( d :: AbstractUncertainValueDataset ,   n :: Int )    Computes the element-wise mean of a dataset of uncertain values. Takes the mean of an  n -draw sample for each element.  source  #  Statistics . median     Method .  1 median ( d :: AbstractUncertainValueDataset ,   n :: Int )    Computes the element-wise median of a dataset of uncertain values. Takes the median of an  n -draw sample for each element.  source  #  Statistics . middle     Method .  1 middle ( d :: AbstractUncertainValueDataset ,   n :: Int )    Compute the middle of  n  realisations of an  AbstractUncertainValueDataset .  source  #  Statistics . std     Method .  1 std ( d :: AbstractUncertainValueDataset ,   n :: Int ;   kwargs ... )    Computes the element-wise standard deviation of a dataset of uncertain values. Takes the standard deviation of an  n -draw sample for each element.  source  #  Statistics . var     Method .  1 var ( d :: AbstractUncertainValueDataset ,   n :: Int ;   kwargs ... )    Computes the element-wise sample variance of a dataset of uncertain values. Takes the sample variance of an  n -draw sample for each element.  source  #  Statistics . quantile     Method .  1 quantile ( d :: AbstractUncertainValueDataset ,   p ,   n :: Int ;   kwargs ... )    Compute element-wise quantile(s)  p of a dataset consisting of uncertain values. Takes the quantiles of an  n -draw sample for each element.  source  #  Statistics . cov     Method .  1 cov ( d1 :: AbstractUncertainValueDataset ,   d2 :: AbstractUncertainValueDataset ,   n :: Int ;   kwargs ... )    Compute the covariance between two  AbstractUncertainValueDataset s by realising both datasets  n  times.  source  #  Statistics . cor     Method .  1 cor ( d1 :: AbstractUncertainValueDataset ,   d2 :: AbstractUncertainValueDataset ,   n :: Int ;   kwargs ... )    Compute the Pearson correlation between two  AbstractUncertainValueDataset s by realising both datasets  n  times.  source", 
            "title": "Statistics on datasets of uncertain values"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/hypothesis_tests_overview/", 
            "text": "In addition to providing ensemble computation of basic statistic measures, this package also wraps various hypothesis tests from \nHypothesisTests\n.\njl\n. This allows us to perform hypothesis testing on ensemble realisations of the data.\n\n\n\n\nImplemented hypothesis tests\n\n\nThe following hypothesis tests are implemented for uncertain data types.\n\n\n\n\nOne sample t-test\n.\n\n\nEqual variance t-test\n.\n\n\nUnequal variance t-test\n.\n\n\nExact Kolmogorov-Smirnov test\n.\n\n\nApproximate two-sample Kolmogorov-Smirnov test\n.\n\n\nOne-sample Anderson\u2013Darling test\n.\n\n\nJarque-Bera test\n.\n\n\n\n\n\n\nTerminology\n\n\nPooled statistics\n are computed by sampling all uncertain values comprising the dataset n times, pooling the values together and treating them as one variable, then computing the statistic.\n\n\nElement-wise statistics\n are computed by sampling each uncertain value n times, keeping the data generated from each uncertain value separate. The statistics are the computed separately for each sample.", 
            "title": "Overview"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/hypothesis_tests_overview/#implemented_hypothesis_tests", 
            "text": "The following hypothesis tests are implemented for uncertain data types.   One sample t-test .  Equal variance t-test .  Unequal variance t-test .  Exact Kolmogorov-Smirnov test .  Approximate two-sample Kolmogorov-Smirnov test .  One-sample Anderson\u2013Darling test .  Jarque-Bera test .", 
            "title": "Implemented hypothesis tests"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/hypothesis_tests_overview/#terminology", 
            "text": "Pooled statistics  are computed by sampling all uncertain values comprising the dataset n times, pooling the values together and treating them as one variable, then computing the statistic.  Element-wise statistics  are computed by sampling each uncertain value n times, keeping the data generated from each uncertain value separate. The statistics are the computed separately for each sample.", 
            "title": "Terminology"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/one_sample_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nOneSampleTTest\n \n \nType\n.\n\n\n1\n2\nOneSampleTTest\n(\nd\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n1000\n;\n\n    \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nOneSampleTTest\n\n\n\n\n\n\n\nPerform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean \n\u03bc\n0\n against the alternative hypothesis that its distribution does not have mean \n\u03bc\n0\n. \nn\n indicates the number of draws during resampling.\n\n\nsource\n\n\nExample:\n\n\n1\n2\n3\n4\n5\n6\n# Normally distributed uncertain observation with mean = 2.1\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.1\n,\n \n0.2\n)\n\n\n\n# Perform a one-sample t-test to test the null hypothesis that\n\n\n# the sample comes from a distribution with mean \u03bc0\n\n\nOneSampleTTest\n(\nuv\n,\n \n1000\n,\n \n\u03bc0\n \n=\n \n2.1\n)\n\n\n\n\n\n\n\nWhich gives the following output:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Which results in\n\n\nOne\n \nsample\n \nt\n-\ntest\n\n\n-----------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nMean\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n2.1\n\n    \npoint\n \nestimate\n:\n          \n2.1031909275381566\n\n    \n95\n%\n \nconfidence\n \ninterval\n:\n \n(\n2.091\n,\n \n2.1154\n)\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nfail\n \nto\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n0.6089\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n1000\n\n    \nt\n-\nstatistic\n:\n              \n0.5117722099885472\n\n    \ndegrees\n \nof\n \nfreedom\n:\n       \n999\n\n    \nempirical\n \nstandard\n \nerror\n:\n \n0.00623505433839\n\n\n\n\n\n\n\nThus, we cannot reject the null-hypothesis that the sample comes from a distribution with mean = 2.1. Therefore, we accept the alternative hypothesis that our sample \ndoes\n in fact come from such a distribution. This is of course true, because we defined the uncertain value as a normal distribution with mean 2.1.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nOneSampleTTestPooled\n \n \nFunction\n.\n\n\n1\n2\n3\nOneSampleTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nOneSampleTTest\n\n\n\n\n\n\n\nFirst, sample \nn\n draws of each uncertain value in each dataset, pooling the draws from the elements of \nd1\n and the draws from the elements of \nd2\n separately. Then, perform a paired sample t-test of the null hypothesis that the differences between pairs of uncertain values in \nd1\n and \nd2\n come from a distribution with mean \n\u03bc\n0\n against the alternative hypothesis that the distribution does not have mean \n\u03bc\n0\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nOneSampleTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\n3\nOneSampleTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nOneSampleTTest\n}\n\n\n\n\n\n\n\nPerform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean \n\u03bc\n0\n against the alternative hypothesis that its distribution does not have mean \n\u03bc\n0\n for uncertain value in \nd\n.\n\n\nn\n indicates the number of draws during resampling.\n\n\nsource", 
            "title": "One sample t-test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/one_sample_t_test/#regular_test", 
            "text": "#  HypothesisTests . OneSampleTTest     Type .  1\n2 OneSampleTTest ( d :: AbstractUncertainValue ,   n :: Int   =   1000 ; \n     \u03bc0 :: Real   =   0 )   -   OneSampleTTest    Perform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean  \u03bc 0  against the alternative hypothesis that its distribution does not have mean  \u03bc 0 .  n  indicates the number of draws during resampling.  source  Example:  1\n2\n3\n4\n5\n6 # Normally distributed uncertain observation with mean = 2.1  uv   =   UncertainValue ( Normal ,   2.1 ,   0.2 )  # Perform a one-sample t-test to test the null hypothesis that  # the sample comes from a distribution with mean \u03bc0  OneSampleTTest ( uv ,   1000 ,   \u03bc0   =   2.1 )    Which gives the following output:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Which results in  One   sample   t - test  -----------------  Population   details : \n     parameter   of   interest :     Mean \n     value   under   h_0 :           2.1 \n     point   estimate :            2.1031909275381566 \n     95 %   confidence   interval :   ( 2.091 ,   2.1154 )  Test   summary : \n     outcome   with   95 %   confidence :   fail   to   reject   h_0 \n     two - sided   p - value :             0.6089  Details : \n     number   of   observations :     1000 \n     t - statistic :                0.5117722099885472 \n     degrees   of   freedom :         999 \n     empirical   standard   error :   0.00623505433839    Thus, we cannot reject the null-hypothesis that the sample comes from a distribution with mean = 2.1. Therefore, we accept the alternative hypothesis that our sample  does  in fact come from such a distribution. This is of course true, because we defined the uncertain value as a normal distribution with mean 2.1.", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/one_sample_t_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . OneSampleTTestPooled     Function .  1\n2\n3 OneSampleTTestPooled ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   OneSampleTTest    First, sample  n  draws of each uncertain value in each dataset, pooling the draws from the elements of  d1  and the draws from the elements of  d2  separately. Then, perform a paired sample t-test of the null hypothesis that the differences between pairs of uncertain values in  d1  and  d2  come from a distribution with mean  \u03bc 0  against the alternative hypothesis that the distribution does not have mean  \u03bc 0 .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/one_sample_t_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . OneSampleTTestElementWise     Function .  1\n2\n3 OneSampleTTestElementWise ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   Vector { OneSampleTTest }    Perform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean  \u03bc 0  against the alternative hypothesis that its distribution does not have mean  \u03bc 0  for uncertain value in  d .  n  indicates the number of draws during resampling.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/equal_variance_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nEqualVarianceTTest\n \n \nType\n.\n\n\n1\n2\nEqualVarianceTTest\n(\nd1\n::\nAbstractUncertainValue\n,\n \nd2\n::\nAbstractUncertainValue\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nEqualVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n and \ns2\n, each consisting of \nn\n random draws from the distributions furnishing \nd1\n and \nd2\n, respectively.\n\n\nThis function performs a two-sample t-test of the null hypothesis that \ns1\n and \ns2\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource\n\n\nExample\n\n\nLet's create two uncertain values furnished by distributions of different types. We'll perform the equal variance t-test to check if there is support for the null-hypothesis that the distributions furnishing the uncertain values come from distributions with equal means and variances.\n\n\nWe expect the test to reject this null-hypothesis, because we've created two very different distributions.\n\n\n1\n2\n3\n4\n5\nuv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1.2\n,\n \n0.3\n)\n\n\nuv2\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2\n,\n \n3\n)\n\n\n\n# EqualVarianceTTest on 1000 draws for each variable\n\n\nEqualVarianceTTest\n(\nuv1\n,\n \nuv2\n,\n \n1000\n)\n\n\n\n\n\n\n\nThe output is:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nTwo\n \nsample\n \nt\n-\ntest\n \n(\nequal\n \nvariance\n)\n\n\n----------------------------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nMean\n \ndifference\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n0\n\n    \npoint\n \nestimate\n:\n          \n-\n4.782470406651697\n\n    \n95\n%\n \nconfidence\n \ninterval\n:\n \n(\n-\n5.0428\n,\n \n-\n4.5222\n)\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n1e-99\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n[\n1000\n,\n1000\n]\n\n    \nt\n-\nstatistic\n:\n              \n-\n36.03293014520585\n\n    \ndegrees\n \nof\n \nfreedom\n:\n       \n1998\n\n    \nempirical\n \nstandard\n \nerror\n:\n \n0.1327249931487462\n\n\n\n\n\n\n\nThe test rejects the null-hypothesis, so we accept the alternative hypothesis that the samples come from distributions with different means and variances.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nEqualVarianceTTestPooled\n \n \nFunction\n.\n\n\n1\n2\nEqualVarianceTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nEqualVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1\n[\ni\n]\n and \nd2\n[\ni\n]\n, respectively. Gather all \ns1\n[\ni\n]\n in a pooled sample \nS1\n, and all \ns2\n[\ni\n]\n in a pooled sample \nS2\n.\n\n\nPerform a two-sample t-test of the null hypothesis that \nS1\n and \nS2\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nEqualVarianceTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nEqualVarianceTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nEqualVarianceTTest\n}\n\n\n\n\n\n\n\nConsider two samples \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1\n[\ni\n]\n and \nd2\n[\ni\n]\n, respectively. This function performs an elementwise \nEqualVarianceTTest\n on the pairs \n(\ns1\n[\ni\n]\n,\n \ns2\n[\ni\n]\n)\n. Specifically:\n\n\nPerforms an pairwise two-sample t-test of the null hypothesis that \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource", 
            "title": "Equal variance t-test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/equal_variance_t_test/#regular_test", 
            "text": "#  HypothesisTests . EqualVarianceTTest     Type .  1\n2 EqualVarianceTTest ( d1 :: AbstractUncertainValue ,   d2 :: AbstractUncertainValue , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   EqualVarianceTTest    Consider two samples  s1  and  s2 , each consisting of  n  random draws from the distributions furnishing  d1  and  d2 , respectively.  This function performs a two-sample t-test of the null hypothesis that  s1  and  s2  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source  Example  Let's create two uncertain values furnished by distributions of different types. We'll perform the equal variance t-test to check if there is support for the null-hypothesis that the distributions furnishing the uncertain values come from distributions with equal means and variances.  We expect the test to reject this null-hypothesis, because we've created two very different distributions.  1\n2\n3\n4\n5 uv1   =   UncertainValue ( Normal ,   1.2 ,   0.3 )  uv2   =   UncertainValue ( Gamma ,   2 ,   3 )  # EqualVarianceTTest on 1000 draws for each variable  EqualVarianceTTest ( uv1 ,   uv2 ,   1000 )    The output is:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 Two   sample   t - test   ( equal   variance )  ----------------------------------  Population   details : \n     parameter   of   interest :     Mean   difference \n     value   under   h_0 :           0 \n     point   estimate :            - 4.782470406651697 \n     95 %   confidence   interval :   ( - 5.0428 ,   - 4.5222 )  Test   summary : \n     outcome   with   95 %   confidence :   reject   h_0 \n     two - sided   p - value :             1e-99  Details : \n     number   of   observations :     [ 1000 , 1000 ] \n     t - statistic :                - 36.03293014520585 \n     degrees   of   freedom :         1998 \n     empirical   standard   error :   0.1327249931487462    The test rejects the null-hypothesis, so we accept the alternative hypothesis that the samples come from distributions with different means and variances.", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/equal_variance_t_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . EqualVarianceTTestPooled     Function .  1\n2 EqualVarianceTTestPooled ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   EqualVarianceTTest    Consider two samples  s1 [ i ]  and  s2 [ i ] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1 [ i ]  and  d2 [ i ] , respectively. Gather all  s1 [ i ]  in a pooled sample  S1 , and all  s2 [ i ]  in a pooled sample  S2 .  Perform a two-sample t-test of the null hypothesis that  S1  and  S2  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/equal_variance_t_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . EqualVarianceTTestElementWise     Function .  1\n2 EqualVarianceTTestElementWise ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   Vector { EqualVarianceTTest }    Consider two samples  s1 [ i ]  and  s2 [ i ] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1 [ i ]  and  d2 [ i ] , respectively. This function performs an elementwise  EqualVarianceTTest  on the pairs  ( s1 [ i ] ,   s2 [ i ] ) . Specifically:  Performs an pairwise two-sample t-test of the null hypothesis that  s1 [ i ]  and  s2 [ i ]  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/unequal_variance_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nUnequalVarianceTTest\n \n \nType\n.\n\n\n1\n2\nUnequalVarianceTTest\n(\nd1\n::\nAbstractUncertainValue\n,\n \nd2\n::\nAbstractUncertainValue\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nUnequalVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n and \ns2\n, each consisting of \nn\n random draws from the distributions furnishing \nd1\n and \nd2\n, respectively.\n\n\nPerform an unequal variance two-sample t-test of the null hypothesis that \ns1\n and \ns2\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nUnequalVarianceTTestPooled\n \n \nFunction\n.\n\n\n1\n2\nUnequalVarianceTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nUnequalVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1\n[\ni\n]\n and \nd2\n[\ni\n]\n, respectively. Gather all \ns1\n[\ni\n]\n in a pooled sample \nS1\n, and all \ns2\n[\ni\n]\n in a pooled sample \nS2\n.\n\n\nThis function performs an unequal variance two-sample t-test of the null hypothesis that \nS1\n and \nS2\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nUnequalVarianceTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nUnequalVarianceTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nUnequalVarianceTTest\n}\n\n\n\n\n\n\n\nConsider two samples \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1\n[\ni\n]\n and \nd2\n[\ni\n]\n, respectively. This function performs an elementwise \nEqualVarianceTTest\n on the pairs \n(\ns1\n[\ni\n]\n,\n \ns2\n[\ni\n]\n)\n. Specifically:\n\n\nPerforms an pairwise unequal variance two-sample t-test of the null hypothesis that \ns1\n[\ni\n]\n and \ns2\n[\ni\n]\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nThis test is sometimes known as Welch's t-test. It differs from the equal variance t-test in that it computes the number of degrees of freedom of the test using the Welch-Satterthwaite equation:\n\n\n\n\n\n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}\n\n\n\n\n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}\n\n\n\n\n\nsource", 
            "title": "Unequal variance t-test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/unequal_variance_t_test/#regular_test", 
            "text": "#  HypothesisTests . UnequalVarianceTTest     Type .  1\n2 UnequalVarianceTTest ( d1 :: AbstractUncertainValue ,   d2 :: AbstractUncertainValue , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   UnequalVarianceTTest    Consider two samples  s1  and  s2 , each consisting of  n  random draws from the distributions furnishing  d1  and  d2 , respectively.  Perform an unequal variance two-sample t-test of the null hypothesis that  s1  and  s2  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/unequal_variance_t_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . UnequalVarianceTTestPooled     Function .  1\n2 UnequalVarianceTTestPooled ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   UnequalVarianceTTest    Consider two samples  s1 [ i ]  and  s2 [ i ] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1 [ i ]  and  d2 [ i ] , respectively. Gather all  s1 [ i ]  in a pooled sample  S1 , and all  s2 [ i ]  in a pooled sample  S2 .  This function performs an unequal variance two-sample t-test of the null hypothesis that  S1  and  S2  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/unequal_variance_t_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . UnequalVarianceTTestElementWise     Function .  1\n2 UnequalVarianceTTestElementWise ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc0 :: Real   =   0 )   -   Vector { UnequalVarianceTTest }    Consider two samples  s1 [ i ]  and  s2 [ i ] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1 [ i ]  and  d2 [ i ] , respectively. This function performs an elementwise  EqualVarianceTTest  on the pairs  ( s1 [ i ] ,   s2 [ i ] ) . Specifically:  Performs an pairwise unequal variance two-sample t-test of the null hypothesis that  s1 [ i ]  and  s2 [ i ]  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  This test is sometimes known as Welch's t-test. It differs from the equal variance t-test in that it computes the number of degrees of freedom of the test using the Welch-Satterthwaite equation:   \n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}  \n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}   source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/exact_kolmogorov_smirnov_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nExactOneSampleKSTest\n \n \nType\n.\n\n\n1\n2\nExactOneSampleKSTest\n(\nuv\n::\nAbstractUncertainValue\n,\n\n    \nd\n::\nUnivariateDistribution\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nExactOneSampleKSTest\n\n\n\n\n\n\n\nPerform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that a draw of \nn\n realisations of the uncertain value \nuv\n comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\nExample\n\n\nWe'll test whether the uncertain value \nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2\n,\n \n4\n)\n comes from the theoretical distribution \nGamma\n(\n2\n,\n \n4\n)\n. Of course, we expect the test to confirm this, because we're using the exact same distribution.\n\n\n1\n2\n3\n4\n5\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2\n,\n \n4\n)\n\n\n\n# Perform the Kolgomorov-Smirnov test by drawing 1000 samples from the\n\n\n# uncertain value.\n\n\nExactOneSampleKSTest\n(\nuv\n,\n \nGamma\n(\n2\n,\n \n4\n),\n \n1000\n)\n\n\n\n\n\n\n\nThat gives the following output:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nExact\n \none\n \nsample\n \nKolmogorov\n-\nSmirnov\n \ntest\n\n\n----------------------------------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nSupremum\n \nof\n \nCDF\n \ndifferences\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n0.0\n\n    \npoint\n \nestimate\n:\n          \n0.0228345021301449\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nfail\n \nto\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n0.6655\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n1000\n\n\n\n\n\n\n\nAs expected, the test can't reject the hypothesis that the uncertain value \nuv\n comes from the theoretical distribution \nGamma\n(\n2\n,\n \n4\n)\n, precisely because it does.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nExactOneSampleKSTestPooled\n \n \nFunction\n.\n\n\n1\n2\nExactOneSampleKSTestPooled\n(\nud\n::\nUncertainDataset\n,\n\n    \nd\n::\nUnivariateDistribution\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nExactOneSampleKSTest\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that the pooled values comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nExactOneSampleKSTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nExactOneSampleKSTestElementWise\n(\nud\n::\nUncertainDataset\n,\n\n    \nd\n::\nUnivariateDistribution\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nVector\n{\nExactOneSampleKSTest\n}\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value.\n\n\nThen, perform an element-wise (pool-wise) one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that each value pool comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource", 
            "title": "Exact Kolmogorov-Smirnov test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/exact_kolmogorov_smirnov_test/#regular_test", 
            "text": "#  HypothesisTests . ExactOneSampleKSTest     Type .  1\n2 ExactOneSampleKSTest ( uv :: AbstractUncertainValue , \n     d :: UnivariateDistribution ,   n :: Int   =   1000 )   -   ExactOneSampleKSTest    Perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that a draw of  n  realisations of the uncertain value  uv  comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source  Example  We'll test whether the uncertain value  uv   =   UncertainValue ( Gamma ,   2 ,   4 )  comes from the theoretical distribution  Gamma ( 2 ,   4 ) . Of course, we expect the test to confirm this, because we're using the exact same distribution.  1\n2\n3\n4\n5 uv   =   UncertainValue ( Gamma ,   2 ,   4 )  # Perform the Kolgomorov-Smirnov test by drawing 1000 samples from the  # uncertain value.  ExactOneSampleKSTest ( uv ,   Gamma ( 2 ,   4 ),   1000 )    That gives the following output:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 Exact   one   sample   Kolmogorov - Smirnov   test  ----------------------------------------  Population   details : \n     parameter   of   interest :     Supremum   of   CDF   differences \n     value   under   h_0 :           0.0 \n     point   estimate :            0.0228345021301449  Test   summary : \n     outcome   with   95 %   confidence :   fail   to   reject   h_0 \n     two - sided   p - value :             0.6655  Details : \n     number   of   observations :     1000    As expected, the test can't reject the hypothesis that the uncertain value  uv  comes from the theoretical distribution  Gamma ( 2 ,   4 ) , precisely because it does.", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/exact_kolmogorov_smirnov_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . ExactOneSampleKSTestPooled     Function .  1\n2 ExactOneSampleKSTestPooled ( ud :: UncertainDataset , \n     d :: UnivariateDistribution ,   n :: Int   =   1000 )   -   ExactOneSampleKSTest    First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that the pooled values comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/exact_kolmogorov_smirnov_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . ExactOneSampleKSTestElementWise     Function .  1\n2 ExactOneSampleKSTestElementWise ( ud :: UncertainDataset , \n     d :: UnivariateDistribution ,   n :: Int   =   1000 )   -   Vector { ExactOneSampleKSTest }    First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value.  Then, perform an element-wise (pool-wise) one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that each value pool comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/", 
            "text": "Pooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nApproximateTwoSampleKSTestPooled\n \n \nFunction\n.\n\n\n1\n2\nApproximateTwoSampleKSTestPooled\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nApproximateTwoSampleKSTest\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nd1\n, then separately draw \nn\n realisations of each uncertain value in \nd2\n. Then, pool all realisations for \nd1\n together and all realisations of \nd2\n together.\n\n\nOn the pooled realisations, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the distribution furnishing the \nd1\n value pool represents the same distribution as the distribution furnishing the \nd2\n value pool, against the alternative hypothesis that the furnishing distributions are different.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nApproximateTwoSampleKSTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nApproximateTwoSampleKSTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nVector\n{\nApproximateTwoSampleKSTest\n}\n\n\n\n\n\n\n\nAssuming \nd1\n and \nd2\n contain the same number of uncertain observations, draw \nn\n realisations of each uncertain value in \nd1\n, then separately and separately draw \nn\n realisations of each uncertain value in \nd2\n.\n\n\nThen, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the uncertain values in \nd1\n and \nd2\n come from the same distribution against the alternative hypothesis that the (element-wise) values in  \nd1\n and \nd2\n come from different distributions.\n\n\nThe test is performed pairwise, i.e. ApproximateTwoSampleKSTest(d1[i], d2[i]) with \nn\n draws for the \ni\ni\n-ith pair of uncertain values.\n\n\nsource", 
            "title": "Approximate two-sample Kolmogorov-Smirnov test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . ApproximateTwoSampleKSTestPooled     Function .  1\n2 ApproximateTwoSampleKSTestPooled ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset ,   n :: Int   =   1000 )   -   ApproximateTwoSampleKSTest    First, draw  n  realisations of each uncertain value in  d1 , then separately draw  n  realisations of each uncertain value in  d2 . Then, pool all realisations for  d1  together and all realisations of  d2  together.  On the pooled realisations, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the distribution furnishing the  d1  value pool represents the same distribution as the distribution furnishing the  d2  value pool, against the alternative hypothesis that the furnishing distributions are different.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . ApproximateTwoSampleKSTestElementWise     Function .  1\n2 ApproximateTwoSampleKSTestElementWise ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset ,   n :: Int   =   1000 )   -   Vector { ApproximateTwoSampleKSTest }    Assuming  d1  and  d2  contain the same number of uncertain observations, draw  n  realisations of each uncertain value in  d1 , then separately and separately draw  n  realisations of each uncertain value in  d2 .  Then, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the uncertain values in  d1  and  d2  come from the same distribution against the alternative hypothesis that the (element-wise) values in   d1  and  d2  come from different distributions.  The test is performed pairwise, i.e. ApproximateTwoSampleKSTest(d1[i], d2[i]) with  n  draws for the  i i -ith pair of uncertain values.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/jarque_bera_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nJarqueBeraTest\n \n \nType\n.\n\n\n1\nJarqueBeraTest\n(\nd\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nJarqueBeraTest\n\n\n\n\n\n\n\nCompute the Jarque-Bera statistic to test the null hypothesis that an uncertain value is normally distributed.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nJarqueBeraTestPooled\n \n \nFunction\n.\n\n\n1\nJarqueBeraTestPooled\n(\nud\n::\nUncertainDataset\n,\n \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nJarqueBeraTest\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then, compute the Jarque-Bera statistic to test the null hypothesis that the values of the pool are normally distributed.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nJarqueBeraTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestElementWise\n(\nud\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nVector\n{\nJarqueBeraTest\n}\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value.\n\n\nThen, compute the Jarque-Bera statistic to test the null hypothesis that each value pool is normally distributed.\n\n\nsource", 
            "title": "Jarque-Bera test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/jarque_bera_test/#regular_test", 
            "text": "#  HypothesisTests . JarqueBeraTest     Type .  1 JarqueBeraTest ( d :: AbstractUncertainValue ,   n :: Int   =   1000 )   -   JarqueBeraTest    Compute the Jarque-Bera statistic to test the null hypothesis that an uncertain value is normally distributed.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/jarque_bera_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . JarqueBeraTestPooled     Function .  1 JarqueBeraTestPooled ( ud :: UncertainDataset ,   n :: Int   =   1000 )   -   JarqueBeraTest    First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then, compute the Jarque-Bera statistic to test the null hypothesis that the values of the pool are normally distributed.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/jarque_bera_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . JarqueBeraTestElementWise     Function .  1\n2 OneSampleADTestElementWise ( ud :: UncertainDataset , \n     n :: Int   =   1000 )   -   Vector { JarqueBeraTest }    First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value.  Then, compute the Jarque-Bera statistic to test the null hypothesis that each value pool is normally distributed.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/mann_whitney_u_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nMannWhitneyUTest\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest\n(\nd1\n::\nAbstractUncertainValue\n,\n \nd2\n::\nAbstractUncertainValue\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nMannWhitneyUTest\n\n\n\n\n\n\n\nLet \ns1\n and \ns2\n be samples of \nn\n realisations from the distributions furnishing the uncertain values \nd1\n and \nd2\n.\n\n\nPerform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \ns1\n is greater than an observation drawn from the same population as \ns2\n is equal to the probability that an observation drawn from the same population as \ns2\n is greater than an observation drawn from the same population as \ns1\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nMannWhitneyUTestPooled\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nMannWhitneyUTest\n\n\n\n\n\n\n\nLet \ns_{1_{i}}\ns_{1_{i}}\n be a sample of \nn\n realisations of the distribution furnishing the uncertain value \nd1\n[\ni\n]\n, where \ni \\in [1, 2, \\ldots, N]\ni \\in [1, 2, \\ldots, N]\n and \nN\nN\n is the number of uncertain values in \nd1\n.  Next, gather the samples for all \ns_{1_{i}}\ns_{1_{i}}\n in a pooled sample \nS_1\nS_1\n.  Do the same for the second uncertain dataset \nd2\n, yielding the pooled sample  \nS_2\nS_2\n.\n\n\nPerform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \nS_1\nS_1\n is greater than an observation drawn from the same population as \nS_2\nS_2\n is equal to the probability that an observation drawn from the same population as \nS_2\nS_2\n is greater than an observation drawn from the same population as \nS_1\nS_1\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nMannWhitneyUTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nVector\n{\nMannWhitneyUTest\n}\n\n\n\n\n\n\n\nAssume \nd1\n and \nd2\n consist of the same number of uncertain values. Let \ns_{1_{i}}\ns_{1_{i}}\n be a sample of \nn\n realisations of the distribution furnishing the uncertain value \nd1\n[\ni\n]\n, where \ni \\in [1, 2, \\ldots, N]\ni \\in [1, 2, \\ldots, N]\n and \nN\nN\n is the number of uncertain values in \nd1\n. Let \ns_{2_{i}}\ns_{2_{i}}\n be the corresponding sample for \nd2\n[\ni\n]\n. This function\n\n\nPerform an element-wise Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \ns_{1_{i}}\ns_{1_{i}}\n is greater than an observation drawn from the same population as \ns_{2_{i}}\ns_{2_{i}}\n is equal to the probability that an observation drawn from the same population as \ns_{2_{i}}\ns_{2_{i}}\n is greater than an observation drawn from the same population as \ns_{1_{i}}\ns_{1_{i}}\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource", 
            "title": "Mann-Whitney u-test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/mann_whitney_u_test/#regular_test", 
            "text": "#  HypothesisTests . MannWhitneyUTest     Function .  1\n2 MannWhitneyUTest ( d1 :: AbstractUncertainValue ,   d2 :: AbstractUncertainValue , \n     n :: Int   =   1000 )   -   MannWhitneyUTest    Let  s1  and  s2  be samples of  n  realisations from the distributions furnishing the uncertain values  d1  and  d2 .  Perform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  s1  is greater than an observation drawn from the same population as  s2  is equal to the probability that an observation drawn from the same population as  s2  is greater than an observation drawn from the same population as  s1  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/mann_whitney_u_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . MannWhitneyUTestPooled     Function .  1\n2 MannWhitneyUTest ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 )   -   MannWhitneyUTest    Let  s_{1_{i}} s_{1_{i}}  be a sample of  n  realisations of the distribution furnishing the uncertain value  d1 [ i ] , where  i \\in [1, 2, \\ldots, N] i \\in [1, 2, \\ldots, N]  and  N N  is the number of uncertain values in  d1 .  Next, gather the samples for all  s_{1_{i}} s_{1_{i}}  in a pooled sample  S_1 S_1 .  Do the same for the second uncertain dataset  d2 , yielding the pooled sample   S_2 S_2 .  Perform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  S_1 S_1  is greater than an observation drawn from the same population as  S_2 S_2  is equal to the probability that an observation drawn from the same population as  S_2 S_2  is greater than an observation drawn from the same population as  S_1 S_1  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/mann_whitney_u_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . MannWhitneyUTestElementWise     Function .  1\n2 MannWhitneyUTest ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 )   -   Vector { MannWhitneyUTest }    Assume  d1  and  d2  consist of the same number of uncertain values. Let  s_{1_{i}} s_{1_{i}}  be a sample of  n  realisations of the distribution furnishing the uncertain value  d1 [ i ] , where  i \\in [1, 2, \\ldots, N] i \\in [1, 2, \\ldots, N]  and  N N  is the number of uncertain values in  d1 . Let  s_{2_{i}} s_{2_{i}}  be the corresponding sample for  d2 [ i ] . This function  Perform an element-wise Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  s_{1_{i}} s_{1_{i}}  is greater than an observation drawn from the same population as  s_{2_{i}} s_{2_{i}}  is equal to the probability that an observation drawn from the same population as  s_{2_{i}} s_{2_{i}}  is greater than an observation drawn from the same population as  s_{1_{i}} s_{1_{i}}  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/anderson_darling_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests\n.\nOneSampleADTest\n \n \nType\n.\n\n\n1\n2\nOneSampleADTest\n(\nuv\n::\nUncertainValue\n,\n \nd\n::\nUnivariateDistribution\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n)\n \n-\n \nOneSampleADTest\n\n\n\n\n\n\n\nPerform a one-sample Anderson\u2013Darling test of the null hypothesis that a draw of \nn\n realisations of the uncertain value \nuv\n comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nOneSampleADTestPooled\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestPooled\n(\nud\n::\nUncertainDataset\n,\n \nd\n::\nUnivariateDistribution\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n))\n \n-\n \nOneSampleADTest\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then perform a one-sample Anderson\u2013Darling test of the null hypothesis that the pooled values comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData\n.\nUncertainStatistics\n.\nOneSampleADTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestElementWise\n(\nud\n::\nUncertainDataset\n,\n \nd\n::\nUnivariateDistribution\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n))\n \n-\n \nVector\n{\nOneSampleADTest\n}\n\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value. Then, perform an element-wise (pool-wise) one-sample Anderson\u2013Darling test of the null hypothesis that each value pool comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource", 
            "title": "Anderson-Darling test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/anderson_darling_test/#regular_test", 
            "text": "#  HypothesisTests . OneSampleADTest     Type .  1\n2 OneSampleADTest ( uv :: UncertainValue ,   d :: UnivariateDistribution , \n     n :: Int   =   1000 )   -   OneSampleADTest    Perform a one-sample Anderson\u2013Darling test of the null hypothesis that a draw of  n  realisations of the uncertain value  uv  comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/anderson_darling_test/#pooled_test", 
            "text": "#  UncertainData . UncertainStatistics . OneSampleADTestPooled     Function .  1\n2 OneSampleADTestPooled ( ud :: UncertainDataset ,   d :: UnivariateDistribution , \n     n :: Int   =   1000 ))   -   OneSampleADTest    First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then perform a one-sample Anderson\u2013Darling test of the null hypothesis that the pooled values comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/uncertain_statistics/hypothesistests/anderson_darling_test/#element-wise_test", 
            "text": "#  UncertainData . UncertainStatistics . OneSampleADTestElementWise     Function .  1\n2 OneSampleADTestElementWise ( ud :: UncertainDataset ,   d :: UnivariateDistribution , \n     n :: Int   =   1000 ))   -   Vector { OneSampleADTest }    First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value. Then, perform an element-wise (pool-wise) one-sample Anderson\u2013Darling test of the null hypothesis that each value pool comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/implementing_algorithms_for_uncertaindata/", 
            "text": "Extending existing algorithms for uncertain data types\n\n\nDo you already have an algorithm computing some statistic that you want to obtain uncertainty estimates for? Simply use Julia's multiple dispatch and create a version of the algorithm function that accepts the \nAbstractUncertainValue\n and \nAbstractUncertainDataset\n types, along with a \nSamplingConstraints\n specifying how the uncertain values are should be resampled.\n\n\nA basic function skeleton could be\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Some algorithm computing a statistic for a scalar-valued vector\n\n\nfunction\n \nmyalgorithm\n(\ndataset\n::\nVector\n{\nT\n};\n \nkwargs\n...\n)\n \nwhere\n \nT\n\n    \n# some algorithm returning a single-valued statistic\n\n\nend\n\n\n\n# Applying the algorithm to an ensemble of realisations from\n\n\n# an uncertain dataset, given a sampling constraint.\n\n\nfunction\n \nmyalgorithm\n(\nd\n::\nUncertainDataset\n,\n \nconstraint\n::\nC\n;\n\n        \nn_ensemble_realisations\n \n=\n \n100\n,\n \nkwargs\n...\n)\n\n        \nwhere\n \n{\nC\n \n:\n \nSamplingConstraint\n}\n\n\n    \nensemble_stats\n \n=\n \nzeros\n(\nn_ensemble_realisations\n)\n\n\n    \nfor\n \ni\n \nin\n \n1\n:\nn_ensemble_realisations\n\n        \nensemble_stats\n[\ni\n]\n \n=\n \nmyalgorithm\n(\nresample\n(\nd\n,\n \nconstraint\n);\n \nkwargs\n...\n)\n\n    \nend\n\n\n    \nreturn\n \nensemble_stats\n\n\nend", 
            "title": "Implementing algorithms for uncertain data"
        }, 
        {
            "location": "/implementing_algorithms_for_uncertaindata/#extending_existing_algorithms_for_uncertain_data_types", 
            "text": "Do you already have an algorithm computing some statistic that you want to obtain uncertainty estimates for? Simply use Julia's multiple dispatch and create a version of the algorithm function that accepts the  AbstractUncertainValue  and  AbstractUncertainDataset  types, along with a  SamplingConstraints  specifying how the uncertain values are should be resampled.  A basic function skeleton could be   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Some algorithm computing a statistic for a scalar-valued vector  function   myalgorithm ( dataset :: Vector { T };   kwargs ... )   where   T \n     # some algorithm returning a single-valued statistic  end  # Applying the algorithm to an ensemble of realisations from  # an uncertain dataset, given a sampling constraint.  function   myalgorithm ( d :: UncertainDataset ,   constraint :: C ; \n         n_ensemble_realisations   =   100 ,   kwargs ... ) \n         where   { C   :   SamplingConstraint } \n\n     ensemble_stats   =   zeros ( n_ensemble_realisations ) \n\n     for   i   in   1 : n_ensemble_realisations \n         ensemble_stats [ i ]   =   myalgorithm ( resample ( d ,   constraint );   kwargs ... ) \n     end \n\n     return   ensemble_stats  end", 
            "title": "Extending existing algorithms for uncertain data types"
        }, 
        {
            "location": "/changelog/", 
            "text": "Changelog\n\n\n\n\nUncertainData.jl v0.4.0\n\n\n\n\nNew functionality\n\n\n\n\nIntroduce an abstract resampling type \nAbstractUncertainDataResampling\n for this    package pending the implementation of \nAbstractResampling\n in StatsBase.jl.\n\n\nAdded \nConstrainedResampling\n resampling scheme.\n\n\n\n\nResample vectors of uncertain values without constraints. Syntax:\n\n\n\n\nresample\n(::\nVector\n{\n:\nAbstractUncertainValue\n}\n for single draws.\n\n\nresample\n(::\nVector\n{\n:\nAbstractUncertainValue\n}\n,\n \n::\nInt\n}\n for multiple draws.\n\n\n\n\nResample vectors of uncertain values with constraint(s) multiple times. Syntax:\n\n\n\n\n\n\nresample\n(::\nVector\n{\n:\nAbstractUncertainValue\n}\n,\n \n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\n:\nSamplingConstraint\n}}\n for single draws.\n\n\n\n\nresample\n(::\nVector\n{\n:\nAbstractUncertainValue\n}\n,\n \n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\n:\nSamplingConstraint\n}}\n,\n \n::\nInt\n for multiple draws.\n\n\n\n\n\n\n\n\n\n\nUncertainData.jl v0.3.0\n\n\n\n\nNew functionality\n\n\n\n\nAdded additional resampling methods for uncertain index and uncertain value datasets,    allowing passing vectors of constraints that are mapped to each value in the dataset. The    syntax is \nresample\n(::\nAbstractUncertainValueDataset\n,\n \n::\nVector\n{\n:\nSamplingConstraint\n}\n for a    single draw, and \nresample\n(::\nAbstractUncertainValueDataset\n,\n \n::\nVector\n{\n:\nSamplingConstraint\n}\n,\n \nn\n::\nInt\n   for \nn\n draws.\n\n\n\n\n\n\nUncertainData.jl v0.2.3\n\n\n\n\nImprovements\n\n\n\n\nAdded input validation when initialising \nTruncateQuantiles\n, \nTruncateRange\n and    \nTruncateStd\n.\n\n\nSeparate parameters types for \nTruncateQuantiles\n and \nTruncateRange\n, so one can do for    example \nTruncateRange\n(\n1\n,\n \n8\n.\n0\n)\n, instead of having to promote to \nFloat64\n.\n\n\nAdded validation for distribution truncation when resampling.\n\n\n\n\n\n\nUncertainData.jl v0.2.2\n\n\n\n\nNew functionality and syntax changes\n\n\n\n\nResampling vectors consisting of uncertain values (done in #61)\n\n\n\n\nresample\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n}\n,\n \nn\n::\nInt\n)\n is now interpreted as \"treat    \nuvals\n as a dataset and sample it \nn\n times\". Thus, it now behaves as    \nresample\n(\nAbstractUncertainDataset\n,\n \nn\n::\nInt\n)\n, returning \nn\n vectors of length    \nlength\n(\nuvals\n)\n, where the i-th element is a unique draw of \nuvals\n[\ni\n]\n.\n\n\nresample_elwise\n(\nuvals\n::\nVector\n{\nAbstractUncertainValue\n}\n,\n \nn\n::\nInt\n)\n takes over the role as    \"sample \nuvals\n element-wise and \nn\n times for each element\". Returns a vector of    length \nlength\n(\nuvals\n)\n, where the i-th element is a \nn\n-element vector of unique draws    of \nuvals\n[\ni\n]\n.\n\n\n\n\n\n\nResampling with subtypes of \nAbstractUncertainValueDataset\n\n\nCurrently, this affects the generic \nUncertainDataset\ns, as well as the specialized  \nUncertainIndexDataset\ns and \nUncertainValueDataset\ns.\n\n\n\n\nresample_elwise\n(\nuvd\n::\nAbstractUncertainValueDataset\n,\n \nn\n::\nInt\n)\n is now interpreted as    \"draw \nn\n realisations of each value in \nuvd\n\". Returns a vector of length \nlength\n(\nuvals\n)\n    where the i-th element is a \nn\n-element vector of unique draws of \nuvals\n[\ni\n]\n. This works    for \nUncertainDataset\ns, \nUncertainIndexDataset\ns, and \nUncertainValueDataset\ns.\n\n\nresample_elwise\n(\nuvd\n::\nAbstractUncertainValueDataset\n,\n \nconstraint\n::\nUnion\n{\nSamplingConstraint\n,\n \nVector\n{\nSamplingConstraint\n}}\n,\n \nn\n::\nInt\n)\n    is now interpreted as \"draw \nn\n realisations of each value in \nuvd\n, subjecting each value    in \nuvd\n to some sampling \nconstraint\n(s) during resampling\". Returns a vector of    length \nlength\n(\nuvals\n)\n where the i-th element is a \nn\n-element vector of unique draws    of \nuvals\n[\ni\n]\n, where the support of \nuvals\n[\ni\n]\n has been truncated by the provided    \nconstraint\n(s).\n\n\n\n\n\n\nBug fixes\n\n\n\n\nRemoved extra blank line from print method for \nAbstractUncertainPopulation\n.\n\n\n\n\n\n\nUncertainData.jl v0.2.1\n\n\n\n\nNew functionality\n\n\n\n\nmerge\n(\nuvals\n::\nVector\n{\n:AbstractUncertainValue\n}\n;\n \nn\n \n=\n \n1000\n)\n now makes it possible to    combine many uncertain values of different into one uncertain value represented by a    kernel density estimate. This is achieved by resampling each of the values \nn\n times,    then pooling the draws and estimating a total distribution using KDE.\n\n\nmerge\n(\nuvals\n::\nVector\n{\n:AbstractUncertainValue\n}\n;\n \nweights\n::\nWeights\n \nn\n \n=\n \n1000\n)\n,    \nmerge\n(\nuvals\n::\nVector\n{\n:AbstractUncertainValue\n}\n;\n \nweights\n::\nAnalyticalWeights\n \nn\n \n=\n \n1000\n)\n    and    \nmerge\n(\nuvals\n::\nVector\n{\n:AbstractUncertainValue\n}\n;\n \nweights\n::\nProbabilityWeights\n \nn\n \n=\n \n1000\n)\n   merges uncertain values by resampling them proportionally to \nweights\n, then pooling    the draws and performing KDE. These are all functionally equivalent, but implementations   for different weights are provided for compatibility with StatsBase.\n\n\nmerge\n(\nuvals\n::\nVector\n{\n:AbstractUncertainValue\n}\n;\n \nweights\n::\nFrequencyWeights\n \nn\n \n=\n \n1000\n)\n    merges uncertain values by sampling them according to the number of samples provided    with \nweights\n.\n\n\n\n\n\n\nBug fixes\n\n\n\n\nresample\n didn't work for \nUncertainIndexDataset\ns due to the data being stored in the    \nindices\n field, not the \nvalues\n field as for other subtypes of    \nAbstractUncertainValueDataset\n. This is now fixed.\n\n\n\n\n\n\nUncertainData.jl v0.2.0\n\n\n\n\nNotes\n\n\n\n\nJulia 1.1 is required for version \n v.0.2.0.\n\n\n\n\n\n\nNew functionality\n\n\n\n\nSpline interpolation on a regular grid.\n\n\nLinear interpolation on an irregular grid.\n\n\n\n\n\n\nImprovements\n\n\n\n\nsupport_overlap\n now returns an interval (from \nIntervalArithmetic\n), in line with    what \nsupport\n returns.\n\n\n\n\n\n\nUncertainData.jl v0.1.8\n\n\n\n\nBug fixes\n\n\n\n\nAdded missing package dependencies which were not caught by CI.\n\n\n\n\n\n\nUncertainData.jl v0.1.7\n\n\n\n\nNew functionality\n\n\n\n\nUncertainIndexValueDataset\ns can now be constructed from vectors of uncertain values.    To do so, provide a vector of uncertain values for the indices, and the same for the    values, e.g. \nUncertainIndexValueDataset\n([\nidx1\n,\n \nidx2\n],\n \n[\nval1\n,\n \nval2\n])\n.\n\n\nIndex-value dataset realizations can now be    \ninterpolated on a regular grid\n.\n\n\n\n\n\n\nBug fixes\n\n\n\n\nminima\n and \nmaxima\n now returns the global minimum for a dataset instead of a vector    of elementwise minima and maxima.\n\n\nImplemented the option to linearly interpolate index-value dataset realizations.    To do so, provide \nresample\n with a \nRegularGrid\n instance.\n\n\nMerged redundant methods for assigning some distributions.\n\n\nFixed non-critical indexing bug for uncertain index-value datasets.\n\n\nRemoved redudant method definitions and multiple imports of the same files causing    definitions to be overwritten and printing warnings statements when loading the package.\n\n\n\n\n\n\nUncertainData.jl v0.1.6\n\n\n\n\nNew functionality\n\n\n\n\nImplemented sequential sampling constraints \nStrictlyIncreasing\n and \nStrictlyDecreasing\n   for \nUncertainIndexValueDataset\ns.\n\n\nAdded \nUncertainScalarPopulation\n type, representing    vectors of values that should be sampled according to a vector of probabilities.\n\n\n\n\n\n\nImprovements\n\n\n\n\nImproved documentation for \nCertainValue\ns.\n\n\nAdded documentation for \nUncertainScalarPopulation\n.\n\n\nAdded \nUncertainScalarPopulation\n to uncertain value overview list in the documentation.\n\n\nFixed duplicate docs for \ncot\n, \ncotd\n, \ncoth\n and added missing \nacot\n, \nacotd\n, \nacoth\n   docs.\n\n\nShortened and updated main documentation page with more links.\n\n\n\n\n\n\nBug fixes\n\n\n\n\nImport \nBase\n functions properly when defining \nCertainValue\n, so that no unexpected    behaviour is introduced.\n\n\nFixed links in documentation that pointed to the wrong locations.\n\n\nRemove model resampling docs which was not supposed to be published until the    functionality is properly implemented.\n\n\n\n\n\n\nUncertainData.jl v0.1.5\n\n\n\n\nNew functionality\n\n\n\n\nAdded \nCertainValue\n type to represent scalars without   any uncertainty. Even though a scalar is not uncertain, we'll define it as subtype of    \nAbstractUncertainValue\n to treat certain values alongside uncertain values in datasets.\n\n\nAdded plot recipe for \nCertainValue\ns. They are just plotted as regular points.\n\n\nAdded method \nresample\n(\nVector\n{\nAbstractUncertainValue\n}\n)\n for resampling vectors of    uncertain values. Operates element-wise, just as for an uncertain dataset.\n\n\nAdded an abstract type \nSequentialSamplingConstraint\n to separate sequential constraints    from general constraints that might be applied \nbefore\n resampling according to    the sequential constraints.\n\n\nAdded abstract type (\nOrderedSamplingAlgorithm\n) and composite types    (\nStartToEnd\n, \nEndToStart\n, \nMidpointOutwards\n, \nChunksForwards\n, \nChunksBackwards\n)    which indicates how to sample sequential realizations when resampling an uncertain    dataset. Only \nStartToEnd\n is used at the moment.\n\n\nAdded abstract type \nSequentialSamplingConstraint\n which is the supertype for all    sequential constraints.\n\n\nAdded function to check if strictly increasing sequences through an uncertain dataset    exist: \nstrictly_increasing_sequence_exists\n(\nudata\n::\nAbstractUncertainValueDataset\n.\n\n\nAdded function to check if strictly decreasing sequences through an uncertain dataset    exist: \nstrictly_increasing_sequence_exists\n(\nudata\n::\nAbstractUncertainValueDataset\n.\n\n\nAdded the \nStrictlyIncreasing\n{\nT\n}\n \nwhere\n \n{\nT\n:\nOrderedSamplingAlgorithm\n}\n sequential    constraint for resampling uncertain datasets.\n\n\nAdded the \nStrictlyDecreasing\n{\nT\n}\n \nwhere\n \n{\nT\n:\nOrderedSamplingAlgorithm\n}\n sequential    constraint for resampling uncertain datasets.\n\n\n\n\nAdded resampling methods\n\n\n\n\nresample\n(\nudata\n,\n \nsequential_constraint\n::\nStrictlyIncreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\nresample\n(\nudata\n,\n \nsequential_constraint\n::\nStrictlyDecreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\nresample\n(\nudata\n,\n \nconstraint\n::\nSamplingConstraint\n,\n \nsequential_constraint\n::\nStrictlyIncreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\nresample\n(\nudata\n,\n \nconstraint\n::\nSamplingConstraint\n,\n \nsequential_constraint\n::\nStrictlyDecreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\nresample\n(\nudata\n,\n \nconstraint\n::\nVector\n{\nSamplingConstraint\n}\n,\n \nsequential_constraint\n::\nStrictlyIncreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\nresample\n(\nudata\n,\n \nconstraint\n::\nVector\n{\nSamplingConstraint\n}\n,\n \nsequential_constraint\n::\nStrictlyDecreasing\n{\nT\n}\n \nwhere\n \n{\nT\n \n:\n \nStartToEnd\n}\n\n\n\n\n\n\n\n\n\n\nImprovements\n\n\n\n\nAdded \ndocumentation on sequential constraints\n, clearly separating it from the general constraints.\n\n\n\n\n\n\nUncertainData.jl v0.1.4\n\n\n\n\nBreaking changes\n\n\n\n\nElementary operations for \n(\nscalar\n,\n \nuncertain_value\n)\n, \n(\nuncertain_value\n,\n \nscalar\n)\n and    \n(\nuncertain_value\n,\n \nuncertain_value\n)\n pairs now returns an uncertain value instead of    a vector of resampled realizations. The default behaviour is to perform a kernel    density estimate over the vector of results of the element-wise operations (which    was previously returned without representing it as an uncertain value).\n\n\n\n\n\n\nNew functionality\n\n\n\n\nImplemented constraints for datasets that have already been constrained.    \nconstrain\n(\nudata\n::\nConstrainedDataset\n,\n \ns\n::\nSamplingConstraint\n)\n will now return another    \nConstrainedDataset\n. The same applies for \nConstrainedIndexDataset\n and    \nConstrainedValueDataset\n.\n\n\nAdded \nmaximum\n(\nVector\n{\nAbstractUncertainValue\n}\n)\n and    \nminimum\n(\nVector\n{\nAbstractUncertainValue\n}\n)\n methods.\n\n\nAdded plot recipe for \nVector\n{\nAbstractUncertainValue\n}\ns. Behaves just as plotting an   uncertain dataset, assuming an implicit indices \n1\n:\nlength\n(\nv\n)\n. Error bars may be    tuned by providing a second argument of quantiles to \nplot\n, e.g. \nplot\n(\nv\n,\n \n[\n0\n.\n2\n,\n \n0\n.\n8\n]\n   gives error bars covering the 20\nth\n to 80\nth\n percentile range of the data.\n\n\n\n\n\n\nImprovements\n\n\n\n\nAdded documentation for \nStrictlyIncreasing\n and \nStrictlyDecreasing\n sampling    constraints.\n\n\nAdded \nshow\n function for \nAbstractUncertainIndexDataset\n. \nshow\n errored previously,    because it assumed the default behaviour of \nAbstractUncertainValueDataset\n, which    does not have the \nindices\n field.\n\n\n\n\n\n\nBug fixes\n\n\n\n\nFixed bug when resampling an uncertain dataset using the \nNoConstraint\n constraint,    which did not work to due to a reference to a non-existing variable.\n\n\nFixed test bug where when resampling an uncertain value with the \nTruncateStd\n sampling   constraint, the test compared the result to a fixed scalar, not the standar deviation    of the value. This sometimes made the travis build fail.\n\n\n\n\n\n\nUncertainData.jl v0.1.3\n\n\n\n\nNew functionality\n\n\n\n\nAllow both the \nindices\n and \nvalues\n fields of \nUncertainIndexValueDataset\n to be any    subtype of \nAbstractUncertainValueDataset\n. This way, you don't \nhave\n to use an    index dataset type for the indices if not necessary.\n\n\n\n\n\n\nImprovements\n\n\n\n\nImproved documentation for \nUncertainIndexDataset\n, \nUncertainValueDataset\n,    \nUncertainDataset\n and \nUncertainIndexValueDataset\n types and added an    \noverview page\n in the documentation    to explain the difference between these types.\n\n\nAdded an \noverview\n section for the resampling    documentation.\n\n\nCleaned and improved \ndocumentation for uncertain values\n.\n\n\nAdded separate \ndocumentation for the uncertain index dataset type\n.\n\n\nAdded separate \ndocumentation for the uncertain value dataset type\n.\n\n\nImproved \ndocumentation for the generic uncertain dataset type\n\n\nMerged documentation for sampling constraints and resampling.\n\n\nAdded missing documentation for the \nsinc\n, \nsincos\n, \nsinpi\n, \ncosc\n and \ncospi\n trig    functions.\n\n\n\n\n\n\nUncertainData.jl v0.1.2\n\n\n\n\nNew functionality\n\n\n\n\nSupport \nelementary mathematical operations\n    (\n+\n, \n-\n, \n*\n and \n/\n) between arbitrary    uncertain values of different types. Also works with the combination of scalars and    uncertain values. Because elementary operations should work on arbitrary uncertain    values, a resampling approach is used to perform the mathematical operations. This    means that all mathematical operations return a vector containing the results of    repeated element-wise operations (where each element is a resampled draw from the    furnishing distribution(s) of the uncertain value(s)). The default number of    realizations is set to \n10000\n. This allows calling \nuval1\n \n+\n \nuval2\n for two uncertain    values \nuval1\n and \nuval2\n. If you need to tune the number of resample draws to \nn\n,    you need to use the \n+\n(\nuval1\n,\n \nuval2\n,\n \nn\n)\n syntax (similar for the operators). In the    future, elementary operations might be improved for certain combinations of uncertain   values where exact expressions for error propagation are now, for example using the    machinery in \nMeasurements\n.\njl\n for normally distributed values.\n\n\nSupport for \ntrigonometric functions\n added (\nsin\n, \nsind\n, \nsinh\n, \ncos\n,   \ncosd\n, \ncosh\n, \ntan\n, \ntand\n, \ntanh\n, \ncsc\n, \ncscd\n, \ncsch\n, \ncsc\n, \ncscd\n, \ncsch\n,    \nsec\n, \nsecd\n, \nsech\n, \ncot\n, \ncotd\n, \ncoth\n, \nsincos\n, \nsinc\n, \nsinpi\n, \ncosc\n,    \ncospi\n). Inverses are also defined (\nasin\n, \nasind\n, \nasinh\n, \nacos\n,   \nacosd\n, \nacosh\n, \natan\n, \natand\n, \natanh\n, \nacsc\n, \nacscd\n, \nacsch\n, \nacsc\n, \nacscd\n,    \nacsch\n, \nasec\n, \nasecd\n, \nasech\n, \nacot\n, \nacotd\n, \nacoth\n).   Beware: if the support of the funishing distribution for an uncertain value lies partly    outside the domain of the function, you risk encountering errors.   These also use a resampling approach, using \n10000\n realizations by default.    Use either the \nsin\n(\nuval\n)\n syntax for the default, and \nsin\n(\nuval\n,\n \nn\n::\nInt\n)\n to tune the    number of samples.\n\n\nSupport non-integer multiples of the standard deviation in the \nTruncateStd\n sampling    constraint.\n\n\n\n\n\n\nFixes\n\n\n\n\nFixed bug in resampling of index-value datasets, where the \nn\n arguments wasn't used.\n\n\nBugfix: due to \nStatsBase\n.\nstd\n not being defined for \nFittedDistribution\n instances,    uncertain values represented by \nUncertainScalarTheoreticalFit\n instances were not    compatible with the \nTruncateStd\n sampling constraint. Now fixed!\n\n\nAdded missing \nresample\n(\nuv\n::\nAbstractUncertainValue\n,\n \nconstraint\n::\nTruncateRange\n,\n \nn\n::\nInt\n)\n    method.\n\n\n\n\n\n\nImprovements\n\n\n\n\nImproved resampling documentation for \nUncertainIndexValueDataset\ns. Now shows    the documentation for the main methods, as well as examples of how to use different    sampling constraints for each individual index and data value.\n\n\nImproved resampling documentation for \nUncertainDataset\ns. Now shows    the documentation for the main methods.\n\n\n\n\n\n\nUncertainData.jl v0.1.1\n\n\n\n\nNew functionality\n\n\n\n\nIndexing implemented for \nUncertainIndexValueDataset\n.\n\n\nResampling implemented for \nUncertainIndexValueDataset\n.\n\n\nUncertain values and uncertain datasets now support \nminimum\n and \nmaximum\n.\n\n\nsupport\n(\nuv\n::\nAbstractUncertainValue\n)\n now always returns an interval from    \nIntervalArithmetic.jl\n\n\nsupport_overlap\n now computes overlaps also for fitted theoretical distributions.\n\n\nAdded more plotting recipes.\n\n\nAll implemented uncertain data types now support resampling.\n\n\n\n\n\n\nImprovements\n\n\n\n\nImproved general documentation. Added a reference to \n   Measurements.jl\n and an explanation    for the differences between the packages.\n\n\nImproved resampling documentation with detailed explanation and plots.\n\n\n\n\n\n\nUncertainData.jl v0.1.0\n\n\n\n\nBasic functionality in place.", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v040", 
            "text": "", 
            "title": "UncertainData.jl v0.4.0"
        }, 
        {
            "location": "/changelog/#new_functionality", 
            "text": "Introduce an abstract resampling type  AbstractUncertainDataResampling  for this    package pending the implementation of  AbstractResampling  in StatsBase.jl.  Added  ConstrainedResampling  resampling scheme.   Resample vectors of uncertain values without constraints. Syntax:   resample (:: Vector { : AbstractUncertainValue }  for single draws.  resample (:: Vector { : AbstractUncertainValue } ,   :: Int }  for multiple draws.   Resample vectors of uncertain values with constraint(s) multiple times. Syntax:    resample (:: Vector { : AbstractUncertainValue } ,   :: Union { SamplingConstraint ,   Vector { : SamplingConstraint }}  for single draws.   resample (:: Vector { : AbstractUncertainValue } ,   :: Union { SamplingConstraint ,   Vector { : SamplingConstraint }} ,   :: Int  for multiple draws.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v030", 
            "text": "", 
            "title": "UncertainData.jl v0.3.0"
        }, 
        {
            "location": "/changelog/#new_functionality_1", 
            "text": "Added additional resampling methods for uncertain index and uncertain value datasets,    allowing passing vectors of constraints that are mapped to each value in the dataset. The    syntax is  resample (:: AbstractUncertainValueDataset ,   :: Vector { : SamplingConstraint }  for a    single draw, and  resample (:: AbstractUncertainValueDataset ,   :: Vector { : SamplingConstraint } ,   n :: Int    for  n  draws.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v023", 
            "text": "", 
            "title": "UncertainData.jl v0.2.3"
        }, 
        {
            "location": "/changelog/#improvements", 
            "text": "Added input validation when initialising  TruncateQuantiles ,  TruncateRange  and     TruncateStd .  Separate parameters types for  TruncateQuantiles  and  TruncateRange , so one can do for    example  TruncateRange ( 1 ,   8 . 0 ) , instead of having to promote to  Float64 .  Added validation for distribution truncation when resampling.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v022", 
            "text": "", 
            "title": "UncertainData.jl v0.2.2"
        }, 
        {
            "location": "/changelog/#new_functionality_and_syntax_changes", 
            "text": "", 
            "title": "New functionality and syntax changes"
        }, 
        {
            "location": "/changelog/#resampling_vectors_consisting_of_uncertain_values_done_in_61", 
            "text": "resample ( uvals :: Vector { AbstractUncertainValue } ,   n :: Int )  is now interpreted as \"treat     uvals  as a dataset and sample it  n  times\". Thus, it now behaves as     resample ( AbstractUncertainDataset ,   n :: Int ) , returning  n  vectors of length     length ( uvals ) , where the i-th element is a unique draw of  uvals [ i ] .  resample_elwise ( uvals :: Vector { AbstractUncertainValue } ,   n :: Int )  takes over the role as    \"sample  uvals  element-wise and  n  times for each element\". Returns a vector of    length  length ( uvals ) , where the i-th element is a  n -element vector of unique draws    of  uvals [ i ] .", 
            "title": "Resampling vectors consisting of uncertain values (done in #61)"
        }, 
        {
            "location": "/changelog/#resampling_with_subtypes_of_abstractuncertainvaluedataset", 
            "text": "Currently, this affects the generic  UncertainDataset s, as well as the specialized   UncertainIndexDataset s and  UncertainValueDataset s.   resample_elwise ( uvd :: AbstractUncertainValueDataset ,   n :: Int )  is now interpreted as    \"draw  n  realisations of each value in  uvd \". Returns a vector of length  length ( uvals )     where the i-th element is a  n -element vector of unique draws of  uvals [ i ] . This works    for  UncertainDataset s,  UncertainIndexDataset s, and  UncertainValueDataset s.  resample_elwise ( uvd :: AbstractUncertainValueDataset ,   constraint :: Union { SamplingConstraint ,   Vector { SamplingConstraint }} ,   n :: Int )     is now interpreted as \"draw  n  realisations of each value in  uvd , subjecting each value    in  uvd  to some sampling  constraint (s) during resampling\". Returns a vector of    length  length ( uvals )  where the i-th element is a  n -element vector of unique draws    of  uvals [ i ] , where the support of  uvals [ i ]  has been truncated by the provided     constraint (s).", 
            "title": "Resampling with subtypes of AbstractUncertainValueDataset"
        }, 
        {
            "location": "/changelog/#bug_fixes", 
            "text": "Removed extra blank line from print method for  AbstractUncertainPopulation .", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v021", 
            "text": "", 
            "title": "UncertainData.jl v0.2.1"
        }, 
        {
            "location": "/changelog/#new_functionality_2", 
            "text": "merge ( uvals :: Vector { :AbstractUncertainValue } ;   n   =   1000 )  now makes it possible to    combine many uncertain values of different into one uncertain value represented by a    kernel density estimate. This is achieved by resampling each of the values  n  times,    then pooling the draws and estimating a total distribution using KDE.  merge ( uvals :: Vector { :AbstractUncertainValue } ;   weights :: Weights   n   =   1000 ) ,     merge ( uvals :: Vector { :AbstractUncertainValue } ;   weights :: AnalyticalWeights   n   =   1000 )     and     merge ( uvals :: Vector { :AbstractUncertainValue } ;   weights :: ProbabilityWeights   n   =   1000 )    merges uncertain values by resampling them proportionally to  weights , then pooling    the draws and performing KDE. These are all functionally equivalent, but implementations   for different weights are provided for compatibility with StatsBase.  merge ( uvals :: Vector { :AbstractUncertainValue } ;   weights :: FrequencyWeights   n   =   1000 )     merges uncertain values by sampling them according to the number of samples provided    with  weights .", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#bug_fixes_1", 
            "text": "resample  didn't work for  UncertainIndexDataset s due to the data being stored in the     indices  field, not the  values  field as for other subtypes of     AbstractUncertainValueDataset . This is now fixed.", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v020", 
            "text": "", 
            "title": "UncertainData.jl v0.2.0"
        }, 
        {
            "location": "/changelog/#notes", 
            "text": "Julia 1.1 is required for version   v.0.2.0.", 
            "title": "Notes"
        }, 
        {
            "location": "/changelog/#new_functionality_3", 
            "text": "Spline interpolation on a regular grid.  Linear interpolation on an irregular grid.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_1", 
            "text": "support_overlap  now returns an interval (from  IntervalArithmetic ), in line with    what  support  returns.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v018", 
            "text": "", 
            "title": "UncertainData.jl v0.1.8"
        }, 
        {
            "location": "/changelog/#bug_fixes_2", 
            "text": "Added missing package dependencies which were not caught by CI.", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v017", 
            "text": "", 
            "title": "UncertainData.jl v0.1.7"
        }, 
        {
            "location": "/changelog/#new_functionality_4", 
            "text": "UncertainIndexValueDataset s can now be constructed from vectors of uncertain values.    To do so, provide a vector of uncertain values for the indices, and the same for the    values, e.g.  UncertainIndexValueDataset ([ idx1 ,   idx2 ],   [ val1 ,   val2 ]) .  Index-value dataset realizations can now be     interpolated on a regular grid .", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#bug_fixes_3", 
            "text": "minima  and  maxima  now returns the global minimum for a dataset instead of a vector    of elementwise minima and maxima.  Implemented the option to linearly interpolate index-value dataset realizations.    To do so, provide  resample  with a  RegularGrid  instance.  Merged redundant methods for assigning some distributions.  Fixed non-critical indexing bug for uncertain index-value datasets.  Removed redudant method definitions and multiple imports of the same files causing    definitions to be overwritten and printing warnings statements when loading the package.", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v016", 
            "text": "", 
            "title": "UncertainData.jl v0.1.6"
        }, 
        {
            "location": "/changelog/#new_functionality_5", 
            "text": "Implemented sequential sampling constraints  StrictlyIncreasing  and  StrictlyDecreasing    for  UncertainIndexValueDataset s.  Added  UncertainScalarPopulation  type, representing    vectors of values that should be sampled according to a vector of probabilities.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_2", 
            "text": "Improved documentation for  CertainValue s.  Added documentation for  UncertainScalarPopulation .  Added  UncertainScalarPopulation  to uncertain value overview list in the documentation.  Fixed duplicate docs for  cot ,  cotd ,  coth  and added missing  acot ,  acotd ,  acoth    docs.  Shortened and updated main documentation page with more links.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#bug_fixes_4", 
            "text": "Import  Base  functions properly when defining  CertainValue , so that no unexpected    behaviour is introduced.  Fixed links in documentation that pointed to the wrong locations.  Remove model resampling docs which was not supposed to be published until the    functionality is properly implemented.", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v015", 
            "text": "", 
            "title": "UncertainData.jl v0.1.5"
        }, 
        {
            "location": "/changelog/#new_functionality_6", 
            "text": "Added  CertainValue  type to represent scalars without   any uncertainty. Even though a scalar is not uncertain, we'll define it as subtype of     AbstractUncertainValue  to treat certain values alongside uncertain values in datasets.  Added plot recipe for  CertainValue s. They are just plotted as regular points.  Added method  resample ( Vector { AbstractUncertainValue } )  for resampling vectors of    uncertain values. Operates element-wise, just as for an uncertain dataset.  Added an abstract type  SequentialSamplingConstraint  to separate sequential constraints    from general constraints that might be applied  before  resampling according to    the sequential constraints.  Added abstract type ( OrderedSamplingAlgorithm ) and composite types    ( StartToEnd ,  EndToStart ,  MidpointOutwards ,  ChunksForwards ,  ChunksBackwards )    which indicates how to sample sequential realizations when resampling an uncertain    dataset. Only  StartToEnd  is used at the moment.  Added abstract type  SequentialSamplingConstraint  which is the supertype for all    sequential constraints.  Added function to check if strictly increasing sequences through an uncertain dataset    exist:  strictly_increasing_sequence_exists ( udata :: AbstractUncertainValueDataset .  Added function to check if strictly decreasing sequences through an uncertain dataset    exist:  strictly_increasing_sequence_exists ( udata :: AbstractUncertainValueDataset .  Added the  StrictlyIncreasing { T }   where   { T : OrderedSamplingAlgorithm }  sequential    constraint for resampling uncertain datasets.  Added the  StrictlyDecreasing { T }   where   { T : OrderedSamplingAlgorithm }  sequential    constraint for resampling uncertain datasets.   Added resampling methods   resample ( udata ,   sequential_constraint :: StrictlyIncreasing { T }   where   { T   :   StartToEnd }  resample ( udata ,   sequential_constraint :: StrictlyDecreasing { T }   where   { T   :   StartToEnd }  resample ( udata ,   constraint :: SamplingConstraint ,   sequential_constraint :: StrictlyIncreasing { T }   where   { T   :   StartToEnd }  resample ( udata ,   constraint :: SamplingConstraint ,   sequential_constraint :: StrictlyDecreasing { T }   where   { T   :   StartToEnd }  resample ( udata ,   constraint :: Vector { SamplingConstraint } ,   sequential_constraint :: StrictlyIncreasing { T }   where   { T   :   StartToEnd }  resample ( udata ,   constraint :: Vector { SamplingConstraint } ,   sequential_constraint :: StrictlyDecreasing { T }   where   { T   :   StartToEnd }", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_3", 
            "text": "Added  documentation on sequential constraints , clearly separating it from the general constraints.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v014", 
            "text": "", 
            "title": "UncertainData.jl v0.1.4"
        }, 
        {
            "location": "/changelog/#breaking_changes", 
            "text": "Elementary operations for  ( scalar ,   uncertain_value ) ,  ( uncertain_value ,   scalar )  and     ( uncertain_value ,   uncertain_value )  pairs now returns an uncertain value instead of    a vector of resampled realizations. The default behaviour is to perform a kernel    density estimate over the vector of results of the element-wise operations (which    was previously returned without representing it as an uncertain value).", 
            "title": "Breaking changes"
        }, 
        {
            "location": "/changelog/#new_functionality_7", 
            "text": "Implemented constraints for datasets that have already been constrained.     constrain ( udata :: ConstrainedDataset ,   s :: SamplingConstraint )  will now return another     ConstrainedDataset . The same applies for  ConstrainedIndexDataset  and     ConstrainedValueDataset .  Added  maximum ( Vector { AbstractUncertainValue } )  and     minimum ( Vector { AbstractUncertainValue } )  methods.  Added plot recipe for  Vector { AbstractUncertainValue } s. Behaves just as plotting an   uncertain dataset, assuming an implicit indices  1 : length ( v ) . Error bars may be    tuned by providing a second argument of quantiles to  plot , e.g.  plot ( v ,   [ 0 . 2 ,   0 . 8 ]    gives error bars covering the 20 th  to 80 th  percentile range of the data.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_4", 
            "text": "Added documentation for  StrictlyIncreasing  and  StrictlyDecreasing  sampling    constraints.  Added  show  function for  AbstractUncertainIndexDataset .  show  errored previously,    because it assumed the default behaviour of  AbstractUncertainValueDataset , which    does not have the  indices  field.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#bug_fixes_5", 
            "text": "Fixed bug when resampling an uncertain dataset using the  NoConstraint  constraint,    which did not work to due to a reference to a non-existing variable.  Fixed test bug where when resampling an uncertain value with the  TruncateStd  sampling   constraint, the test compared the result to a fixed scalar, not the standar deviation    of the value. This sometimes made the travis build fail.", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v013", 
            "text": "", 
            "title": "UncertainData.jl v0.1.3"
        }, 
        {
            "location": "/changelog/#new_functionality_8", 
            "text": "Allow both the  indices  and  values  fields of  UncertainIndexValueDataset  to be any    subtype of  AbstractUncertainValueDataset . This way, you don't  have  to use an    index dataset type for the indices if not necessary.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_5", 
            "text": "Improved documentation for  UncertainIndexDataset ,  UncertainValueDataset ,     UncertainDataset  and  UncertainIndexValueDataset  types and added an     overview page  in the documentation    to explain the difference between these types.  Added an  overview  section for the resampling    documentation.  Cleaned and improved  documentation for uncertain values .  Added separate  documentation for the uncertain index dataset type .  Added separate  documentation for the uncertain value dataset type .  Improved  documentation for the generic uncertain dataset type  Merged documentation for sampling constraints and resampling.  Added missing documentation for the  sinc ,  sincos ,  sinpi ,  cosc  and  cospi  trig    functions.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v012", 
            "text": "", 
            "title": "UncertainData.jl v0.1.2"
        }, 
        {
            "location": "/changelog/#new_functionality_9", 
            "text": "Support  elementary mathematical operations     ( + ,  - ,  *  and  / ) between arbitrary    uncertain values of different types. Also works with the combination of scalars and    uncertain values. Because elementary operations should work on arbitrary uncertain    values, a resampling approach is used to perform the mathematical operations. This    means that all mathematical operations return a vector containing the results of    repeated element-wise operations (where each element is a resampled draw from the    furnishing distribution(s) of the uncertain value(s)). The default number of    realizations is set to  10000 . This allows calling  uval1   +   uval2  for two uncertain    values  uval1  and  uval2 . If you need to tune the number of resample draws to  n ,    you need to use the  + ( uval1 ,   uval2 ,   n )  syntax (similar for the operators). In the    future, elementary operations might be improved for certain combinations of uncertain   values where exact expressions for error propagation are now, for example using the    machinery in  Measurements . jl  for normally distributed values.  Support for  trigonometric functions  added ( sin ,  sind ,  sinh ,  cos ,    cosd ,  cosh ,  tan ,  tand ,  tanh ,  csc ,  cscd ,  csch ,  csc ,  cscd ,  csch ,     sec ,  secd ,  sech ,  cot ,  cotd ,  coth ,  sincos ,  sinc ,  sinpi ,  cosc ,     cospi ). Inverses are also defined ( asin ,  asind ,  asinh ,  acos ,    acosd ,  acosh ,  atan ,  atand ,  atanh ,  acsc ,  acscd ,  acsch ,  acsc ,  acscd ,     acsch ,  asec ,  asecd ,  asech ,  acot ,  acotd ,  acoth ).   Beware: if the support of the funishing distribution for an uncertain value lies partly    outside the domain of the function, you risk encountering errors.   These also use a resampling approach, using  10000  realizations by default.    Use either the  sin ( uval )  syntax for the default, and  sin ( uval ,   n :: Int )  to tune the    number of samples.  Support non-integer multiples of the standard deviation in the  TruncateStd  sampling    constraint.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#fixes", 
            "text": "Fixed bug in resampling of index-value datasets, where the  n  arguments wasn't used.  Bugfix: due to  StatsBase . std  not being defined for  FittedDistribution  instances,    uncertain values represented by  UncertainScalarTheoreticalFit  instances were not    compatible with the  TruncateStd  sampling constraint. Now fixed!  Added missing  resample ( uv :: AbstractUncertainValue ,   constraint :: TruncateRange ,   n :: Int )     method.", 
            "title": "Fixes"
        }, 
        {
            "location": "/changelog/#improvements_6", 
            "text": "Improved resampling documentation for  UncertainIndexValueDataset s. Now shows    the documentation for the main methods, as well as examples of how to use different    sampling constraints for each individual index and data value.  Improved resampling documentation for  UncertainDataset s. Now shows    the documentation for the main methods.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v011", 
            "text": "", 
            "title": "UncertainData.jl v0.1.1"
        }, 
        {
            "location": "/changelog/#new_functionality_10", 
            "text": "Indexing implemented for  UncertainIndexValueDataset .  Resampling implemented for  UncertainIndexValueDataset .  Uncertain values and uncertain datasets now support  minimum  and  maximum .  support ( uv :: AbstractUncertainValue )  now always returns an interval from     IntervalArithmetic.jl  support_overlap  now computes overlaps also for fitted theoretical distributions.  Added more plotting recipes.  All implemented uncertain data types now support resampling.", 
            "title": "New functionality"
        }, 
        {
            "location": "/changelog/#improvements_7", 
            "text": "Improved general documentation. Added a reference to     Measurements.jl  and an explanation    for the differences between the packages.  Improved resampling documentation with detailed explanation and plots.", 
            "title": "Improvements"
        }, 
        {
            "location": "/changelog/#uncertaindatajl_v010", 
            "text": "Basic functionality in place.", 
            "title": "UncertainData.jl v0.1.0"
        }, 
        {
            "location": "/publications/", 
            "text": "Scientific papers\n\n\n\n\nVasskog, Kristian, John\u2010Inge Svendsen, Jan Mangerud, Kristian Agas\u00f8ster Haaga,    Arve Svean, and Eva Maria Lunnan. \"Evidence of early deglaciation (18 000 cal a bp)    and a postglacial relative sea\u2010level curve from southern Karm\u00f8y, south\u2010west Norway.\"    Journal of Quaternary Science    (2019)\n.\n\n\n\n\n\n\nSoftware\n\n\n\n\nIn a coming release, \nCausalityTools.jl\n    uses UncertainData.jl to detect causal relationships between time series with    uncertainties both in (time) indices and data values.", 
            "title": "Publications and software"
        }, 
        {
            "location": "/publications/#scientific_papers", 
            "text": "Vasskog, Kristian, John\u2010Inge Svendsen, Jan Mangerud, Kristian Agas\u00f8ster Haaga,    Arve Svean, and Eva Maria Lunnan. \"Evidence of early deglaciation (18 000 cal a bp)    and a postglacial relative sea\u2010level curve from southern Karm\u00f8y, south\u2010west Norway.\"    Journal of Quaternary Science    (2019) .", 
            "title": "Scientific papers"
        }, 
        {
            "location": "/publications/#software", 
            "text": "In a coming release,  CausalityTools.jl     uses UncertainData.jl to detect causal relationships between time series with    uncertainties both in (time) indices and data values.", 
            "title": "Software"
        }
    ]
}